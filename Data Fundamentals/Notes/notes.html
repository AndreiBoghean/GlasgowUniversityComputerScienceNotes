<h1>ndarrays</h1>
<div>
    <p>ndarrays, or&nbsp;<em>n</em>-dimensional arrays can be used to represent various types of data, e.g. sound, images, video, tabular data, etc. Multi-dimensional arrays can be easily manipulated with vectorised operations. These are operations applied on an entire arrays without explicitly writing a loop.<br>For example, assuming an array&nbsp;<code>pos</code>&nbsp;containing 100 2D coordinates, to scale all the coordinates by 2 and add an offset of 10 in the y-value, we could simply write&nbsp;<code>pos=pos*2+[0,10]</code>. There are no explicit loops, making the operation more readable.<br>In particular, vectorisation is a special type of parallel computing where the arrays are restricted to a particular size.</p>
</div>
<h2>Shapes of Arrays</h2>
<div>
    <p>ndarrays have dimensions( or ranks). A 1D arrays is a&nbsp;<strong>vector</strong>. A 2D array is a&nbsp;<strong>matrix</strong>. An array of higher dimension is a&nbsp;<strong>tensor</strong>. The shape of an array refers to the number of elements it can hold in each dimension. We refer to each dimension within an array by axis. This is indexed by 0 (0 being the rows, 1 being the columns, 2 is the depth/frames/etc)</p>
</div>
<h3>Array Operations</h3>
<div>
    <p>There are many types of operations that we can perform on arrays, such as;</p>
</div>
<div>
    <ul>
        <li>
            <div></div>Arithmetic (addition, subtraction)
        </li>
        <li>
            <div></div>Indexing and slicing
        </li>
        <li>
            <div></div>Generating arrays (an array with uniformly distributed numbers)
        </li>
        <li>
            <div></div>Rearranging arrays (taking the transpose of a matrix or reshaping the array)
        </li>
        <li>
            <div></div>Ordering arrays (sorting the array)
        </li>
        <li>
            <div></div>Aggregation (finding the sum, start and stop)
        </li>
    </ul>
</div>
<h3>Static Arrays</h3>
<div>
    <p>There are many differences between a list and an ndarray, as highlighted below:</p>
</div>
<div>
    <ul>
        <li>
            <div></div>ndarrays have a fixed, pre-defined size (or shape). They cannot be extended/resized after definition
        </li>
        <li>
            <div></div>ndarrays have a fixed, predefined type
        </li>
        <li>
            <div></div>ndarrays can only hold numbers (usually)
        </li>
        <li>
            <div></div>ndarrays are meant to hold multi-dimensional data
        </li>
        <li>
            <div></div>ndarrays must be rectangular in shape, e.g. equal numbers of rows in each column
        </li>
    </ul>
</div>
<div>
    <p>Nonetheless, ndarrays are still mutable, e.g. we can set a value of the array without creating a new copy. When initialising ndarrays, we need to specify its type. When initialising ndarrays, we need to specify its type. This is because an ndarray is stored as a block of raw numbers. For this reason, ndarrays have a thin wrapper&nbsp;for&nbsp;raw blocks of memory. As a result, they are much more efficiently stored and operations can be performed on them&nbsp;much&nbsp;faster.</p>
</div>
<h1>NumPy</h1>
<div>
    <p>NumPy contains a class for ndarrays. An array in NumPy has both a shape, (e.g.&nbsp;<code>(5,6)</code>) and a dtype(e.g.&nbsp;<code>float64). Given a list, we can convert it into an array using&nbsp;</code>np.array(list)`. We can also create an array in NumPy in the following ways;</p>
</div>
<div>
    <ul>
        <li>
            <div></div><code>np.empty(shape)</code>&nbsp;allocates space for an array of the given shape
        </li>
        <li>
            <div></div><code>np.zeros(shape)</code>&nbsp;initialises an array of the given shape with zeroes
        </li>
        <li>
            <div></div><code>np.ones(shape)</code>&nbsp;initialises an array of the given shape with ones
        </li>
        <li>
            <div></div><code>np.full(shape,val)</code>&nbsp;initialises an array off the given shape with the given value
        </li>
    </ul>
</div>
<div>
    <p>The functions which initialise an array first call&nbsp;<code>np.empty</code>&nbsp;to allocate the space. They also have a&nbsp;<code>like</code>&nbsp;version, where they are provided an array instead of a shape and take its shape, for example&nbsp;<code>np.ones_like(array)</code>&nbsp;takes the shape of the input array.</p>
</div>
<div>
    <p>We can also create arrays with random entries given below:</p>
</div>
<div>
    <ul>
        <li>
            <div></div><code>np.random.randint(start, stop, shape)</code>&nbsp;creates an array with uniform random integers between the given start (inclusive) and the given stop (exclusive)
        </li>
        <li>
            <div></div><code>np.random.uniform(start, stop, shape)</code>&nbsp;creates an array with uniform floating point numbers between the given stop and start.
        </li>
        <li>
            <div></div><code>np.random.normal(mu,sigma,shape)</code>&nbsp;creates an array with normally distributed random floating point numbers with the mean mu, and standard deviation sigma.
        </li>
    </ul>
</div>
<div>
    <p>We can also create a range of numbers:</p>
</div>
<div>
    <ul>
        <li>
            <div></div><code>np.arange(value)</code>&nbsp;creates a 1D array of numbers from 0 to value
        </li>
        <li>
            <div></div><code>np.arange(start, stop)</code>&nbsp;creates a 1D array of numbers from start(inclusive) to stop(exclusive)
        </li>
        <li>
            <div></div>
            <div></div><code>np.arange(params)</code>
            <ul>
                <li>
                    <div></div>params:<code>value</code>&nbsp;– creates a 1D array of numbers from 0 to value
                </li>
                <li>
                    <div></div>params:&nbsp;<code>start, stop</code>&nbsp;– creates a 1D array of numbers from start(inclusive) to stop(exclusive)
                </li>
                <li>
                    <div></div>params:&nbsp;<code>start,stop,step</code>&nbsp;– creates a 1D array of numbers from start(inclusive) to stop(exclusive) with the given step
                </li>
            </ul>
        </li>
        <li>
            <div></div><code>np.linspace(start,stop,space)</code>&nbsp;creates a 1D array of numbers from start(inclusive) to stop(inclusive) with the given step
        </li>
    </ul>
</div>
<div>
    <p>We can open a text file using&nbsp;<code>np.loadtxt(filename)</code>&nbsp;and save a file using&nbsp;<code>np.savetxt(array,filename)</code></p>
</div>
<h2>Slicing and Indexing</h2>
<div>
    <p>Indexing is taking an element from the array, while slicing is taking a subset of the array. We take the multi-dimensional indices when indexing. We have 3 parameters in slicing -<code>start, stop, and step</code>, like in vanilla Python. We can reverse an array by using step size -1.<br>Slicing does not change the rank of an array, it selects a rectangular subset with the same number of dimensions. On the other hand, indexing usually reduces the rank - it selects a rectangular subset where one dimension is a singleton.<br>We can also transpose an array - this exchanges the rows and columns for a 2D array, and flips the order of the axes in general. It is given by&nbsp;<code>array.T</code>. This operation takes&nbsp;&nbsp;time.</p>
</div>
<h3>Concatenation and Stacking</h3>
<div>
    <p>We can join two arrays (concatenation), or create a new array with the two arrays (stacking). The arrays must be of the right shape for this to happen. If we are concatenating with multiple dimensions, the axis of concatenation must be specified</p>
</div>
<h3>Tiling</h3>
<div>
    <p>We can repeat an array multiple times. The function&nbsp;<code>np.tile(array,lines)</code>&nbsp;returns another array where the shape of the tiling states how the array should be repeated or joined</p>
</div>
<div>
    <h3>Boolean Arrays</h3>
    <div>
        <p>Using boolean arrays, we can check whether elements in the array satisfy a condition. Using the condition&nbsp;<code>np.where(bool, a, b)</code>, we generate a new array. In this array,&nbsp;<code>a</code>&nbsp;represents the value for elements that meet the condition, and&nbsp;<code>b</code>&nbsp;represents the value for elements which do not. Using&nbsp;<code>np.nonzero(bool)</code></p>
    </div>
    <h1>Map and Broadcast</h1>
    <h2>Maps</h2>
    <div>
        <p>We can apply a function to each element within an array - this is elementwise computation. A map operation can have</p>
    </div>
    <div>
        <ul>
            <li>
                <div></div>A single argument, e.g.&nbsp;<code>np.tan(x)</code>&nbsp;or&nbsp;<code>-x</code>
            </li>
            <li>
                <div></div>Two arguments, e.g.&nbsp;<code>x-y</code>&nbsp;or&nbsp;<code>np.maximum(x,y)</code>
            </li>
            <li>
                <div></div>something else, e.g.&nbsp;<code>np.where(x,0,1)</code>
            </li>
        </ul>
    </div>
    <div>
        <p>In a map operation with multiple arrays, their shapes must be compatible. If the two arrays have the same shape, then the operations is applied elementwise. Otherwise, one of the arrays will be broadcasted so that they have the same shape. Broadcasting involves tiling one of the arrays until the two arrays have the same shape, if possible. This is much more efficient that explicit broadcasting.</p>
    </div>
    <h2>Broadcasting</h2>
    <div>
        <p>Broadcasting is how arithmetic operations are done on arrays which have different shapes. The rules are:</p>
    </div>
    <div>
        <ul>
            <li>
                <div></div>If the two arrays have the same number of dimensions, then they must have the same shape. The operation is done elementwise.
            </li>
            <li>
                <div></div>If one array has fewer dimensions than the other, then the last dimension of the bigger array must match the dimension of the smaller array.
            </li>
        </ul>
    </div>
    <div>
        <p>For example,</p>
    </div>
    <div>
        <ul>
            <li>
                <div></div><code>shape(2,2) * shape(2,2)</code>&nbsp;is allowed
            </li>
            <li>
                <div></div><code>shape(2,3,4) * shape(3,4)</code>&nbsp;is allowed
            </li>
            <li>
                <div></div><code>shape(2,3,4) * shape(2,3)</code>&nbsp;is not allowed
            </li>
        </ul>
    </div>
    <div>
        <p>To perform the operation column-wise, we can transpose, perform the operation, then transpose back.</p>
    </div>
    <h1>Reduction and Accumulation</h1>
    <h2>Reduction</h2>
    <div>
        <p>A reduction/aggregation applies a function to two elements within the array repeatedly to return a scalar. These operations are:</p>
    </div>
    <div>
        <ul>
            <li>
                <div></div><code>np.all(array)</code>&nbsp;combines the elements in the (boolean) array with&nbsp;<code>AND</code>
            </li>
            <li>
                <div></div><code>np.any(A)</code>&nbsp;combines the elements in the (boolean) array with&nbsp;<code>OR</code>
            </li>
            <li>
                <div></div><code>np.min(A)</code>&nbsp;combines the elements in the array with the minimum operation
            </li>
            <li>
                <div></div><code>np.max(A)</code>&nbsp;combines the elements in the array with the maximum operation
            </li>
            <li>
                <div></div><code>np.sum(A)</code>&nbsp;combines the elements in the array with +
            </li>
            <li>
                <div></div><code>np.prod(A)</code>&nbsp;combines the elements in the array with *
            </li>
            <li>
                <div></div><code>np.mean(A)</code>&nbsp;calls&nbsp;<code>np.sum(A)</code>&nbsp;and divides the result by `len(A)
            </li>
            <li>
                <div></div><code>np.std(A)</code>&nbsp;calls&nbsp;<code>np.mean(A)</code>&nbsp;calls&nbsp;<code>np.mean(A)</code>&nbsp;and then applied the standard deviation formula
            </li>
        </ul>
    </div>
    <div>
        <p>We can specify the axis/axes to only reduce with respect to the given dimension(s). By default, it reduces with respect to all the axes.</p>
    </div>
    <h2>Accumulation</h2>
    <div>
        <p>Accumulation refers to the process of computing the cumulative sum or product of the array elements. The operations are:</p>
    </div>
    <div>
        <ul>
            <li>
                <div></div><code>np.cumsum(array)</code>&nbsp;is the accumulation of +
            </li>
            <li>
                <div></div><code>np.cumprod(A)</code>&nbsp;is the accumulation of *
            </li>
            <li>
                <div></div><code>np.diff(A)</code>&nbsp;is the accumulation of - (and has one less output than input)
            </li>
            <li>
                <div></div><code>np.gradient(A)</code>&nbsp;is like&nbsp;<code>np.diff</code>, but uses central differences to get the same length output, and it computes the gradient over every axis and returns them all in a list
            </li>
        </ul>
    </div>
    <h2>Finding</h2>
    <div>
        <p>We can find the indices that match the given criteria</p>
    </div>
    <div>
        <ul>
            <li>
                <div></div><code>np.argmax(array)</code>&nbsp;returns the index of the maximum element in the array
            </li>
            <li>
                <div></div><code>np.argmin(A)</code>&nbsp;returns the index of the minimum element of the array
            </li>
            <li>
                <div></div><code>np.argsort(A)</code>&nbsp;returns the indices of the elements in the array such that those indices represent the array sorted (we can index the array using the&nbsp;<code>argsorted</code>&nbsp;array to get the sorted array)
            </li>
            <li>
                <div></div><code>np.nonzero(A)</code>&nbsp;returns the indices of all the non-zero elements in the array (or True in boolean arrays)
            </li>
        </ul>
    </div>
    <h1>Floating Point</h1>
    <div>
        <p>There are various representations of numbers. The two main classes of numbers are integers and floats. The ranges of different data types is given below:</p>
    </div>
    <div>
        <table>
            <thead>
                <tr>
                    <th>Name</th>
                    <th>Bytes</th>
                    <th>Min</th>
                    <th>Max</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>int8</td>
                    <td>1</td>
                    <td>-128</td>
                    <td>127</td>
                </tr>
                <tr>
                    <td>uint6</td>
                    <td>1</td>
                    <td>0</td>
                    <td>255</td>
                </tr>
                <tr>
                    <td>int16</td>
                    <td>2</td>
                    <td>−32,768</td>
                    <td>32,767</td>
                </tr>
                <tr>
                    <td>uint16</td>
                    <td>2</td>
                    <td>0</td>
                    <td>65,535</td>
                </tr>
                <tr>
                    <td>int32</td>
                    <td>4</td>
                    <td>−2,147,483,648</td>
                    <td>2,147,483,647</td>
                </tr>
                <tr>
                    <td>uint32</td>
                    <td>4</td>
                    <td>0</td>
                    <td>4,294,967,295</td>
                </tr>
                <tr>
                    <td>int64</td>
                    <td>8</td>
                    <td>−9,223,372,036,854,775,808</td>
                    <td>+9,223,372,036,854,775,807</td>
                </tr>
                <tr>
                    <td>uint64</td>
                    <td>8</td>
                    <td>0</td>
                    <td>18,446,744,073,709,551,615</td>
                </tr>
            </tbody>
        </table>
    </div>
    <div>
        <p>If we apply an operation that exceeds that maximum value, then we have overflow. Overflows have undefined behaviour. For example adding 8 to the int8 value 120 (exceeds 127; result might be 127, or −128, or some other number). In most systems you will ever see, the result will be to wrap around.<br>Floating point numbers can represent a much bigger range of numbers by storing</p>
    </div>
    <div>
        <ul>
            <li>
                <div></div>The mantissa: a fractional number with a standardised range (number between 1.0 and 2.0)
            </li>
            <li>
                <div></div>The exponent: a scaling factor (powers of 2)
            </li>
        </ul>
    </div>
    <div>
        <p>Using this representation, we can store a lot of digits with very few values. They are, however, less precise than integers for this reason. Numbers close to 0 can be represented quite precisely, but large numbers cannot be represented with as much precision. A floating point number also stores the sign(i.e. whether the number is positive of negative.)<br>A floating point number can therefore be constructed using the three parts, in particular;</p>
    </div>
    <div>
        <ul>
            <li>
                <div></div>value = sign * (1.[mantissa])*2<sup>exponent</sup>
            </li>
        </ul>
    </div>
    <div>
        <p>Since the value before the mantissa is always 1, we do not need to store it explicitly. For example, the&nbsp;<code>float32</code>&nbsp;format of a number is:</p>
    </div>
    <div>
        <ul>
            <li>
                <div></div>1 bit for the sign
            </li>
            <li>
                <div></div>8 bits for the exponent
            </li>
            <li>
                <div></div>the remaining 23 bits for the mantissa
            </li>
        </ul>
    </div>
    <div>
        <p>In binary, a float32 format is:</p>
    </div>
    <div>
        <pre>     1    10000011    00100111010001001000101</pre>
    </div>
    <div>
        <p>Here,</p>
    </div>
    <div>
        <ul>
            <li>
                <div></div>The number is negative, since the sign is 1
            </li>
            <li>
                <div></div>The mantissa represents 1.00100111010001001000101. In decimal, this is 1.153389573097229<sub>10</sub>
            </li>
            <li>
                <div></div>The exponent represents 2<sup>131−127</sup>&nbsp;= 2<sup>4</sup>&nbsp;= 16 (10000011<sub>2</sub>&nbsp;= 131<sub>10</sub>This is because of the implied offset)
            </li>
        </ul>
    </div>
    <p>Therefore, the number is -18.454233169555664</p>
    <p>The standard for floating point numbers is IEEE754, which specifies the representation of floats, operations on them and some "special" numbers. The standards are given below:&nbsp;<br></p>
    <p><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/928868568/image%20%28221%29.png" alt="" width="800" height="165" class="img-fluid atto_image_button_text-bottom"><br></p>
    <p>Most of the time, we will be using either single precision float32 (float) or double precision float64(double). GPUs typically are fastest (by a lot) using float32, but can do double precision float64 computations at some significant cost. Most desktop CPUs (e.g. x86) have specialised float64 hardware.&nbsp;</p>
    <p><br></p>
    <h2>Binary Representation of Floats&nbsp;</h2>
    <p>We will now look at some float representations.&nbsp;</p>
    <p>
    </p>
    <ul>
        <li>The float representation for 1 is&nbsp;<br>
            <pre>0    01111111111    00.00</pre>
            The sign bit is 0, so the number is positive. The exponent is 0 (after subtracting the bias), and the mantissa is all 0s. So, the value is $$2^0$$
        </li>
        <li>The float representation for 4 is&nbsp;<br>
            <pre>0    10000000001    00.00</pre>The exponent is 2 and the mantissa is all 0s, so the value is $$2^2$$<br>
        </li>
        <li>The float representation for 5 is&nbsp;<br>
            <pre>0    10000000001    0100.00</pre>The exponent is 2 and the mantissa is 1.25, so the value is $$2^2+1.25$$<br>
        </li>
        <li>...</li>
    </ul>
    <p><br></p>
    <h1>Floating Point weirdness</h1>
    <p><br></p>
    <p>There are 5 types of errors with floats:&nbsp;</p>
    <p></p>
    <ul>
        <li><strong>Invalid operations</strong>: This occurs when the result of the operation is not a real number/undefined, e.g. 0/0 or sqrt(-1)<br></li>
        <li><strong>Division by zero</strong>: this occurs when dividing a (nonzero) number by 0</li>
        <li><strong>Overflow</strong>: This occurs if the result of a computation exceeds the limits of the floating point number</li>
        <li><strong>Underflow</strong>: This occurs if the result of a computation is smaller than the smallest representable number, and so is rounded off to zero;</li>
        <li><strong>Inexact</strong>: This occurs if a computation will produce an inexact result due to rounding.&nbsp;</li>
    </ul>
    <p>An exception might be trapped or untrapped. An untrapped exception will not halt execution, and will instead do some default operation (e.g. untrapped divide by zero will output <em>np.inf</em> instead of halting). A trapped exception will cause the process to be signalled in to indicate that the operation is problematic, at which point it can either halt or take another action. Typically, invalid operations are trapped, while inexact and underflows aren't trapped, division by zero and underflow might or might not be trapped. It is possible to manually trap them.</p>
    <p>There are 2 versions of 0, $$+0$$ and $$-0$$. The float64 representation of $$+0.0$$ is&nbsp;</p>
    <pre>0     00000000000     00.00</pre>
    <p>and for $$-0.0$$ is</p>
    <pre>1     00000000000     00.00</pre>
    <p>They only differ in their sign bit, and both have mantissa and exponent 0. Similarly, IEEE754 floats encode $$+\infty$$ and $$-\infty$$. The float representation for each of them respectively is&nbsp;</p>
    <pre>0     11111111111     00.00</pre>
    <pre>1     11111111111     00.00</pre>
    <p>The sign bit represents the sign of the infinity, the mantissa is all 0, and the exponent is all 1.</p>
    <p>The infinity values satisfies <code>np.inf+value = np.inf</code> and <code>np.inf*value =np.inf</code>. NaN (Not a Number) represents invalid values. The following operations result in NaN:</p>
    <p></p>
    <ul>
        <li>$$0/0$$</li>
        <li><code>np.inf/np.inf</code> (either positive or negative)</li>
        <li><code>np.inf - np.inf</code> or <code>np.inf + -np.inf</code><br></li>
        <li><code>np.inf * 0</code> or <code>0 * -np.inf</code><br></li>
        <li><code>sqrt(value)</code>, where value is strictly negative</li>
        <li><code>log(value)</code>, where value is strictly negative</li>
    </ul>
    <p>The value NaN propagates, any operation on <code>np.nan</code> will return <code>np.nan</code> (except <code>1**np.nan = 1.0</code>) Every comparison with NaN results in False. <code>np.nan == np.nan</code> returns false, but we can check whether the value is NaN by using <code>np.isnan(value)</code>. However, NaN is equivalent to False.</p>
    <p>As a floating point number, NaN has exponent all 1 and any non-zero mantissa. So, there are $$2^{52}-1$$ versions of NaN in float64. An example of such representation is:</p>
    <pre>0     11111111111      11.11 </pre>
    <p>NaN can be the result of some underflow, which results in division by 0, for example. It can also be used for missing data.</p>
    <p><br></p>
    <h1>Roundoff and Precision</h1>
    <p><br></p>
    <p>Due to their representation, floats have roundoff errors. There can be huge gaps between floating point representations for big numbers, for instance. The value <code>1.0+1e-8</code> can be represented quite precisely in float64, but the value <code>1.0+1e-16</code> gets converted to 1, there is no number closer than 1 for it to be represented by.</p>
    <p>We can order the operations to minimise roundoff errors, e.g. <code>(1.0e30 + 1) - 1.0e300</code> will return 0, but <code>(1.0e30 - 1.0e30) + 1.0</code> will return 1. The main issues with roundoff are:</p>
    <p>
    </p>
    <ul>
        <li>x+y will have a large error if x and y have different magnitudes (magnitude error) e.g. 1.0 + 1.0 is fine, but 1.0 + 1e300 is bad</li>
        <li>x-y will have a large error if x and y are approximately equal (cancellation error) e.g. 1000 - 1200 is fine, but 5000.0 - 5000.0000000000005 is not.</li>
    </ul>
    <p>Because of roundoff errors, we should not compare floats for equality, instead, we should check whether the values are close enough, using <code>np.allclose(x,y)</code>. This checks whether the difference/ratio of the two numbers is close to 0 or 1 respectively. For a number, we define the relative error of its floating point representation by</p>
    <p style="text-align:center;">$$ \epsilon = \displaystyle\frac{|\text{float}(x)-x|}{|x|} $$</p>
    <p>i.e the absolute difference between a floating point number and its real part, normalised by the magnitude of the real number. IEEE754 ensures that the value of $$\epsilon$$ is quite small, e.g. for float64, it is $$\epsilon = 2^{-53}$$</p>
    <p><br></p>
    <h1>Arrays in Memory</h1>
    <p><br></p>
    <p>Arrays are tightly packed in memory, which means that they have very
        small, constant overhead over the storage of elements. There is a short header
        which describes how the data is present in memory, followed by the data itself.</p>
    <p>Arrays are stored as a sequence of numbrs in a long list. This is true for
        every array- there is a short header followed by a long sequence of numbers.
        We can see the order of the sequence using <code>np.ravel(array)</code>.<br></p>
    <p><br></p>
    <h2>Strides and Shape</h2>
    <p>We implement multi-dimensional arrays by striding. It is a set of offset
        consts (called strides) which specify how to index into the array. There is one
        value per axis.</p>
    <p>For each dimension, there is a stride that tells us how many bytes forward to seek to the next dimension. For 1D arrays, there is one stride, which is the length of the data type. For 2D arrays, there are two strides, the first may be 8 (one float), and the second may be <code>8*x.shape[0]</code>. So, to move to the next column, add 8, to move to the next row, add 8 * number of columns. Strides are given in bytes (instead of elements) for more efficiency.</p>
    <p>In general, to find the array element at index <code>[i,j]</code> in a 2D matrix, the memory offset from the start of the number block will be <code> 1 * stride[0] + j * stride[1]</code></p>
    <p>This generalises to higher dimensions, like 3D and 4D tensors. To iterate through an array, the computations simply increment by the appropriate stride to move to the next element.&nbsp;</p>
    <p>This representation of arrays is called a dope vector, which refers to the information in the strides. It is separate to the data, and part of the header. For example, the following is a float64 2D matrix of shape of shape (6,5)</p>
    <p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/928868568/image%20%28222%29.png" alt="" width="300" height="192" class="img-fluid atto_image_button_text-bottom"><br></p>
    <p style="text-align:left;">To move to the next row, we move by 40 bytes, to move to the next column, we move by 8. Using strides, transposition becomes $$O(1)$$. This is because we merely change the strides (the dope vector) and the shape. So the transposition of the above matrix has the following shape:</p>
    <p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/928868568/image%20%28223%29.png" alt="" width="300" height="195" class="img-fluid atto_image_button_text-bottom"><br></p>
    <p style="text-align:left;">So, to move the the next row, instead of moving by 40 bytes we move by 8 in the transpose. Similarly, to move to the next column, we move by 40 in the transpose instead of 8.</p>
    <p style="text-align:left;">There are other operations that change the layout of the matrix, but can be performed in $$O(1)$$, such as:</p>
    <p style="text-align:left;"></p>
    <ul>
        <li><code>np.flipup(array)</code> : flips the matrix up/down. So, if we flip the $$6 \times 5$$ matrix, we get the following shape<br><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/928868568/image%20%28224%29.png" alt="" class="img-fluid"><br>We have reversed the order of the rows. For this reason, the start offset is the $$(6-1) \times 5 = 25$$th element, and moving to the next row is moving back 40 bytes</li>
        <li><code>np.fliplr(array)</code>: flips the matrix left/right<br><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/928868568/image%20%28225%29.png" alt="" class="img-fluid"><br>We have reversed the order of the columns, the start is the last element in the first column, and moving to the next column is moving back 8 bytes</li>
        <li><code>np.rot90(array)</code> : rotates the matrix by 90 degrees<br><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/928868568/image%20%28226%29.png" alt="" class="img-fluid"><br>Rotation by 90 degrees is equivalent to taking the transpose after reversing the order of the columns, the shape and the strides are flipped but the offset remains the same</li>
    </ul>
    <p>NumPy gives the strides of the original matrix as (40,8) and not (8,40). This is because NumPy uses C/row major ordering, in which the last index changes first, as opposed to Fortran/column major ordering, in which the first index changes first. Both methods can be used to unravel an array though</p>
    <p><br></p>
    <h1>Tensor Operations</h1>
    <p><br></p>
    <h2>Reshaping</h2>
    <p>Along with transposition and flips, we can also reshape an array in constant time. This just changes how the array is indexed, i.e. its strides. We cannot change the number of elements in the array by reshaping. We reshape with <code>array.reshape(shape)</code></p>
    <p>When we reshape, the following rules are obeyed:</p>
    <p>
    </p>
    <ul>
        <li>The number of elements in the array remains the same</li>
        <li>The order of the elements remain the same</li>
        <li>the last dimension changes first, the second last next, and so on.</li>
    </ul>
    <p>We can add a singleton dimension by indexing with <code>None</code>, i.e. <code>x[ : , None]</code> transforms a 1D vector x into a 2D matrix with one column in each row. We can get rid of singleton dimensions using <code>np.squeeze</code>. In tensors, we can avoid listing all the indices when slicing, e.g. <code>x[2, :, :, :, 5]</code> can be written as <code>x[2, ..., 5]</code>. This is called <strong>elision</strong>.</p>
    <p><br></p>
    <h2>Swapping and Rearranging Axes</h2>
    <p>We can rearrange axes using <code>np.swapacs(array, axis1, axis2)</code> to swap the specified axes. For example. consider the following representation of a $$3 \times 4 \times 5$$ tensor</p>
    <p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/928868568/image%20%28227%29.png" alt="" class="img-fluid"><br></p>
    <p style="text-align:left;">If we swap axes 0 and 2, we get the following shape</p>
    <p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/928868568/image%20%28228%29.png" alt="" class="img-fluid"><br></p>
    <p style="text-align:left;">This happens in constant time since it merely changes the strides. Reshaping uses the pouring rule, i.e. last dimensions pours first. However, we might not want to amend the final dimension, in which case, we swap axes, perform the operation, and swap back.</p>
    <p style="text-align:left;">We might need to swap multiple axes at once, which we can do more efficiently using <code>np.einsum(expression,array)</code>, where the expression states how the axes are moved. For example, if we change the the $$3 \times 4 \times 5$$ tensor using the einsum, the expression <code>ijk -&gt;kij</code> changes the axes from $$(0,1,2)$$ to $$(2,0,1)$$. The representation of the earlier matrix after this einsum is</p>
    <p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/928868568/image%20%28229%29.png" alt="" class="img-fluid"><br></p>
</div>
<h1>Grammar of Graphics</h1>
<p><br></p>
<p>The following Terminologies are used when describing graphics.</p>
<p></p>
<ul>
    <li><strong>Stat</strong> : A statistic computed from the dataset to allow the data to be summarised compactly in a graphic, for example the mean, value, standard deviation bars, binning values in a histogram, etc.</li>
    <li><strong>Mapping</strong>: A transformation of data into visual values, maps stats and raw values from the dataset to visual values, with a scale:&nbsp;<ul>
            <li><strong>Scale</strong>: A scale specifies the transformation of units in the dataset/stats to visual units, e.g. meter scale to $$x$$ position, altitude to colours. It also specifies the range of values to be mapped.&nbsp;</li>
            <li><strong>Guide</strong>: A visual reference that explains the meaning of the mapping, including the scale and the attribute, e.g. axis, tick marks, labels, colour scales, legends, etc.&nbsp;</li>
        </ul>
    </li>

    <li><strong>Geom</strong>: A geometric representation of the data after it has been mapped, e.g. points, which have attributes shape, size, and colour; lines, which have attributes size, dash styles, thickness; and patches/polygons.&nbsp;</li>
    <li><strong>Coord</strong>: A coordinate system, which connects mapped data onto points on the plane/3D position/etc. The layout of the geoms and guides depends on the coordinate system</li>
    <li><strong>Layer</strong>: One set of geoms, with one mapping to the coordinate system. There can be multiple layers on the same coordinate system, e.g. two stats may be plotted on different layers, but the same coordinate system.</li>
    <li><strong>Facet</strong>: Different views of the same dataset, on a different coordinate system, e.g. two conditions of an experiment may be plotted on two different facets. One facet can have multiple layers, all plotted on the same coordinate system.&nbsp;</li>
    <li><strong>Figure</strong>: A set of one or more facets</li>
    <li><strong>Caption</strong>: A description of the visualisation. A figure must have a caption</li>
</ul>
<p>Altogether, this is called the <strong>Layered Grammar of Graphics.</strong> A pictorial representation is given below&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28148%29.png" alt="" width="800" height="981" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">Here are some examples of figures.&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28149%29.png" alt="" width="1100" height="537" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">The figure on the left has 6 facets, the top right figure has one facet with a single layer, the bottom tight figure has one facet and two layers. A plot should have guides to explain how the stats/raw values from the dataset have been mapped to the coordinate system. This includes:</p>
<p style="text-align:left;"></p>
<ul>
    <li>Labelled axes, with units if present</li>
    <li>Ticks, which indicate subdivisions of an axis with respect to the units, ticks in both axes can be minor or major, indicating smaller or larger subdivisions.</li>
    <li>A legend, to explain what each of the markers/lines means (if more than one is present), these help distinguish the different layers.</li>
    <li>A title explaining what the plot is</li>
</ul>
<p>Moreover, a plot can have:</p>
<p></p>
<ul>
    <li>Grid lines, to help the reader line up the data</li>
    <li>Annotations to point out relevant features.</li>
</ul>
<p>To display data, plot have geoms. As mentioned, these are geometric objects that represent some element of the data, these include:</p>
<p></p>
<ul>
    <li>Lines/curves, which represent continuous functions. They have colour, thickness, and style</li>
    <li>Markers, which represent disconnected points. They have colour, size, and style.&nbsp;</li>
    <li>Patches, which represent a shape with an area (e.g. bars in a bar chart). They have colour, size, and style</li>
</ul>
<p>The anatomy of a figure is given below&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28150%29.png" alt="" width="900" height="916" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">In the figure, we have 3 geoms: one blue line geom, one red line geom, and a point geom. All of these have been plotted on the same coord, they form different layers. There is a legend to distinguish the blue and the red lines, along with tick guides to help identify the $$x$$ and $$y$$ value.</p>
<p style="text-align:left;"><br></p>
<h1>Simple Plots</h1>
<p><br></p>
<p>Most of the time, a plot will only have two variables, one independent and one dependent (which depends <em>on&nbsp;</em>the independent variable). in a function $$y=f(x)$$, $$x$$ is the independent variable, and $$y$$ is the dependent variable, as it depends on the value of $$x$$. As with the notation, the independent variable is plotted on the $$x$$-axis, and the dependent variable on the $$y$$-axis.&nbsp;</p>
<p>For example, assume we have a dataset that compares the number of door head hitting incidents with height. The independent variable is the height, and the dependent variable is the door head hitting incidents. When looking at the dataset comparing height and number of incidents, we would expect as the height increases, as does the number of head hitting incidents.</p>
<p>2D plots are used to visualise two columns of a dataset (stat), and maps one to the $$x$$-axis, and the other to the $$y$$-axis. The following are some common types of 2D plot:&nbsp;</p>
<p></p>
<ul>
    <li>Scatterplot: a scatterplot marks ($$x,y$$) locations with markers. Markers are point geoms<br><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28151%29.png" alt="" width="500" height="254" class="img-fluid atto_image_button_text-bottom"><br></li>
    <li>Bar chart: for ($$x,y$$) in the dataset, it draws bars at $$x$$ proportional to the value of $$y$$. These are patch geoms<br><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28152%29.png" alt="" width="500" height="261" class="img-fluid atto_image_button_text-bottom"><br></li>
    <li>Line plot: Draws line segments between the values of ($$x,y$$) provided. These are line geoms<br><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28154%29.png" alt="" width="500" height="252" class="img-fluid atto_image_button_text-bottom"><br>We can add markers to the line plot to highlight the actual measurements we observed. These are point geoms<br><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28155%29.png" alt="" width="500" height="264" class="img-fluid atto_image_button_text-bottom"><br></li>
    <li>Ribbon plot: Given ($$x,y_{min},y_{max}$$), plot lines ($$x,y_{min}$$) and ($$x,y_{max}$$), and cover the area between $$y_{min}$$ and $$y_{max}$$<br><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28156%29.png" alt="" width="500" height="268" class="img-fluid atto_image_button_text-bottom"><br></li>
</ul>
<p>In a layer, we can combine these geoms. For example, we can use:&nbsp;</p>
<p></p>
<ul>
    <li>line geoms for the trend data</li>
    <li>point geoms for the draw data</li>
    <li>area geoms for the uncertainty&nbsp;</li>
</ul>
<p>An example of such a graph is given below&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28157%29.png" alt="" width="500" height="255" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:center;"><br></p>
<h1>How Not to Plot</h1>
<p><br></p>
<p>We will now look at different examples of visualisation and criticise/improve them. First, assume that we are comparing various doses of vitamin C and how that affects the length of guinea pig teeth. The vitamin C is given either as a tablet or orange juice.</p>
<p>If we just loaded the data and plotted as a line graph, carelessly, we could end up with the following graph.</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28158%29.png" alt="" width="500" height="240" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">This is bad for the following reasons</p>
<p style="text-align:left;"></p>
<ul>
    <li>It has no guides, no axis, no ticks, no labels, no units, no title, and no legend.&nbsp;</li>
    <li>It has bad coords, the dependent variable is on the $$x$$ axis and the independent on the $$y$$ axis</li>
    <li>It has bad mappings, there is no way to distinguish the two sets of data (vitamin C and orange juice)</li>
    <li>It has bad geoms, we have plotted lines even though the data is not continuous&nbsp;</li>
</ul>
<p>We can improve this plot by using scatterplots and distinguishing the two datasets.&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28159%29.png" alt="" width="500" height="261" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">It now has:&nbsp;</p>
<p style="text-align:left;"></p>
<ul>
    <li>Guides: the $$x$$ and the $$y$$ values are labelled</li>
    <li>Good coords: the dependent variable is on the $$y$$-axis, and the independent is on the $$x$$-axis</li>
    <li>Good geoms: a scatterplot is better suited in this case than line plot since the data is not continuous&nbsp;</li>
    <li>Good mapping: it is possible to distinguish the two datasets (even though we do not know which set of data corresponds to which colour)</li>
</ul>
<p>Now if we add some labels and guides to the data.</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28160%29.png" alt="" width="500" height="280" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">We have now added guides:</p>
<p style="text-align:left;"></p>
<ul>
    <li>The plot now has a title</li>
    <li>the axes are labelled with units</li>
    <li>the legend identifies the two layers</li>
    <li>the grid helps with reading the coordinates</li>
    <li>the $$x$$-ticks are placed correctly (precisely where the data is)</li>
    <li>the $$y$$-ticks extend to the origin</li>
</ul><span style="font-size:0.9375rem;">We can add $$x$$ and $$y$$ limits so that the bottom left value is the origin&nbsp;</span>
<p></p>
<p style="text-align:center;"><span style="font-size:0.9375rem;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28161%29.png" alt="" width="500" height="266" class="img-fluid atto_image_button_text-bottom"></span></p>
<p style="text-align:left;"><span style="font-size:0.9375rem;">We should always aim to start at the origin, not doing so can make the data more deceiving. Nonetheless, it should not be doe if the plot cannot be visualised clearly after the transformation.</span></p>
<p style="text-align:left;"><span style="font-size:0.9375rem;">At this point, the two views (supplements and orange juice) have been plotted on the same coord as different layers. We can also plot them on different facets, as shown below</span><br></p>
<p style="text-align:center;"><span style="font-size:0.9375rem;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28162%29.png" alt="" width="700" height="373" class="img-fluid atto_image_button_text-bottom"><br></span></p>
<p style="text-align:left;"><br></p>
<h1>Multiple Visualisation of a Dataset</h1>
<p><br></p>
<p>Now, we will look at multiple visualisations of a dataset with another example. Assume we have a dataset that gives the gas usage in the UK from 1960 to 1986, one reading every 3 months. We can create a simple line graph out of it, with markers to show raw data.&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28163%29.png" alt="" width="400" height="196" class="img-fluid atto_image_button_text-bottom"><br></p>
<p>We can simplify this graph by using the general pattern and smoothening the curve, e.g. using a 3 year running mean. This is a stat.&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28164%29.png" alt="" width="400" height="205" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">We can also split the data into multiple layers, breaking it with respect to the seasons.</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28165%29.png" alt="" width="400" height="207" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">Instead of a layered figure, we can also use a faceted figure.</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28166%29.png" alt="" width="600" height="589" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">By using the same axis scale in the 4 plots, comparing each plot becomes easy. We can also transform the plot into showing variation among the years for each season in a layered plot.</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28169%29.png" alt="" width="400" height="214" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">We can use a ribbon plot to show the mean and range for each year</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28170%29.png" alt="" width="400" height="213" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">Instead of a ribbon, we can use a scatter plot to show the maximum and minimum usage each year against the median consumption.&nbsp;</p>
<p style="text-align:left;">Finally, we can product a cumulative line graph to show the increase in CO2 product. In this case, we are plotting a stat instead of the dataset.&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28171%29.png" alt="" width="400" height="206" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:center;"><br></p>
<h1>Stats</h1>
<p><br></p>
<p>A stat is a statistic of the dataset, i.e. a transformation of the data. It is used to summarise the data. Common examples are:&nbsp;</p>
<p>
</p>
<ul>
    <li>Aggregation summary statistics, e.g. central tendency (i.e. mean and median), deviation (i.e. standard deviation, min/max, interquartile)</li>
    <li>Binning operations, e.g. characterising the data into discrete bins and counting the number of elements in each bin&nbsp;</li>
    <li>Smoothing and regression, e.g.&nbsp; approximating function to datasets, such as linear regression, fitting a line through data.&nbsp;</li>
</ul>
<p><br></p>
<h2>Histograms</h2>
<p>Histograms combine binning operations (a stat) and a 2D bar chart (a geom). They count the number of value that fall in the given range. We can visualise how the data is distributed across the bins. The bars within the histogram do not have space, the bind represent contiguous divisions of the input space. An example of a histogram is given below.&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28172%29.png" alt="" width="400" height="210" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">We show how the histogram counts value in bins</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28178%29.png" alt="" width="400" height="215" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">The display of a histogram depends on the choice of the bins. If there are too many bins, the count for most of the values will be low, it will be difficult to see the trend of the data</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28179%29.png" alt="" width="400" height="218" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">Instead, if there are too few bins, the count for most of the values will be high, the data will be smoothened and details will be lost</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28180%29.png" alt="" width="400" height="220" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">Also, if the bins are badly chosen, then they will not capture the meaningful part of the data; many of the bins will be empty&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28181%29.png" alt="" width="400" height="224" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">We can use binning in multiple dimensions as well, though it is difficult to grasp the data in that case</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28183%29.png" alt="" width="400" height="230" class="img-fluid atto_image_button_text-bottom"><br></p>
<p><br></p>
<h2>Ranking Operations</h2>
<p>For a 1D vector dataset, we can plot it against its rank within the array. The stat here is conversion from the value to the rank.&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28184%29.png" alt="" width="400" height="222" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">In the plot above, we have sorted the usage of gas per quarter, and plotted it against its rank, the index it has in the sorted array. The rank plot can be a scatterplot as well.</p>
<p style="text-align:center;">&nbsp;<img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28185%29.png" alt="" width="400" height="201" class="img-fluid atto_image_button_text-bottom"></p>
<p style="text-align:left;"><br></p>
<h2>Aggregate Summaries</h2>
<p style="text-align:left;">It is common to show the range of data (e.g. showing the min/max, standard deviation, or the interquartile range). These are often ranges of groupings of the data (e.g. conditions in the experiment). Below is a faceted view of the mean, standard deviation, min, max, centre of range, and median of the gas example</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28186%29.png" alt="" width="800" height="531" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">The 3 facets above show the labelled data. It is more common, however, to show all of this data in one geom, a box plot.&nbsp;</p>
<p style="text-align:left;">A box plot computes multiple stats of a dataset and combines them into one geom. It is great for seeing the distribution of the values in the dataset. Usually, the following are shown:&nbsp;</p>
<p style="text-align:left;"></p>
<ul>
    <li>The interquartile range (the range between 25% and 75%), this is shown as a box.</li>
    <li>The median, shown as a horizontal line</li>
    <li>The extrema (often representing the 2.5 ad 98.5 percentiles), shown as whiskers</li>
    <li>The outliers (outside the extrema) shown as crosses or circles</li>
</ul>
<p>An example box plot is given below&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28187%29.png" alt="" width="600" height="325" class="img-fluid atto_image_button_text-bottom"><br></p>
<p>Normally, they are used to plot different conditions and compare them.&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28188%29.png" alt="" width="600" height="317" class="img-fluid atto_image_button_text-bottom"><br></p>
<p>Instead of using box plots, we can use violin plots to represent the data more precisely. It plots the full distribution of the data, instead of a simple box. It uses the smoothing technique called kernel density estimate.&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28189%29.png" alt="" width="600" height="323" class="img-fluid atto_image_button_text-bottom"><br></p>
<p><br></p>
<h2>Regression and Smoothing</h2>
<p>Regression finds an approximate function that closely matches the data. This function is usually simple. The most common type of regression is linear regression, i.e. fitting a line through the data. These are an important class of stats for proposing hypotheses to explain the pattern we see in data.</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28190%29.png" alt="" width="600" height="325" class="img-fluid atto_image_button_text-bottom"><br></p>
<p>Instead of having a simple line, we can have a moving average, it finds a simpler version of the data that hides changes within the dataset. This is smoothening of the data. A good smoothening reveals the important properties, while hiding the irrelevant ones.</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28191%29.png" alt="" width="600" height="303" class="img-fluid atto_image_button_text-bottom"><br></p>
<p><br></p>
<h1>Geoms and Colour Scales</h1>
<p><br></p>
<h2>Markers</h2>
<p>Markers are geoms that represent points in a coordinate system. These typically represent the raw data. They can just be points on the graph, but we can vary them to convey more information.&nbsp;</p>
<p>We can use different markers, i.e. different geoms, to distinguish different layers on the coordinate system. We can also vary the shape and colour of a marker</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28192%29.png" alt="" width="600" height="322" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28193%29.png" alt="" width="600" height="324" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">It is important to choose colours wisely. It is possible that the data might get printed in black and white. Moreover a lot of people have some form of colour blindness, so we should avoid separating layers using only colour</p>
<p style="text-align:left;">Instead of identifying layers, we can use markers to display another attribute. For example, we can vary the colour and the scale of the marker to convey some extra information. To see this, consider the following graph that shows earthquakes in Fiji.&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28194%29.png" alt="" width="400" height="381" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">We can use the size of the marker to display the magnitude of the earthquake.&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28195%29.png" alt="" width="400" height="392" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">In the plot above, we had 3 variables, longitude, latitude, and the magnitude of the earthquake. We can use colour to represent a further variable, depth of the earthquake.</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28196%29.png" alt="" width="400" height="405" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">Colouring markers is done using a colour map, it maps scalars to colours. We present a colour bar label with the data, this is a guide, used for aesthetic mapping beyond the 2 variable plot.&nbsp;</p>
<p style="text-align:left;">While there are many possible colour maps possible, there are few good choices. In general, there are 2 versions, one for unsigned/continuous values and one for distinct values.</p>
<p style="text-align:left;">If the data is composed of positive scalars (e.g. height of individuals), then we should use a colour map that varies monotonically with respect to brightness. That is, as the attribute increases, the colour gets lighter/darker consistently. In matplotlib, <code>vidris</code> and <code>magma</code> are good options for this. They are perceptually uniform, i.e. a change in value corresponds to a perceptually uniform change in colour intensity across the whole scale. It is possible to use grayscale/monochrome colour maps, but colours with brightness and hue are easier to interpret. As the data value increases, the visual brightness should also increase. A constant interval increase in data should lead to a perceptually constant increase in colour intensity.</p>
<p style="text-align:left;">Below, we have a colour mapping from <code>vidris</code> and <code>jet</code></p>
<p style="text-align:center;"><code><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28197%29.png" alt="" width="500" height="272" class="img-fluid atto_image_button_text-bottom"><br></code></p>
<p style="text-align:center;"><code><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28198%29.png" alt="" width="500" height="274" class="img-fluid atto_image_button_text-bottom"><br></code></p>
<p style="text-align:left;">Jet is bad because it introduces false contours in visualisation, the markers do not have monotonic brightness. If the data is signed, (positive and negative), then the colour map should diverge around 0, and be monotonic in brightness around 0. A good example is given below&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28199%29.png" alt="" width="500" height="272" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">The following on the other hand is a bad colour map since it does not converge at 0.</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28201%29.png" alt="" width="500" height="260" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">It should be possible to go back from the colour to value using the label, the colour map guide is essential for this.&nbsp;</p>
<p style="text-align:left;"><br></p>
<h2>Lines</h2>
<p>Lines are geoms that connect points in the dataset/stat. A line only makes sense if data can exist between two datapoints, i.e. there is a continuum of values. It is a good choice when we have two arrays which represent samples from an apparent continuous function. Line geoms can have variable thickness and colour, as shown in the example below.&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28202%29.png" alt="" width="500" height="260" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">We can also vary dash patterns. This might be combined with colour changed to help those with colour blindness to interpret the figure. Nonetheless, it is a bad idea to use more than 4 dash patterns.</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28203%29.png" alt="" width="500" height="271" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">If the data doesn't make sense as a line graph, for example coin-tosses, then we can use the staircase&nbsp; approach to represent the data</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28204%29.png" alt="" width="400" height="214" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">As we can see above, the values here remain constant until the next observation, when it jumps to that position. There is no linear interpolation in the middle.&nbsp;&nbsp;</p>
<p style="text-align:left;">If the measurements of $$x$$ are discrete (e.g. conditions in an experiment), we should use a bar chart instead of a line chart</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28205%29.png" alt="" width="500" height="261" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;"><br></p>
<h2>Alphas and Transparency</h2>
<p>We can render geoms with transparency, so that values behind can show clearly. This is called <strong>opacity</strong>&nbsp;(the inverse of transparency) or <strong>alpha</strong>. They can be used when a large number of geoms overlap (e.g. on a dense scatterplot), or to (de)emphasise some geoms, along with line thickness. We should not use transparency a lot since it can make the data hard to see. Nonetheless, it can allow us to focus on certain geoms.&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28206%29.png" alt="" width="250" height="218" class="img-fluid atto_image_button_text-bottom"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28207%29.png" alt="" width="250" height="216" class="img-fluid atto_image_button_text-bottom"><br></p>
<p><br></p>
<h1>Coords</h1>
<p><br></p>
<p>We can specify visual units to present data with different shape.<br></p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28208%29.png" alt="" width="175" height="110" class="img-fluid atto_image_button_text-bottom">
    <img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28210%29.png" alt="" width="340" height="135" class="img-fluid atto_image_button_text-bottom"><br>
</p>
<p>Similarly, we can use axis limits to specify the data unit range which is mapped
    on.<br></p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28212%29.png" alt="" width="500" height="257" class="img-fluid atto_image_button_text-bottom"><br></p>
<p>Also, we can change the aspect ratio of the coordinate system. This can be
    used to ensure that images do not get stretched out/squashed<br></p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28213%29.png" alt="" width="700" height="210" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">A coordinate system projects data onto the 2D plane. We have only looked
    at the Cartesian plane until now, but we can transform it into log/polar coordinates, as we see fit.<br></p>
<p style="text-align:left;">If the data has large spans of magnitude, we should plot it in log coordinates.
    Log coordinates can be used in $$x$$-axis, $$y$$-axis, or both. If we have log on $$x$$ or $$y$$-axis, it is called semilog $$x$$ or semilog $$y$$. If we use it in both axes, it is called
    log-log.<br></p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28214%29.png" alt="" width="700" height="327" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">A function of the form $$f(x) = x^k$$ looks linear in a log-log plot</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28215%29.png" alt="" width="700" height="372" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">We cannot use log plots in every case- logarithms are only dened for strictly
    positive numbers. So, we cannot plot signed data on a log scale (without
    shifting it to strictly positive values). There are modified versions of the log
    scale that ignore some area around 0, and plot $$ \log(\text{abs}(x))*\text{sign}(x) $$.This is called the symmetric logarithm, or symlog. The area around 0 that is cut out is plotted linear, which distorts the plot.&nbsp;<br></p>
<p style="text-align:left;">For angular measurements, we can use polar coordinates. These have axes, radius $$r$$ and angle $$\theta$$</p>
<p style="text-align:left;"><br></p>
<h1>Facets and Layers</h1>
<p><br></p>
<p>Multiple geoms can be rendered in the same visualisation as either:&nbsp;</p>
<p></p>
<ul>
    <li>distinct layers on the same coordinate</li>
    <li>distinct facets on separate coordinate systems (with separate scales and guides</li>
</ul>
<p>Layering is more appropriate when geoms are closely related, and the data mappings are in the same units. We need a legend (a guide) to distinguish the two layers.</p>
<p>For example, we can plot the price of wheat and weekly wage in the same plot, they have the same units.&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28216%29.png" alt="" width="600" height="323" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">We should not plot the ratio of the price and the wage in the same plot. Instead, they should be plotted in two different facets.&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28217%29.png" alt="" width="700" height="368" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">We could also use double $$y$$-axes. There, the $$x$$ mapping layer is common for all the dataset, but there are 2 $$y$$-mapping layers. We have two slightly different coordinate systems layered over each other. This should be avoided as it can be confusing to interpret.&nbsp;</p>
<p style="text-align:left;">Like above, it is much better to use different facets, separate coordinate systems for separate aspects of the dataset. Facets need not share anything in common. Nonetheless, if they show the same attribute, then it is a good idea to use the same scale (if possible) to make comparisons easy between the coords.</p>
<p style="text-align:left;">There are many ways to lay out facets in a figure</p>
<p style="text-align:left;"><br></p>
<h1>Communicating Uncertainty&nbsp;</h1>
<p><br></p>
<p>A figure must communicate uncertainty correctly and not mislead the reader. It is common to have observation error, so the samples maintained may not represent the true values. For example, the reading on a thermometer is not the true temperature of the air.&nbsp;</p>
<p>There are other possible sources of error, e.g. rounding errors in numerical simulations, uncertainty over the right choice/its parameters. We can use stats such as standard deviation and interquartile range to show summaries of a collection of data. For example, in the vitamin C example, we can consider averages over different doses.</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28218%29.png" alt="" width="600" height="310" class="img-fluid atto_image_button_text-bottom"><br></p>
<p>Although the data above shows the mean, the raw data was a sample of measurements, so we should represent the variation of data, e.g. using error bars.</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28219%29.png" alt="" width="600" height="320" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">There are many different choices for error bars, e.g.:</p>
<p style="text-align:left;"></p>
<ul>
    <li>standard deviation</li>
    <li>standard error</li>
    <li>confidence interval (e.g. 95%)</li>
    <li>non-parametric intervals (e.g. interquartile range)</li>
</ul>
<p>Box plots are a good way of visualising the spread of values in a compact form. An alternative is to jitter the points on the $$x$$-axis slightly. This is called a dot plot</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/111360742/image%20%28220%29.png" alt="" width="600" height="332" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">This is good to use with large collections of data.</p>
<p><br></p>
<h1>Vector Spaces</h1>
<p><br></p>
<p>Vectors are ordered tuples of real numbers;</p>
<p>&nbsp; &nbsp; &nbsp; &nbsp; \( [ x_1~~x_2~~ ...~~ x_n] \)<br></p>
<p>A vector has fixed dimension n. This is the length of the tuple. For example \( [5~~3~~7] \) is a vector in&nbsp;\( \mathbb{R}^3 \). We use the following notation to denote sets:</p>
<p></p>
<ul>
    <li>\( \mathbb{R} \) - The set of real numbers<br></li>
    <li>\( \mathbb{R}_{\geq 0} \) - The set of non-negative real numbers</li>
    <li><span style="font-size:0.9375rem;">\( \mathbb{R}^n \) - The set of tuples of real numbers of length \(n\)</span></li>
    <li>\( \mathbb{R}^{n \times m} \) - The set of 2D arrays (i.e. matrices) with \(n\) rows and \(m\) columns&nbsp;<br></li>
    <li>\( \mathbb{R}^n ,\mathbb{R}^n&nbsp;\rightarrow&nbsp;\mathbb{R} \) - An operation that maps two vectors in (\\mathbb{R}^n\) to \(\mathbb{R}\)<br></li>
</ul>

<h2>Vector Operations</h2>
<p><span style="color:rgb(73,80,87);font-size:0.9375rem;font-weight:400;"><br></span><span style="color:rgb(73,80,87);font-size:0.9375rem;font-weight:400;">The set \( \mathbb{R}^n \) is an \(n\)-dimensional real vector space (i.e tuples containing&nbsp; \(n\) elements). They have the following operations:&nbsp;</span></p>
<ul>
    <li>Scalar multiplication, i.e. multiplying a vector with a real number. If x = \( [ x_1~~x_2~~ ...~~ x_n] \), then \( ax[ ax_1~~ax_2~~...~~ax_n]\). It is an operation&nbsp;\( \mathbb{R}^n ,\mathbb{R}^n&nbsp;\rightarrow&nbsp;\mathbb{R} \)&nbsp;</li>
</ul>
<ul>
    <li>Vector addition, i.e. adding two vectors. if x&nbsp; =&nbsp;\( [ x_1~~x_2~~ ...~~ x_n] \), and y =&nbsp;\( [ y_1~~y_2~~ ...~~ y_n] \), then \(x+y = [x_1+y_2~~x_2+y_2~~...~~x_n+y_n]\).&nbsp;&nbsp;It is an operation of the form&nbsp;\( \mathbb{R}^n ,\mathbb{R}^n&nbsp;\rightarrow&nbsp;\mathbb{R} \). Two vectors with different dimensions cannot be added</li>
</ul>
<ul>
    <li>Norm, which measures the length of a vector. It is an operation of the form&nbsp; \( \mathbb{R}^n \rightarrow \mathbb{R}_{\geq 0}\)</li>
</ul>
<ul>
    <li>Inner product, which allows the angles of two vectors to be compared. If two vectors are orthogonal, their inner product is 0. In \( \mathbb{R}^n \), the inner product \( x \cdot \ y = x_1y_1 + x_2y_2 + ... + x_ny_n \). It is an operation of the form&nbsp;\( \mathbb{R}^n ,\mathbb{R}^n&nbsp;\rightarrow&nbsp;\mathbb{R} \)&nbsp;</li>
</ul>
<p><br></p>
<p>A normed vector space (i.e. a vector space with a norm) is a topological vector space. This implies that the space is continuous, i.e. it makes sense to talk about vectors being close together/ a vector having a neighborhood around it. If the vector space has an inner product defined, it is an inner product space - it makes sense to talk about an angle between the vectors.&nbsp;</p>
<p>Vectors can be thought of as: points in space, arrows pointing from the origin, or tuples of numbers. Practically vectors <strong>are </strong>tuples of numbers, but it is a good idea to think of them as points - they represent points from data spaces. Matrices on the other hand, represent operations of data-matrices wrap spaces. We can represent vectors and matrices as arrays</p>
<p></p><br>
<p></p>
<p></p>
<h1>Using Vectors</h1>
<p><br></p>
<p>Vectors are used a lot in data science. This is because vectors can be:&nbsp;</p>
<p>
</p>
<ul>
    <li>composed (by addition)</li>
    <li>compared (using the norm/inner product)</li>
    <li>weighted (by scalar multiplication)</li>
</ul>
<p>For this reason, they also represent transformations we want to do to some data. Because ndarrays are very efficient, their usage is in turn efficient and concise.&nbsp;</p>
<p>Most datasets are 2D tables, i.e. lists of vectors. A row vector represents an observation, whereas a column vector represents an element of the vector from all the observations. For example, consider the following table.</p>
<p><br>
</p>
<table>
    <caption></caption>
    <thead>
        <tr>
            <th scope="col">Heart Rate</th>
            <th scope="col">Systolic</th>
            <th scope="col">Diastolic</th>
            <th scope="col">vo2&nbsp;</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>67</td>
            <td>110</td>
            <td>72</td>
            <td>98&nbsp;</td>
        </tr>
        <tr>
            <td>65</td>
            <td>111</td>
            <td>70</td>
            <td>98&nbsp;<br></td>
        </tr>
        <tr>
            <td>&nbsp;64</td>
            <td>110</td>
            <td>69&nbsp;</td>
            <td>97&nbsp;<br></td>
        </tr>
    </tbody>
</table><br>Each row of data lives in \( \mathbb{R}^4 \), and is a set of physiological measurements. The matrix of data is a sequence of vectors in the same vector space, i.e. we can compare/make geometric statements about the entire data.&nbsp;<br>
<p><br>
</p>
<p></p>
<h2>Geometric Operations</h2>
<p>Standard geometric operations we can perform in \(\mathbb{R}^3\) or \(\mathbb{R}^2\) are:&nbsp;</p>
<p>
</p>
<ul>
    <li>Scaling</li>
    <li>Rotation</li>
    <li>Flipping (mirroring)</li>
    <li>Translation (shifting)</li>
</ul>
<p>GPUs evolved from devices that can do these sorts of geometric operations. A vector space allows all geometry to have a common representation, and matrices can transform vectors.</p>
<p>Graphical pipelines process everything (position, coordinates, colours) as a large array of vectors. Programming for graphics on GPUs largely involves packing data into a low-dimensional vector arrays (on the CPU) then processing them quickly on the GPU using a shader language. Shader languages, like HLSL and GLSL) have special data types and operators for working with low-level dimensional vectors</p>
<p><br></p>
<p></p>
<h2>Applications in Machine Learning</h2>
<br>
<p>Machine learning relies on vector representation. Essentially, a machine learning process transforms some data into feature vectors, and then transforms the feature vectors into a prediction. The feature vectors are encodings of the data in the vector space. Feature transforms are the operations that take the raw data from the dataset and output feature vectors.&nbsp;</p>
<p>One class of machine learning algorithms is k-nearest neighbours. Here, we have some training set of data - pairs of x\(_i\) (the feature vector) and y\(_i\)(label). When we classify some new feature, we compute \(k\) nearest vectors, using a norm to compute the distance. The prediction is then a class label that occurs most times among these \(k\) neighbours. The value \(k\) is preset/constant.&nbsp;</p>
<p>We expect nearby vectors to share common properties. So, to find a property for a vector we don't know, we look at the properties that its neighbours have.</p>
<p><br></p>
<h2>Applications in Image Compression</h2>
<p><br></p>
<p>Images can be represented as a 2D array of brightness. Groups of pixels can be unraveled to a vector, e.g. an 8 by 8 patch of pixels can be unraveled to a vector in \(\mathbb{R}^{64}\).&nbsp;</p>
<p>Image compression can be done by splitting an image into patches and then unravelling each patch into vectors: \(x_1,x_2,...x_n\). We can then cluster the vectors to find a small number of vectors ( \(y_1,y_2,...,y_m\)) that approximate nearby vectors. Instead of storing the image, we store few representative vectors y\(i\), called the codebook, and the other areas in the image are indices of the closest matching vector in the codebook (i.e. y\(_j\) such that \(||x_i - y_j||\) is minimised). This&nbsp; is vector quantisation - it quantises the vector space into a small number of discrete regions. The process maps visual similarity onto spatial relationships.&nbsp;</p>
<p><br></p>
<h1>Norms, Means, and Interpolation</h1>
<p><br></p>
<p>We have already seen elementwise addition and scalar multiplication on arrays/vectors. These operations allow us to take a weighted sum of vectors:&nbsp;\( \leftthreetimes{x_1}+\leftthreetimes{x_2}+...+\leftthreetimes{x_n}

    \) where all vectors x\(_i\) are from the same vector space( i.e. have the same dimension)</p>
<p><br></p>
<h2>Linear Interpolation</h2>
<p>We can also linearly interpolate between two vectors. This is given by&nbsp;</p>
<p style="text-align:center;">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\( \texttt{lerp}(x,y,\alpha) = (1-\alpha)x+\alpha y
    \)</p>
<p>This is governed by the value of \(\alpha\). As the value of \(\alpha\) goes from 0 to 1, we get a smooth straight line from x to y. The image below illustrates this.</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/9266571/image.png" alt="linear interp" width="400" height="412" class="img-fluid atto_image_button_text-bottom"><br></p>
<p><br></p>
<h2>Norms</h2>
<p>Not every vector space has a natural concept of distance, but \(\mathbb{R}^n\) has a distance defined. The Euclidean length of a vector \(\texttt{x}\) is&nbsp;</p>
<p style="text-align:center;">\( ||\texttt{x}||_2 = \sqrt{x_0^2 + x_1^2 + ... + x_n^2} \)<br></p>
<p>It corresponds to the radius of the hyper sphere that would just touch the position specified by the vector. There are different norms in \(\mathbb{R}^n\). For example, the \(L_p\)/Minkowski norms are given by&nbsp;</p>
<p style="text-align:center;">\( ||\texttt{x}||_p = \sqrt{x_0^p + x_1^p + ... + x_n^p} = \sqrt[p]{\displaystyle\sum_{i=0}^{n}{x^p_i}} \)<br></p>
<p>Some common norms are:&nbsp;</p>
<p></p>
<ul>
    <li>The \(l_2\) norm (or the Euclidean norm) is the normal distance. Geometric the length corresponds to the radius of a sphere at the origin just touching the point</li>
    <li>The \(l_1\) norm (or the taxicab norm) gives the sum of absolute values. It is used to measure distances in high dimensions, or on grids. Geometrically, the length corresponds to the axis-aligned steps to get to that point</li>
    <li>The \(l_&nbsp;\infty \) norm gives the maximum element in the vector. It is used to capture maximum activation or exclusion. Geometrically, the length corresponds to the length of a cube centered at the origin just touching this point;&nbsp;</li>
</ul>
<p>A unit vector has norm 1. We can normalise a vector \( \texttt{x} \) by scaling it by \(\frac{1}{||\texttt{x}||}\). We typically talk about a unit vector with respect to the Euclidean norm. The Euclidean normalisation of random vectors is \(\mathbb{R}^2\) is given below&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/9266571/image%20%281%29.png" alt="" width="300" height="491" class="img-fluid atto_image_button_text-bottom"><br></p>
<p>The \(L_{\infty}\) normalisation of random vectors in \(\mathbb{R}^2\) is given below&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/9266571/image%20%282%29.png" alt="l infty" width="300" height="314" class="img-fluid atto_image_button_text-bottom"><br></p>
<p><br></p>
<h2>Inner product</h2>
<p><br></p>
<p>An inner product \( ( \mathbb{R}^n,\mathbb{R}^n )\ \rightarrow \mathbb{R}\) measures the angle between two real vectors. It is related to the cosine function:&nbsp;</p>
<p style="text-align:center;">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;\( \cos(\theta) = \frac{\texttt{x}\cdot \texttt{y}}{||\texttt{x}||~||\texttt{y}||} \)</p>
<p>The dot product of two vectors in \(\mathbb{R}^n\) is given by&nbsp;</p>
<p style="text-align:center;">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\( \texttt{x} \cdot \texttt{y} = \displaystyle\sum_{i=0}^{n}{x_iy_i} \)</p>
<p>It is the sum of the elementwise products. Note that the vectors must have the same number of dimensions to perform this operation.&nbsp;</p>
<p>We can use the inner product to compare vectors of different magnitudes. It does not depend on their magnitude, just their direction. For example, it is widely used in information retrieval to compare document vectors which have wildly different magnitudes for documents of different lengths&nbsp;</p>
<p><br></p>
<h2>Mean Vector</h2>
<p><br></p>
<p>The mean vector is the generalisation of the mean scalar, i.e.&nbsp;</p>
<p style="text-align:center;">\( \displaystyle\sum_{i=0}^{N}{\frac{1}{N}x_i} \)<br></p>
<p style="text-align:left;">The mean represents the geometric centroid of the vectors, and captures the centre of mass of the vectors. By subtracting the mean from the dataset, we can center it with zero mean.&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/9266571/image%20%283%29.png" alt="dwiuduwahidaiwuhduiwagiu" width="300" height="301" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">There is no simple and direct algorithm to compute the median in high dimensions. This is because the operation cannot be decomposed into scalar multiplication and addition</p>
<p><br></p>
<h1>Vectors in Higher Dimensions</h1>
<p><br>Data science typically involves vectors in a lot of dimensions. For instance, a 512x512 image lives in \(\mathbb{R}^{262144}\). We can consider one data point to be a vector of measurements. Our choice of the measurements will impact the performance and behaviour of the algorithms&nbsp;</p>
<p>Geometry in higher dimensions is counter-intuitive. This is because the volume of space increases exponentially as the dimension increases. This means that there is a lot of empty space in higher dimensions. In particular, if data is sparse, it can be difficult to generalise in high-dimensional spaces. Many algorithms that work well with few dimensions do not generalise to higher dimension. This is due to the curse of dimensionality.&nbsp;</p>
<p>Consider the following histogram that represents data about temperature and humidity&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/9266571/image%20%284%29.png" alt="uyuysetfybergtwqi" width="450" height="304" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">In the histogram above, there are 20 bins in each dimension, for 400 bins total. Each bin only gets around 500 measurements, and in practice, most bins are empty and a few are heavily populated. We cannot use histograms in higher dimensions because of the curse of dimensionality,&nbsp;</p>
<p style="text-align:left;">If we had 10 different measurements (air temperature, air humidity, latitude, longitude, wind speed, wind direction, precipitation, time of day, solar power, sea temperature) and we wanted to subdivide them into 20 bins each, we would need a histogram with \(20^{10}\) bins, over 10 trillion bins. This is the curse of dimensionality, as dimensionality increases, the generalisation gets harder exponentially.&nbsp;</p>
<p style="text-align:left;"><br></p>
<h2>Paradoxes in Higher Dimensions</h2>
<p>We will try to find the volume of a hypersphere within a cube as the dimension changes</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/9266571/image%20%285%29.png" alt="fkuhfuystfufisuye" width="450" height="283" class="img-fluid atto_image_button_text-bottom"><br></p>
<p>So, as the dimension increases, the volume of the sphere decreases, most of the values are in the corners.</p>
<p>Moreover, if we randomly place points within the cube of high dimension, then most of the points are going to be outside of the cube. Furthermore, if we draw a line between two points on a hypercube, the points on the line still end up on the edge of the space. So, the distance between any two points using the Euclidean norm will almost always be the same. The figure below shows this.</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/9266571/image%20%286%29.png" alt="f" width="400" height="259" class="img-fluid atto_image_button_text-bottom"><br></p>
<p>If we instead look at the 2D case, we get an expected result</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/9266571/image%20%287%29.png" alt=";" width="350" height="273" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">We expect the distance between any two points to be spread between some vectors, the distance could be close to 0. However, this isn't true in higher dimensions, as we can see above. Other norms, (e.g. \(L_1 \) and \(L_{\infty}\)) are less sensitive to the curse, but they aren't perfect.&nbsp;</p>
<p><br><br></p>
<h1>Matrices</h1>
<p><br></p>
<p>Matrices are 2D arrays of reals: \(\mathbb{R}^{n \times m}\) (\(n\) rows and \(m\) columns). Vectors represent points in space; matrices represent operations to transform vectors. Operations defined by matrices are rigid transformations</p>
<p><br></p>
<h2>Matrix Operations</h2>
<p>Matrices can be:</p>
<p></p>
<ul>
    <li>Added/Subtracted:&nbsp; \(( \mathbb{R}^{n \times m}, \mathbb{R}^{n \times m} \rightarrow \mathbb{R}^{n \times m})\)&nbsp;</li>
    <li>Scaled (by a scalar): \((&nbsp;\mathbb{R}^{n \times m},\mathbb{R} \rightarrow&nbsp;&nbsp;\mathbb{R}^{n \times m})\)</li>
    <li>Transposed (rows and columns flipped):&nbsp; \(&nbsp;\mathbb{R}^{n \times m} \rightarrow&nbsp;&nbsp;\mathbb{R}^{m \times n}\)</li>
    <li>Applied to vectors: \( (\mathbb{R}^{n \times m},&nbsp;\mathbb{R}^{m}) \rightarrow&nbsp;&nbsp;\mathbb{R}^{n} \)</li>
    <li>Multiplied (i.e. which composes the transformations) : \( ( \mathbb{R}^{p \times q},&nbsp;\mathbb{R}^{q \times r}) \rightarrow&nbsp;&nbsp;\mathbb{R}^{p \times r}\)</li>
</ul>
<p>Matrices represent linear maps of vectors. That is, a matrix is like a function that can be applied to a vector by multiplication, i.e. \(f(x) = Ax\). A matrix representation presents the function in a very compact way. If we have an \(n \times m\) matrix, then the matrix \(A\) represents a function \((\mathbb{R}^m) \rightarrow \mathbb{R}^{n}\) such that:</p>
<p></p>
<ul>
    <li>All straight lines remain straight</li>
    <li>All parallel lines remain parallel</li>
    <li>The origin does not move</li>
</ul>
<p>This is equivalent to saying that:&nbsp;</p>
<p style="text-align:center;">\( f(x+y) = f(x) + f(y) = A(x + y) = A(x) + A(y) \\</p>
<p style="text-align:center;">f(cx) = cf(x) = A(cx) = cA(x) \)<br></p>
<p style="text-align:left;">This property is linearity, and matrices represent linear maps. A linear map is a function \(f : \mathbb{R}^m \rightarrow \mathbb{R}^n \) that satisfies the the linearity conditions. An \(n \times n\) matrix maps from the vector space to itself \((\mathbb{R}^n) \rightarrow \mathbb{R}^n\). This is called a linear transform. If a map satisfies \(AAx = Ax\), i.e. \(f(f(x)) = f(x)\), then it is called a linear projection, i.e. it projects 3D points into a 2D plane. Every linear map of vectors can be written as a real matrix.&nbsp;</p>
<p style="text-align:left;">We will now look at some examples of matrix transformations in 2D:</p>
<p style="text-align:left;"></p>
<ul>
    <li style="text-align:left;">The identity transformation is given by the matrix&nbsp;\( \begin{bmatrix} 1~~0 \\ 0~~1 \end{bmatrix} \). The following is its transformation,<img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/9266571/image%20%288%29.png" alt="" width="500" height="333" class="img-fluid atto_image_button_text-bottom"></li>
    <li style="text-align:left;">The matrix given by&nbsp;\( \begin{bmatrix} 0.5~~0 \\ 0~~0.5 \end{bmatrix} \) uniformly scales the plane. The following is its transformation<img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/9266571/image%20%289%29.png" alt=";" width="500" height="352" class="atto_image_button_text-bottom"></li>
    <li style="text-align:left;">The matrix given by&nbsp;\( \begin{bmatrix} 0.5~~0 \\ 0~~1 \end{bmatrix} \) scales the plane in a non-uniform manner (only the x axis). The following is its transformation&nbsp;&nbsp;<br><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/9266571/image%20%2811%29.png" alt="" width="450" height="309" class="img-fluid atto_image_button_text-bottom"></li>
    <li style="text-align:left;">The matrix given by&nbsp;\( \begin{bmatrix} 0~~-1 \\ 1~~0 \end{bmatrix} \) rotates the plane by 90°. The following is its transformation<img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/9266571/image%20%2812%29.png" alt="" width="500" height="304" class="img-fluid atto_image_button_text-bottom"></li>
    <li style="text-align:left;">The matrix given by&nbsp;\( \begin{bmatrix} -1~~0 \\ 0~~1 \end{bmatrix} \) flips the \(x\)-axis. The following is its transformation<img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/9266571/image%20%2813%29.png" alt="" width="500" height="316" class="img-fluid atto_image_button_text-bottom"></li>
    <li style="text-align:left;">the matrix given by&nbsp;\( \begin{bmatrix} 0.15~~0.75 \\ 0.5~~0.8 \end{bmatrix} \) has a shearing effect to the plane. The following is its transformation&nbsp;<img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/9266571/image%20%2814%29.png" alt="" width="500" height="305" class="img-fluid atto_image_button_text-bottom"></li>
</ul>
<p>We can apply a matrix to a vector. An application takes a weighted sum of the elements (linear combinations) within a vector. If we have an \(n \times m\) matrix, then it is a function that maps an \(m\)-dimensional vector space to an \(n\)-dimensional vector space.&nbsp;</p>
<p>We can also multiply matrices. Multiplication composes the effect of the matrices, i.e. \(BA = g(f(x)) \) where \(g(x) = Bx ~~ \text{and}&nbsp; ~~ f(x) = Ax \). The product \(AB\) only makes sense if \( A~~\text{is}~~p \times q~~\text{and}~~B~~\text{is}~~q \times r \); \(AB\) will be \(p\times r\). The product \(AB\) is \(B\) left-multiply \(A\), and \(A\) right-multiply \(B\)&nbsp;</p>
<p>Matrix multiplication has complexity \(O(pqr)\). If the matrices are square, then it is \(O(n^3)\). There are special forms of matrices which can be multiplied much faster, there are optimised/accelerated algorithms, but they are always worse than \(O(n^2)\). Most accelerated algorithms are impractical for all but the largest matrices because they have enormous constant overhead.&nbsp;</p>
<p>We illustrate matrix multiplication by combining two transformations; the matrix that rotates the 2D plane by 30°; and the matrix that scales the \(x\)-axis by 1/2 and leaves the \(y\)-axis. If we first rotate then scale, we get the following transformation.</p>
<p><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/9266571/image%20%2815%29.png" alt="" width="500" height="300" class="img-fluid atto_image_button_text-bottom"><br></p>
<p>If we instead scale then rotate, we get the following transformation</p>
<p><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/9266571/image%20%2816%29.png" alt="" width="500" height="322" class="img-fluid atto_image_button_text-bottom"><br></p>
<p>As we can see, composition of two matrices depends on the order, i,e, \( AB \neq BA \). We say that matrix multiplication is not commutative. Moreover, just because the matrix \(AB\) is defined, that does not mean that the matrix \(BA\) is defined. In fact, for both \(BA~~\text{and}~~BA\) to be defined, the two matrices need to both be \(n \times n\)</p>
<p>Applying a vector to a matrix is like multiplying an \(n \times m\) matrix with an \(m \times 1\) matrix (i.e. a column vector), and produces an \(n \times 1\) matrix. We can transpose a matrix. This flips the rows and the columns, for instance;</p>
<p style="text-align:center;">\( \begin{bmatrix} 2&amp;-5 \\ 0&amp;1\\3&amp;3 \end{bmatrix}^T = \begin{bmatrix} 2 &amp;1 &amp;3 \\ -5 &amp;0 &amp;3\end{bmatrix} \)<br></p>
<p style="text-align:left;">We only change the strides in the header, so this operation takes constant time. In 1D, the transpose of a row vector is a column vector. We can define the inner product of two vectors using matrix multiplication and transpose, i.e. \(x \cdot y = xy^T\). This gives rise to the outer product \( x \otimes y = x^Ty\). The result is an \(n \times n\) matrix. The transposition of a product is given by \((AB)^T = B^T A^T\). If \(A\) is \(p \times q \) and \(B\) is \(q \times r\), then \(B^T\) is \(r \times q\) and \(A^T\) is \(q \times r\). Also, \((A+B)^T = A^T + B^T\)</p>
<p style="text-align:left;"><br></p>
<p style="text-align:left;"><br></p>
<p><br></p>
<h2>Covariance Matrix</h2>
<p><br></p>
<p>Along with the mean vector, we can compute the variation/spread found in the dataset. In the 1D case, variance is given by&nbsp;</p>
<p style="text-align:center;">\( \frac{1}{N-1}\displaystyle\sum_{i=0}^{N-1}(x_i-\mu)^2 \)<br></p>
<p style="text-align:left;">It measures how spread out the values \(x_i\) are to the mean. The standard deviation is the square root of the variance; it has the same units as x.</p>
<p style="text-align:left;">In the multi-dimensional case (i.e. \(N \times d \) data matrix \(X\), with \(N~~d\)-dimensional vectors), we compute the covariance of every dimension with every other dimension. This is the average squared distance of every column of data from the average of every column. So, the entry \((i,j)\) is given by</p>
<p style="text-align:center;">\( \frac{1}{N-1}\displaystyle\sum_{i=0}^{N-1}(X_{ki}-\mu_i)(X_{kj} - \mu_j)

    \)<br></p>
<p style="text-align:left;">We can visualise the covariance matrix by an ellipse. This represents an (inverse) transform of a unit sphere to an ellipse covering the data. The mean vector is the centre of the ellipse. The ellipse is sometimes called the error ellipse.</p>
<p style="text-align:left;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/9266571/image%20%2817%29.png" alt="" width="550" height="330" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">The covariance matrix and the mean vector generalise the concepts of standard deviation and the mean scalar in the real case respectively. In particular, the mean vector represents the centre of the data points, and the covariance matrix represents the spread of the data.&nbsp;</p>
<p style="text-align:left;"><br></p>
<h2>Types of Matrices</h2>
<p><br></p>
<p>We now look at different types of matrices:</p>
<p></p>
<ul>
    <li>An \(n \times n\) matrix with only non-zero entries in the main diagonal is called a diagonal matrix. The matrices represent the transformation that scales each dimension of the vector separately. All angles remain unchanged, there is no rotation performed. For example, the matrix&nbsp;\( \begin{bmatrix} 1&amp;0&amp;0 \\ 0&amp;2&amp;0 \\ 0&amp;0&amp;3 \end{bmatrix} \) Is a diagonal matrix</li>
    <li>an \(n \times n\) matrix with only non-zero entries in the off diagonal is called an anti-diagonal matrix, for example, the matrix \(\begin{bmatrix} 0&amp;0&amp;1\\0&amp;1&amp;0\\1&amp;0&amp;0 \end{bmatrix}\) is an anti-diagonal matrix</li>
    <li>The identity matrix \(I\) is the \(n \times n\) matrix with zero entities in the non-diagonals and ones everywhere in the diagonal. The identity matrix has no effect on a vector/matrix. That is \( Ix = x = xI\) and \(IA = A = AI\), given that the dimension of \(I\) is compatible. For example, the \(3 \times 3\) identity matrix is \(\begin{bmatrix} 1&amp;0&amp;0\\0&amp;1&amp;0\\0&amp;0&amp;1 \end{bmatrix}\)</li>
    <li>A scalar multiple of the identity matrix has the effect of uniformly scaling all the dimensions of a vector</li>
    <li>The zero matrix \(O\) is the \(m \times n\) matrix with zero entities everywhere. Multiplying any vector/matrix with the zero matrix \(O\) will give the zero vector/matrix. It maps everything to the origin.</li>
    <li>An \(n \times n\) is called a square matrix. These matrices transform a vector in \( \mathbb{R}^n \) to another vector in&nbsp;\( \mathbb{R}^n \). There are many properties only defined on square matrix.&nbsp;</li>
    <li>A square matrix is triangular if it has non-zero entries above (upper triangular) or non-zero entries below (lower triangular) the diagonal. For example, the matrix&nbsp;\( \begin{bmatrix} 1&amp;0&amp;0 \\ 1&amp;2&amp;0 \\ 1&amp;1&amp;3 \end{bmatrix} \) is a lower-triangular matrix, and the matrix&nbsp;\( \begin{bmatrix} 1&amp;2&amp;2 \\ 0&amp;2&amp;2 \\ 0&amp;0&amp;3 \end{bmatrix} \) is an upper-triangular diagonal matrix. The corresponding system of equations can very easily be solved by substitution.&nbsp;</li>
    <li>A square matrix \(A\) is symmetric if \(A = A^T\). The covariance matrix is always symmetric. For example. the matrix&nbsp;\( \begin{bmatrix} 1&amp;1&amp;2 \\ 1&amp;2&amp;3 \\ 2&amp;3&amp;3 \end{bmatrix} \) is a symmetric matrix&nbsp;</li>
</ul>
<p><br></p>
<h1>Graphs and Matrices</h1>
<p><br></p>
<p>We can represent directed graphs using adjacency matrices. For each pair of vertices in the graph, if they have an edge, the entry is 1, otherwise the entry is 0. A graph has the following properties:</p>
<p>
</p>
<ul>
    <li>The in-degree of a vertex: the number of vertices which have an edge leading to this vertex. This is the sum of the column that the vertex represents.&nbsp;</li>
    <li>The out-degree of a vertex: the number of vertices which have an edge going from this vertex. This is the sum of the row that the vertex represents</li>
    <li>If the matrix is symmetric, then it is undirected: we can make any matrix symmetric by adding its transpose. In the case of graphs, this makes the edges bi-directional&nbsp;</li>
    <li>If there is a non-zero diagonal entry, then there is an edge going from the vertex to itself (self-transition)</li>
</ul>
<p>It is possible to have weights associated with the edges. We can easily adapt our matrix representation to add this data, we make the entry in the matrix equal to the value of the weight.&nbsp;<br>For instance, assume that we have a graph that shows the flow of packages from depots, then&nbsp;</p>
<p>
</p>
<ul>
    <li>If the total flow out of a vertex is &gt; 1, (i.e. its rows sum to &gt;1), then the vertex is a source. The depot produces some products</li>
    <li>If the total flow out of a vertex is &lt; 1, then it is a sink. The depot makes use of some products</li>
    <li>If the total flow out of a vertex is exactly 1, then it conserves mass. It reroutes everything</li>
</ul>
<p>If the whole graph consists of vertices whose total outgoing weight is 1, and all weights are positive or zero, then the whole graph preserves mass under flow, nothing is produced or consumed. Each row in the adjacency matrix sums to 1. This is called a conserving adjacency matrix. We can normalise the rows of any matrix (so long as each vertex has at least some flow out of it) to form a conserving adjacency matrix.</p>
<p><br></p>
<h2>Flow Analysis</h2>
<p><br></p>
<p>We can use matrices to model discrete problems. For example, matrices can be used to keep track of how many packages at each depot at an instant in time, and therefore predict how many packages will be there tomorrow or how many were there yesterday.&nbsp;</p>
<p>In particular, we can find some matrix \(A\) such that&nbsp;\( \texttt{x}_1 = A\texttt{x}_0 \) where \(\texttt{x}_0 \) is the number of packages in each depot today (as a vector), and we want to predict \(\texttt{x}_1 \), the number of packages in each depot tomorrow. The advantage of vectorised operations is that they can be accelerated using hardware such as a GPU.</p>
<p>Using matrices, we can attempt to answer the following:</p>
<p></p>
<ul>
    <li>What about in a week's time? what will \(x_7\) be?</li>
    <li>What about in one hours time? what will \(x_{1/24}\) be?</li>
    <li>What about at time \(x_{\infty}\)? what is the long term behaviour? will the system reach a steady state(an equilibrium), or will it oscillate forever?</li>
    <li>What about if we wanted to go backward in time, if we know \(x_0\), can we predict \(x_{-1}\)</li>
</ul>
<p><br></p>
<h1>More Matrix Operations</h1>
<p><br></p>
<h2>Exponentiation</h2>
<p>For a square matrix \(A\), we can apply the square to itself, i.e. \( A^k = AA...A\). This allows us to compute the value in a week's time, this is \(A^7x_0\).</p>
<p>We can illustrate how a rotation matrix \(A\) can be exponentiated in 2D.</p>
<p></p>
<ul>
    <li>The matrix \(A^0\) is the identity transformation&nbsp;<img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/9266571/image%20%2818%29.png" alt="" width="500" height="284" class="img-fluid atto_image_button_text-bottom"></li>
    <li>The matrix \(A\) is a rotation transformation&nbsp;<img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/9266571/image%20%2819%29.png" alt="" width="500" height="274" class="img-fluid atto_image_button_text-bottom"></li>
    <li>The matrix \(A^2\) rotates the 2D plane by double the degree of rotation of \(A\)</li>
</ul>
<p><br></p>
<h1>Eigenvalues and Eigenvectors</h1>
<p><br></p>
<p>Consider the depot example again. We can see how the packages flow over time, where we vary the initial depot&nbsp;</p>
<p><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/9266571/image%20%2820%29.png" alt="" width="500" height="276" class="img-fluid atto_image_button_text-bottom"><br></p>
<p><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/9266571/image%20%2821%29.png" alt="" width="500" height="276" class="atto_image_button_text-bottom"><br></p>
<p><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/9266571/image%20%2822%29.png" alt="" width="500" height="276" class="atto_image_button_text-bottom"><br></p>
<p>No matter which depot we start from, the packages end up about the configuration. This is a steady state, in terms of adjacency matrix, this is an eigenvector.&nbsp;<br></p>
<p>Matrices represent linear transformations, which involves rotation and/or scaling vectors. There are some vectors that do not get rotated by a linear transformation, they only get scaled. Such vectors are called eigenvalues, and the scaling factor is an eigenvalue.&nbsp;<br></p>
<p>We can find the leading (biggest) eigenvalue by power iteration. Given a vector \(\texttt{x}\) and a matrix \(A\), and a number of iterations \(n\), the power iteration computes \(A^n\texttt{x}\), but normalising at each multiplication, i.e. \(\texttt{x}_0 = \texttt{x}\), and&nbsp;</p>
<p style="text-align:center;">\( \texttt{x}_{i+1} = \frac{A\texttt{x}_i}{||A\texttt{x}_i||_\infty} \)<br></p>
<p style="text-align:left;">By normalising, we ensure the value doesn't increas to \(\infty\) or decrease to 0. We can normalise using an norm; the infinity norm is the most ifficient/most common used.&nbsp;</p>
<p style="text-align:left;">Regardless of our choice of \(x\), the vector we find after power iteration is unique, this is true for most square matrices. The resulting vector is the leading eigenvector. It has to just scale the vector (and not rotate) since we are always normalising, the scale effect is removed. We can use the relation \( Ax = \lambda x \) to compute the leading eigenvalue. Here, \(\lambda\) is the leading eigenvalue, and \(\texttt{x}\) is the leading eigenvector.</p>
<p style="text-align:left;">For an \(n \times n \) matrix, there are \(n\) eigenvalues and \(n\) corresponding eigenvectors. The eigenvectors are orthogonal, i.e. the dot product of any two eigenvectors is 0. Power iteration is much faster than the function&nbsp;<code>np.linalg.eig</code> when computing the leading eigenvalue/eigenvector for large matrices.&nbsp;<br></p>
<p style="text-align:left;">For an \(n \times n\) matrix, an eigenvalue \(\lambda\) is a real number and the corresponding eigenvector \(\texttt{x}\) is a vector in \(\mathbb{R}^n\) such that \(A\texttt{x} = \lambda \texttt{x}\). A matrix has a unique set of of eigenvalues, but not a unique set of eigenvectors (e.g. the negative eigenvectors and eigenvalues are called eigenproblems)&nbsp;</p>
<p style="text-align:left;">Using undirected graphs (i.e. symmetric matrices) will ensure that all the eigenvalues/eigenvectors are real. If none of the eigenvalues were real for some matrix, there would be no steady state, it will oscillate.&nbsp;</p>
<p style="text-align:left;">Now, we consider the example of packages going through depots again. Let \(A\) be the adjacency matrix for the weighted graph. We can show the eigenvectors of \(A\), scaled by the eigenvalue:</p>
<p style="text-align:left;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/9266571/image%20%2824%29.png" alt="" width="500" height="436" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">The leading eigenvector says where the package will end up in the long run; they will end up mostly in depot A, D, B, C, and H. smaller eigenvalues represent the routes less commonly used, while larger eigenvalues are the dominant routes taken.</p>
<p style="text-align:left;">From the eigendata, we can construct the eigenspectrum, this is just the eigenvalues ordered by the magnitude (biggest one first). This ranks the eigenvalues in order of importance.</p>
<p style="text-align:left;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/9266571/image%20%2825%29.png" alt="" width="500" height="272" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:center;"><br></p>
<h2>Eigendecomposition</h2>
<p>The function&nbsp;<code>np.linalg.eig</code>&nbsp;is numerically unstable because of rounding errors due to limitations in floating point representations. Nonetheless, it is pretty good most of the time. A more stable algorithm for real, symmetric matrices is&nbsp;<code>np.linalg.eigh</code>. Typically, there is not a big difference in eigenvalue/eigenvectors if they are big. However, there could be a big difference in eigenvalues/eigenvectors when the values are very small however,&nbsp; <code>np.linalg.eigh</code> is typically better.</p>
<p><br></p>
<h1>Principle Component Analysis</h1>
<p><br></p>
<p>We know that the covariance matrix gives us insight into correlations between variables in the dataset. We can plot a representation of the covariance matrix as an ellipse which aligns with the distribution of the data points. Uncorrelated data looks close to a circle, while strong correlations correspond to a long, thin ellipse. The eigenvectors of the covariance matrix, scaled by the eigenvalue, from the principal axes of the ellipse.&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/9266571/image%20%2826%29.png" alt="" width="450" height="592" class="img-fluid atto_image_button_text-bottom"><br></p>
<p>We have decomposed the covariance matrix into eigenvectors and eigenvalues. It is also possible to recompose the covariance matrix using the eigenvalues and eigenvectors. This is given by&nbsp;\( \Sigma = Q \Lambda Q^T \), where \(\Sigma\) is the covariance matrix, \(Q\) is a matrix of unit eigenvectors (same as <code>np.linanlg.eig</code>) and \(\Lambda\) is a diagonal matrix of eigenvalues.</p>
<p>If our dataset has a lot of dimensions, then \(\Sigma\) is going to be very large. In that case, we would want to approximate the covariance matrix by only storing the first few principal components. Keeping the larger principal components means that we keep most of the data.</p>
<p></p><span style="font-size:0.9375rem;">Matrix approximation allows us to simplify the transformation and compress matrices. The approximation depends on the eigenspectrum:<br>
</span>
<ul>
    <li>If we have one big eigenvalue and all the others are much smaller, we can just take the first eigenvalue to approximate the dataset.</li>
    <li>Instead, if all the eigenvalues have similar magnitude, then we will not be able to transform the data easily&nbsp;</li>
</ul>
<p>We can also reduce the dimension of the data by projecting it onto few principal components of the covariance matrix. This involves multiplying the dataset matrix by each component, and saving the projected data into another matrix. We can project the data above to the 1<sup>st </sup>principle component&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/9266571/image%20%2827%29.png" alt="" class="img-fluid"><br></p>
<p style="text-align:left;">Here, we reduced the 2D data into 1D. It is more common to reduce high dimensional data into 2D. It is much easier to visualise in 2D; it allows us to find structure in data, e.g. clusters.&nbsp;</p>
<p style="text-align:left;">Matrix decomposition has applications everywhere. Eigendecomposition can be used if we have a system modelled as a linear transformations (i.e. a linear map \(\mathbb{R}^n \rightarrow \mathbb{R}^n \). It lets us predict behaviour over different time scales, e.g.</p>
<p style="text-align:left;"></p>
<ul>
    <li>Finding modes/resonances of a system&nbsp;</li>
    <li>Predicting behaviour of feedback control systems</li>
    <li>Partitioning graph and cluster data (spectral clustering)</li>
    <li>Predicting graph flows</li>
    <li>Performing Principal Component Analysis on high-dimensional data sets for exploratory data analysis, 2D visualisation, or data compression</li>
</ul>
<p></p>

<p></p>
<h2>Trace, Determinant, and Definiteness</h2>
<p><br></p>
<h3>Trace</h3>
<p>The trace of a matrix is the sum of the diagonal entries. It is also the sum of the eigenvalues. It can be thought of as the perimeter of the parallelotope of a unit cube transformed by the matrix.</p>
<h3>Determinant</h3>
<p>The determinant of a matrix can be thought of as the area of the parallelotope of a unit cube transformed by the matrix; it is the product of the eigenvalues. In particular, if any eigenvalue is 0, then the determinant is 0, meaning that the transformation collapses at least one dimension. Therefore, this transformation cannot be reversed, as information has been lost.&nbsp;</p>
<h3>Definiteness</h3>
<p>There are 4 other ways of classifying a matrix:&nbsp;</p>
<p></p>
<ul>
    <li>If every eigenvalue of a matrix is strictly positive, then the matrix is positive-definite</li>
    <li>If every eigenvalue is non-negative, then the matrix is positive semi-definite</li>
    <li>If every eigenvalue is non-positive, then the matrix is negative semi-definite</li>
    <li>If every eigenvalue if&nbsp; strictly negative, then the matrix is negative-definite</li>
</ul>
<p>A positive definite matrix \(A\) satisfies \( \texttt{x}^T A\texttt{x} \gt 0 \) for all vectors \( \texttt{x}~~\text{in}~~\mathbb{R}^n \). We know that&nbsp;\( \texttt{x}^TA\texttt{x}=|\texttt{x}|~|A\texttt{x}|\cos{\theta} \), so \(A\) myst rotate a vector by at most 90°. We can conclude many properties about the transformation corresponding to a matrix using its eigenspectrum:</p>
<p></p>
<ul>
    <li>If a matrix has one or more zero eigenvalues, the transformation it performs is one that collapses one or more dimensions in a vector space. This type of operation is irreversible, and tells us that \(A\) is singular</li>
    <li>Eigenvectors corresponding to a larger (absolute) eigenvalues are more "important", they represent directions in which data will get stretched by the most.&nbsp;</li>
    <li>If the eigenspectrum is nearly flat (eigenvalues all have similar values), then \(A\) represents a transform that stretches vectors almost equally in all directions (like transforming a sphere to another sphere)</li>
    <li>If the eigenspectrum has few large eigenvalues and lots of small ones, then vectors will get stretched along a few directions, but shrink away to nothing along others (like transforming a sphere to a long, skinny ellipse).&nbsp;</li>
</ul>
<p><br></p>
<h1>Inversion</h1>
<p><br></p>
<p>For some \(n \times n\) matrices \(A\), there exists an inverse matrix \(A^{-1}\) such that:</p>
<p></p>
<ul>
    <li>\(A^{-1}(Ax) = x\)</li>
    <li>\(A^{-1}A=I\)</li>
    <li>\((A^{-1})^{-1} = A\)</li>
</ul>
<p>Therefore, \(A^{-1}\) has the effect of undoing \(A\). Also, for square matrices, \(A, B ,(AB)^{-1} = B^{-1}A^{-1}\). We can invert a square matrix using <code>np.linalg.inv</code>. Inversion&nbsp; can undo a transformation as long as no information was lost as part of the transformation.&nbsp;<br></p>
<p>We illustrate inversion with an example. Let \(A\) be a \(2 \times 2\) matrix that rotates the 2D plane by 30°</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/9266571/image%20%2828%29.png" alt="" width="493" height="277" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">Its inverse matrix gives the following transformation&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/9266571/image%20%2829%29.png" alt="" class="img-fluid"><br></p>
<p style="text-align:left;">A condition for a square matrix for having an inverse is none of the eigenvalues to be 0. It is equivalent to saying that the determinant is non-zero. A matrix being invertible is equivalent to the function represented by the matrix being bijective. An \(n \times m\) matrix which is non-square maps vectors of dimension \(m\) to dimension \(n\). This means that the transformation collapses or creates dimensions. Such a transformation is not uniquely reversible, it is not bijective.&nbsp;</p>
<p style="text-align:left;">A matrix that is not invertible is called singular, in contrast, an invertible matrix is called non-singular. The figure below shows a singular transformation of the 2D plane</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/9266571/image%20%2830%29.png" alt="" class="img-fluid"><br></p>
<p style="text-align:left;">Since matrix operations involve lots of floating point operations, there are many operations for rounding errors to accumulate. Inversion is particularly hard to compute in a stable form directly. Moreover, many matrices that theoretically could be inverted cannot be inverted using floating point representations. Nonetheless:</p>
<p style="text-align:left;"></p>
<ul>
    <li>If a matrix is orthogonal (i.e. rows and columns are all orthogonal unit vectors), then its inverse is its transpose; this can be computed in \(O(1)\) time</li>
    <li>If a matrix is diagonal, then its inverse is just another diagonal matrix with reciprocal of the original diagonal entries this can be computed in \(O(n)\) time.&nbsp;</li>
    <li>If a matrix is positive-definite, then its inverse can be computed in \(O(\n^2)\) time using Cholesky decomposition&nbsp;</li>
    <li>If a matrix is triangular, then its inverse can be computed in \(O(n^2)\) using elimination algorithms.&nbsp;</li>
</ul>
<p>An orthogonal matrix has eigenvalues 1 or -1. An orthogonal matrix corresponds to a matrix that purely rotates the plane. For example, rotation by 30° corresponds to an orthogonal matrix. A diagonal matrix corresponds to one that only a matrix.&nbsp;</p>
<p>In general, the inverse of a sparse matrix is not sparse. This means that sparse matrix algorithms virtually never involve a direct inverse, since a sparse matrix could easily be \(10^6 \times 10^6\), but with maybe only a few million non-zero entries, and might be stored in a few dozen megabytes. The inverse form would have \(10^{12}\) entries and would require a terabyte or more to store&nbsp;</p>
<p><br></p>
<p></p>
<h1>Solving Linear Systems</h1>
<p><br></p>
<p>We can use the inverse matrix to take one step to the past. For instance, if \(A\) corresponds to one day forward, \(A^{-1}\) corresponds to one day backwards. To go \(n\) days backwards, we can compute&nbsp;\((A^{-1})^n\). However, inversion is highly problematic from a numerical point of view, and roundoff error will lead to severely distorted results with repeated negative powers of a matrix. A matrix corresponds to a system of equations, for example, the matrix&nbsp;</p>
<p style="text-align:center;">\( \begin{bmatrix} 0.5 &amp; 1.0 &amp; 2.0 \\ 1.0 &amp; 0.5 &amp; 0.0 \\ 0.6 &amp;1.1 &amp;-0.3 \end{bmatrix}
    \)<br></p>
<p>Corresponds to the system of equations&nbsp;</p>
<p style="text-align:center;">\( y_1 = 0.5x_1 + 1.0x_2 + 2.0x_3 \\ y_2 = 1.0x_1+0.5x_2+0.0x_3 \\ y_3 = 0.6x_1 + 1.1x_2 - 0.3x_3 \)<br></p>
<p style="text-align:left;">We can use the inverse of a matrix to find the solution. A matrix corresponds to \(Ax = y\), so the solution to the system is \(x = A^{-1}y\). As this only works for square matrices, \(x\) and \(y\) must have the same dimension. An alternative approach in solving these linear equations is to find \(x\) such that \(||Ax - y||\)</p>
<p style="text-align:left;"><br></p>
<h2>The SVD</h2>
<p>Like eigendecomposition can be used to decompose square matrices, there is a&nbsp; generalisation, singular value decomposition (SVD). Given \(n \times m\) matrix \(A\), we can write</p>
<p style="text-align:center;">\( A = U\Sigma V^T \)<br></p>
<p style="text-align:left;">where:</p>
<p style="text-align:left;"></p>
<ul>
    <li>\(U\) is an \(m \times m\) square unitary matrix, whose columns contain the left singular vectors.</li>
    <li>\(V\) is an \(n \times n\) square unitary matrix, whose columns contain the right singular vectors.</li>
    <li>\(\Sigma\) is an \(n \times m\) matrix whose diagonal contains the singular values</li>
</ul>
<p>A unitary matrix is one whose conjugate transpose is equal to its inverse. If \(A\) is real, then \(U\) and \(V\) will be orthogonal. The diagonal matrix \(\Sigma\) is the set of singular values, which are closely related to the eigenvalues, but are not quite the same thing (except for special cases like \(A\) is a positive semi-definite symmetric matrix). The singular values are always positive real numbers</p>
<p>The SVD is the same as&nbsp;</p>
<p></p>
<ul>
    <li>Taking the eigenvectors of \(A^T A\) to get U&nbsp;</li>
    <li>Taking the square root of the absolute value of the eigenvalues&nbsp;\( \lambda_i \) of \(A^T A \) to get \( \Sigma_i = \sqrt{|\lambda_i|}\)</li>
    <li>Taking the eigenvectors of \(AA^T\) to get \(V^T\)</li>
</ul>
<p>If \(A\) is a symmetric, positive semi-definite matrix, then the eigenvectors of the columns of \(U\) are the columns of \(V\). The eigenvectors are in \(\Sigma\). So SVD can convert any matrix into a product of 3 matrices, \(U,\Sigma, V\) where</p>
<p></p>
<ul>
    <li><span style="font-size:0.9375rem;">\(U\) is orthogonal, i.e. it is purely rotational&nbsp;</span></li>
    <li><span style="font-size:0.9375rem;">\(\Sigma\) is diagonal, i.e. it is purely scaling</span></li>
    <li><span style="font-size:0.9375rem;">\(V\) is orthogonal, i.e. it is purely rotational&nbsp;&nbsp;</span></li>
</ul>
<p style="text-align:left;"><br></p>
<h2>Using the SVD for Fractional Powers</h2>
<p><br></p>
<p>We can use the SVD to compute fractional powers of a matrix. For an \(n \times m \) matrix, \(A, A^k = V\Sigma^kU^T\) (and if \(A\) is symmetric, then \(A^k= V\Sigma^kU^T\) is also the same thing). We can also use the SVD to compute the inverse of a square matrix \( A^{-1}&nbsp; = V\Sigma^{-1}U^T \). This can be computed in \(O(n)\) time since \(\Sigma^{-1}\) can be computed by reciprocating the diagonal entries in \(\Sigma\).&nbsp;</p>
<p>Moreover, we can compute the pseudo-inverse of a matrix, even if it isn't a square/inverse. It is given by the same formula \(A^+ = V \Sigma^{-1}U^T\). This allows us to invert some non square matrices. We pad \(\Sigma\) with zeroes to invert it, the function <code>np.linalg.pinv</code> does this computation. It is possible that the Problem doesn't have an exact solution, the pseudo-inverse function tries to minimise \( || Ax - y ||_2\)</p>
<p>Assume that we have 2 datasets, one consisting of \(x\) values and the other with \(y\) values, in matrices \(X\) and \(Y\) respectively. We want to fit a line to this dataset to predict \(y\) values, using \(y=&nbsp; Ax\), or predict lots of \(y\) values at the same time, using \(Y = AX\). The equation of the line/plane is given in in \(A\), i..e. \(x = A^+y\).&nbsp;</p>
<p>In general, we have \(A = X^+Y\). We can use this formula to fit a line/plane through a dataset, even when we have many more points than the number of dimensions required to specify the line/plane. A system of equations where there are more inputs than outputs is called overdetermined&nbsp;</p>
<p><br></p>
<h1>Rank and Condition</h1>
<p><br></p>
<h2>Rank of a Matrix</h2>
<p>The rank of a matrix is the number of non-zero singular values. If the number of non-zero singular values is equal to the size of the matrix, then the matrix is full rank. A full rank matrix has a non-zero determinant and will be invertible. The rank tells us how many dimensions the parallelotope that the transform represents will have. If a matrix does not have full rank, it is singular (non-invertible) and has deficient rank. If the number of non-zero singular values is much less than the size of the matrix, the matrix is low rank.</p>
<p>For example, a \(4 \times 4\) matrix with rank 2 will take vectors in \(\mathbb{R}^4\) and output vectors in \(\mathbb{R}^2\) subspace (a plane) of \(\mathbb{R}^4\). The orientation of that plane will be given by the first two eigenvectors of the matrix.</p>
<p><br></p>
<h2>Condition of a Matrix</h2>
<p>The condition of a matrix is the ratio of the largest singular value to the smallest. This is only defined for full rank matrices. The condition number measures how sensitive inversion of the matrix is to small changes. A matrix with a small condition number is called well-conditioned and is unlikely to cause numerical issues. In contrast,&nbsp; a matrix with a large condition number is ill-conditioned, and numerical issues are likely to be significant.&nbsp;</p>
<p>Inverting an ill-conditioned matrix will not be very accurate due to the floating point roundoff errors. We can think of rank as measuring "how singular" the matrix is, i.e. how many dimensions are lost in the transformation. We can think of condition number as measuring how close a non-singular matrix is to being singular. A matrix which is nearly singular may become effectively singular due to floating point roundoff errors</p>
<p><br></p>
<h1>Whitening a Dataset</h1>
<p><br></p>
<p>Whitening removes all linear correlations within the data. Given a matrix \(X\), the whitened matrix \(X^w\) is given by&nbsp;\( X^w = (X - \mu)\Sigma^{-1/2} \), where \(\mu\) is the mean row vector (containing the mean of each column), and \(\Sigma\) is the covariance matrix.</p>
<p>By subtracting the mean, the data gets centered at the origin (zero mean). By multiplying it with \(\Sigma^{-1/2}\), it essentially divides by the standard deviation and normalises the spread of data in each column (squashes data so it has unit covariance)</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/9266571/image%20%2831%29.png" alt="" width="370" height="422" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:center;"><br></p>
<h1>Introduction to Optimisation</h1>
<p><br></p>
<p>Optimisation is the process of finding the best solution to a question. We would like to create algorithm(s) that given the optimal solution given a problem should be efficient, they should try to study as few possible solutions as they can. We can think of optimisation algorithms as searching the solution space and trying to find the best possible solution.&nbsp;</p>
<p>An optimisation algorithm can be applied to any optimisation problem. There are no specific cases depending on the problem. Instead, we give the algorithm the problem in a way such that the right value can be optimised.&nbsp;</p>
<p><br></p>
<h2>Parameters and Objective Function</h2>
<p>An optimisation problem has 2 aspects:</p>
<p></p>
<ul>
    <li>A set of parameter: The values we can change to improve the final value. This can be one singular value, or an array of values. It is denotes by \(\theta\). There parameters exist in some parameters space \(\Theta\). This might be a vector space like \(\mathbb{R}^n\). If so, we call the parameters \(\theta\) a vector, i.e. the parameter vector \(\theta\).</li>
    <li>The objective function: The function that takes all the parameters and maps it to a singular value. This is the value we are trying to optimise. The function is denoted by \(L\) (and called the loss/objective function), and we want to find the maximum/minimum \(L(\theta)\)</li>
</ul>
<p>Therefore, the optimisation process finds the value</p>
<p style="text-align:center;"><span style="font-size:0.9375rem;">\( \underset{\theta\in\Theta }{\theta^\ast = \text{argmin}~L(\theta)} \)</span><br></p>
<p style="text-align:left;">where:</p>
<p style="text-align:left;"></p>
<ul>
    <li>\(\theta^\ast\) is the optimal solution, the value we are trying to find</li>
    <li>\(\Theta\) is the parameter space</li>
    <li>\(\theta\) is a configuration of the parameters, an element in \(\Theta\)</li>
</ul>
<p>Most optimisation problems have an extra aspect, a constraint. This is a limitation to the possible configuration of the parameters. That is, we limit the parameter space into a subspace, which is the feasible set/region of solution, for example, a 500km/h limit for the speed of a car.</p>
<p>We will only consider optimisation processes that minimise a value. We can always make the loss function return the negative value so that the final result will be a maximum.&nbsp;</p>
<p>For example, consider the following optimisation problem: We are given a function \(f\) which takes in some parameter vector \(\theta\) along with a fixed input vector \(x\), and we want to find a particular set of parameters \(\theta^\ast\) such that \(f(x,\theta^\ast)\) is as close to some value \(y\) as possible. In this case, the loss function would try to minimise the distance between \(f(x,\theta)\) and \(y\), i.e.</p>
<p style="text-align:center;">\( L(\theta) = || f(x,\theta) - y~|| \)<br></p>
<p style="text-align:left;">It is possible that the evaluation of the loss function \(L\) is expensive,&nbsp;</p>
<p style="text-align:left;"></p>
<ul>
    <li>It might take a long time to compute the value</li>
    <li>It might require a real world experiment (e.g. do users like a new app layout)</li>
    <li>It might require data that must be bought (literally expensive in this case)</li>
</ul>
<p>A good optimisation algorithm would try to minimise the number of times we call the loss function. To do this, there has to be some mathematical structure within the optimisation process</p>
<p><br></p>
<h1>Discrete and Continuous Problems</h1>
<p><br></p>
<p>If the parameters are from a continuous space (e.g. \mathbb{R}^n\), then the problem is continuous optimisation. If the parameters are from a discrete space, then the problem is discrete optimisation. It is easier to optimise in a continuous space since we can make assumption about the data (using continuity and smoothness).&nbsp;</p>
<p>For example, consider the problem of optimising the angle at which we should throw a stone so that it travels the farthest, considering both gravity and air resistance.</p>
<p><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/298748084/image%20%2832%29.png" alt="" width="500" height="254" class="img-fluid atto_image_button_text-bottom"><br></p>
<p>The optimisation algorithm considers multiple angles (lighter lines) the ball could be thrown, and concluded that the ball should be thrown at about 30° to throw it the farthest, denoted by the darker line.&nbsp;</p>
<p>We will focus on continuous optimisation in \(mathbb{R}^n\), so, the parameter&nbsp;\( \theta = [\theta_1~~~\theta_2~~~...~~~\theta_n] \),&nbsp; and the optimisation problem satisfies&nbsp;<span style="font-size:0.9375rem;">\( \underset{\theta\in\mathbb{R}^n}{\theta^\ast = \text{argmin}L(\theta)} \),&nbsp;</span><span style="font-size:0.9375rem;">subject to constraints.&nbsp;</span></p>
<p><span style="font-size:0.9375rem;">Normally, the loss function is smooth and continuous (different to \(\mathbb{R}^n\) being continuous). There are many classes of optimisation algorithms, such as:</span></p>
<p></p>
<ul>
    <li><span style="font-size:0.9375rem;">Iterative : They generate more optimised solutions with more iterations&nbsp;</span></li>
    <li>Direct : can find the minimum in one step.</li>
</ul>
<p>Now, consider an optimisation algorithm: Finding the median of a dataset in \(\mathbb{R}^2\). The median can be computed by sorting the dataset and choosing the minimum element. Another way of thinking about the median is that it minimises the sum of distances to all the vectors of the dataset. So, this can be viewed as an optimisation algorithm. Here, the parameter set \(\Theta = \mathbb{R}^2\), and we want to find the value \(\theta \in \Theta\) such that the value</p>
<p style="text-align:center;">\( L(\theta) = \displaystyle\sum_{i}||\theta - x_i||_2 \)&nbsp;</p>
<p style="text-align:left;">is minimised, where \(x_i\) are the set of 2D points in the dataset (the collection of target points). The following image illustrates this&nbsp;</p>
<p style="text-align:left;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/298748084/image%20%2833%29.png" alt="" width="500" height="344" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">Here, the blue points are the values in the dataset. The green point is our initial guess, and we iteratively try to optimise the value. The value we find in each iteration is given in orange. We can use the same idea in higher dimensions.</p>
<p style="text-align:left;">Now, consider another problem. Given a set of points in \(mathbb{R}^n\), we want to find out a layout of points such that the points are evenly spaced. In that case, the parameter space \(\Theta = \mathbb{R}^n\), and the loss function is</p>
<p style="text-align:center;">\( L(\theta) = \displaystyle\sum_i\displaystyle\sum_j(\alpha-||x_i-x_j||_2)^2 \)<br></p>
<p style="text-align:left;">where \(alpha\) is the space we want most of the points to differ by, we unravel the \(theta\) into vectors \(x_i\) and \(x_j\). This will try to find a configuration of points that are all \(\alpha\) units apart, as shown below</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/298748084/image%20%2834%29.png" alt="" width="450" height="447" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:center;"><br></p>
<p style="text-align:center;"><br></p>
<p></p>
<h1>Constrained Optimisation</h1>
<p><br></p>
<p>The constrained optimisation equation is</p>
<p style="text-align:center;">\( \underset{\theta\in\Theta}{\theta^\ast = \text{argmin}L(\theta)},\text{ subject to }c(\theta) = 0 \)<br></p>
<p style="text-align:left;">where \(c\) is the constraint function. Alternatively, we can have&nbsp;</p>
<p style="text-align:center;">\( \underset{\theta\in\Theta}{\theta^\ast = \text{argmin}L(\theta)},\text{ subject to }c(\theta) \leq 0 \)<br></p>
<p style="text-align:left;">An equality constraint can be constraining a value to some surface, while an inequality&nbsp; constraint is like constraining the value to a volume.&nbsp;</p>
<p style="text-align:left;">A box constraint is just the requirement that \(\theta\) lies in \(\mathbb{R}^n\), e.g. \(\theta_i\) is between 0 and 1 (the unit cube), or \(\theta_i\) is positive (in the positive orthant). A convex constraint is another constraint with just inequalities of hyperplanes. Box constraint is a specific type of convex constraint.</p>
<p style="text-align:left;">Unconstrained optimisation has no constraints. Typically, the values returned by constrained optimisation are not helpful/feasible in real life. There are ways of implementing a constrained optimisation problem, purey constrained or penalty approach.&nbsp;</p>
<p style="text-align:left;">In a purely constrained approach, we use an optimisation algorithm that implements hard constraints inherently. This might be straightforward in some cases, but is difficult in most. Typically, constraints will be specified as either a convex region or a simple (hyper)rectangular region of the space (a box constraint). Using constrained optimisation, we can guarantee that the solution is feasible (i.e. it obeys the constraints). It might also mean that we perform fewer computations (since there are fewer possible outcomes). However, it ay be less efficient than an unconstrained optimisation; there are fewer algorithms that can be used to optimise, and it can be hard to specify the constraint to the optimiser.</p>
<p style="text-align:left;">In a soft constraint approach, we can add a penalty tern to the loss function to discourage solutions that do not match the constraints. It is more appropriate if there is no set boundary of constraints (e.g. it doesn't matter if the values are just a bit out of the constraint). We modify the loss function to&nbsp;</p>
<p style="text-align:center;">\( L(\theta^\prime) = L(\theta) + \lambda(\theta) \)<br></p>
<p style="text-align:left;">where \(\theta\) is the penalty function. As the value gets further from the constraint, the penalty term increases dramatically. This means that any algorithm can be used to optimise this function, and we can deal with soft constraints sensible.&nbsp;<span style="font-size:0.9375rem;">On the other hand, it is possible that the soft constraints are not respected, it might be difficult to form the constraints, and it does not take advantage of the fact that some solutions need to be checked.&nbsp;</span></p>
<p style="text-align:left;"><span style="font-size:0.9375rem;">We can use a similar approach to solve discrete optimisation problems. We can try to approximate the problem using a continuous optimisation problem. This is called relaxation. A relaxed version of the problem gets solved instead of the original, more difficult problem&nbsp;</span></p>
<p><br></p>
<h2>Penalisation</h2>
<p>Penalisation involves the addition of a penalty term to the objective function in order to minimise a certain property of the function. This is widely used in approximation problems to find solutions that generalise well, i.e. ones which are tuned to approximate some data, but not too closely. This is a relaxation of a hard, constrained, problem into a more relaxed ,unconstrained version.&nbsp;</p>
<p>We can change the stone throwing example to instead optimise how hard to throw it. There is a limit to human strength, so this is constrained optimisation. In this case;</p>
<p></p>
<ul>
    <li>The objective function is the distance that the ball travels</li>
    <li>The parameters are the angle \(\alpha\) and the strength \(v\)</li>
    <li>The constraint is the strength of the throw, it has to satisfy&nbsp;\( 0 \leq v \leq v_k \) where \(v_k\) is a fixed maximum human strength&nbsp;</li>
</ul>
<p>We can either require these constraints with a constrained optimisation algorithm, or change the objective function by penalising strength greater than \(v_k&nbsp; = 1\). If we apply a constrained optimisation, then we get the following graph of solutions:<br></p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/298748084/image%20%2835%29.png" alt="" width="500" height="347" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">The optimisation throw angle is 32.4° and strength 1.00, with distance 104.0m. Instead, if we used a penalty term to optimise, the penalty function could be the following&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/298748084/image%20%2836%29.png" alt="" width="500" height="340" class="img-fluid atto_image_button_text-bottom"><br></p>
<p>We want to discourage solutions that exceed strength values of 1, so the penalty function increases exponentially after 1. This gives us the following graph&nbsp; of solutions</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/298748084/image%20%2837%29.png" alt="" width="500" height="353" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">The optimised throw angle is 35.2° and strength 1.06, with distance 101.1m</p>
<p style="text-align:left;"><br></p>
<p></p>
<h1>Properties of the Objective Function</h1>
<p><br></p>
<h2>Convexity: Global and Local Minima</h2>
<p>An objective function can have a local minima. A local minimum is a point where the function is increasing in every direction from that point. If the function is convex, then there is precisely one global minimum. For example, the following functions are convex.</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/298748084/image%20%2838%29.png" alt="" width="500" height="239" class="img-fluid atto_image_button_text-bottom"><br></p>
<p>Other function can have multiple local minima, i.e. there could be some local minimum that is not the optimal value (not the global minimum). For example, the following functions are not convex.</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/298748084/image%20%2839%29.png" alt="" width="500" height="237" class="img-fluid atto_image_button_text-bottom"><br></p>
<p>If the objective function is convex and the constraints form a convex portion of the search space, then the problem is called convex optimisation. These can be solved very optimally, even if they have hundreds of variables. These include:&nbsp;</p>
<p></p>
<ul>
    <li>Linear programming, i.e. the constraint and the objective function are linear</li>
    <li>Quadratic programming, i.e. the objective function is quadratic, and the constraint is linear</li>
    <li>Some other special cases, e.g. semi-quadratic programming, quadratically constrained quadratic program</li>
</ul>
<p></p>
<p>For non-convex functions, we typically use some iterative method to optimise. It is also possible to relax/approximate a function into some convex function.&nbsp;</p>
<p><br></p>
<h2>Continuity</h2>
<p>An objective function is continuous if small changes in \(\theta\) lead to small changes in \(L(\theta)\). In other words, there are no jumps in the function. For example, the following function is continuous.&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/298748084/image%20%2840%29.png" alt="" width="500" height="274" class="img-fluid atto_image_button_text-bottom"><br></p>
<p>If the function is discontinuous, local search methods do not need to converge to a solution. For example, the following function is discontinuous.&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/298748084/image%20%2841%29.png" alt="" width="500" height="235" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">Optimising discontinuous objective functions is typically much harder than optimising continuous functions. Small changes in the parameter can lead to arbitrary changes in the objective function. Continuity is why continuous optimisation is easier than discrete optimisation.&nbsp;</p>
<p style="text-align:left;"><br></p>
<h1>Algorithms</h1>
<p><br></p>
<h2>Linear Least Squares</h2>
<p>The linear least squares is an optimisation technique which tries to find the closest set of parameters \(x\) such that \(Ax\) is as close to \(y\). So the objective function is</p>
<p style="text-align:center;">\( L(x) = ||Ax-y||_2 \)<br></p>
<p style="text-align:left;">This equation is convex, the function is quadratic, so it must have one global minimum (even if \(x\) has multiple dimensions). Quadratic functions have either one or zero minimum.&nbsp;</p>
<h2>Line Fitting</h2>
<p>Given a pair of vectors \(x\) and \(y\), we can try to fit a line through it. As shown previously, the pseudo-inverse (via SVD) can be used to fit a line. We will create samples from \(y=2x+1\) with some noise. Using the SVD, we get the following estimate:</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/298748084/image%20%2842%29.png" alt="" width="500" height="351" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:center;"><br></p>
<h2>Iterative Optimisation</h2>
<p>An iterative optimisation takes a series of steps in the parameter space to find the local minimum, until the terminating criterion is met. There is a current parameter (or a collection of them). The algorithm essentially involves:</p>
<pre><code>
choose a starting point x
while objective function is changing
    adjust parameters
    evaluate objective function
    if better solution found, record and change best parameter
return best parameter set found
</code>
</pre>
<h2>Grid Search</h2>
<p>Grid search is a simple yet inefficient optimisation for multidimensional problems. We sample the parameter space by breaking it into equally dividing it in each dimension, usually with a fixed number of divisions per dimension.&nbsp;</p>
<p>The objective function is evaluated at each \(\theta\) on this grid, and the lowest \(\theta\) found so far is tracked. It is sometimes used to optimise hyperparameters, where the objective function may be complex but the absolute minimum isn't essential.</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/298748084/image%20%2843%29.png" alt="" width="611" height="1100" class="img-fluid atto_image_button_text-bottom"><br></p>
<p><br></p>
<p>Grid search is inefficient at optimisation in higher due to the curse of dimensionality. If we want to break a grid into 8 portions, in 1D, this would be 8 blocks. in 2D however, this would be 64 blocks, and so on.&nbsp;</p>
<p>Expanding to a 100 dimensional space, this would need \(8^{100}\) evaluations of the objective functions&nbsp;\( (2.04\times 10^{89}) \). even with 3 points in each dimension, this is still \( (3^{100} \approx 5.15 \times 10^{46}) \).</p>
<p>If the objective function is not very smooth,the grid would need to be very dense to catch any minima and find the optimisation configuration. The following figure describes how the minimum is missed during the grid search because of the way the grid has been divided.&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/298748084/image%20%2844%29.png" alt="" width="800" height="314" class="img-fluid atto_image_button_text-bottom"><br></p>
<h3>Pros</h3>
<p></p>
<ul>
    <li>Works for any continuous parameter space</li>
    <li>Requires no knowledge of the objective function&nbsp;</li>
    <li>Trivial to implement</li>
</ul>
<p></p>
<h3>Cons</h3>
<p></p>
<ul>
    <li>Incredibly inefficient&nbsp;</li>
    <li>Must specify the space bounds in advance</li>
    <li>Highly biased to finding things near the "early corners" of the space</li>
    <li>Depends heavily on the number of divisions chosen</li>
    <li>Hard to tune so that minima aren't missed entirely&nbsp;</li>
</ul>
<p><br></p>
<p></p>
<h2>Hyperparameters</h2>
<p>Grid search depends on how we partition the grid. Most optimisation algorithms have some parameters that can be tweaked and affect the efficiency of the algorithm. The properties that affect how an optimiser can find the solution are called the hyperparameters. They are not parameters of the objective function, but they affect the results obtained. Fewer hyperparameters is usually better, as it is less cumbersome to tune the optimiser to work. A perfect optimiser would have no hyperparameters.</p>
<h2>Random Search</h2>
<p>The simplest optimisation algorithm is random searching. It makes no assumptions other than we can draw random samples from the parameter space. Its process is simple:</p>
<p></p>
<ul>
    <li>Guess a random parameter \(\theta\)</li>
    <li>Check the objective function \(L(\theta\)</li>
    <li>If better than the previous parameter, change to this one</li>
</ul>
<p>There are many possible termination conditions, e.g. certain number of iterations</p>
<p><strong>Pros:</strong></p>
<p></p>
<ul>
    <li>Cannot get trapped in a local minimum (as there is no structure in the algorithm)</li>
    <li>Requires no knowledge of the objective function&nbsp;</li>
    <li>Very simple to implement</li>
    <li>Better than grid search (almost) always</li>
</ul>
<p><strong>Cons:</strong></p>
<p></p>
<ul>
    <li>Extremely inefficient, and only appropriate if there is on structure in data to exploit.</li>
    <li>Must be possible to random sample the parameter space</li>
    <li>Results do not necessarily get better, i.e.&nbsp; there is no way to predict how the optimisation would proceed</li>
</ul><br>
<h1>Metaheuristics</h1>
<p><br></p>
<p>We can use metaheuristics to improve random searching.</p>
<p></p>
<ul>
    <li><strong>Locality</strong>: If the objective function is continuous, changing the parameters slightly will only change the value of the objective function slightly;</li>
    <li><strong>Temperature:&nbsp;</strong>&nbsp;We can change the rate of movement on the parameter space as the optimisation proceeds, assuming there exists a local minimum.&nbsp;</li>
    <li><strong>Population</strong>: We can keep track of multiple parameter configurations at the same time, and mix them around to combine the best of bost sets</li>
    <li><strong>Memory</strong>: We can keep track of good and bad steps we took and try avoid/revisiting them.&nbsp;</li>
</ul>
<p></p>
<h2>Locality</h2>
<p>Local searching algorithms make incremental changes towards a local minimum. If the function is continuous, these are much more efficient that random/grid searching. But, since they only find local minima, there is no guarantee that the minima is global, there may be a smaller value the function attains, but the iterative algorithm gets stuck in another, larger local minima</p>
<p>The local minima we find depends on the initial conditions. Local searching creates a trajectory through the parameter space such that the objective function is getting smaller. The following graph illustrates these two points</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/298748084/image%20%2845%29.png" alt="" width="600" height="334" class="img-fluid atto_image_button_text-bottom"><br></p>
<p><strong>Hill Climbing </strong>is a local searching algorithm. It modifies the random searching algorithm and assumes some topology of the parameter space so we can find a neighbourhood around the parameter vector. This allows us to make incremental changes to it. Instead of drawing random samples, we sample near the current best parameter in hopes of finding an even more optimal configuration. We only change the parameters given that the loss function gets smaller.</p>
<p>Simple hill climbing only changes one of the parameter vector elements in one go, examining each direction in turn. Stochastic hill climbing makes a random change to the parameter vector (i.e. all the components get changed) It either accepts or rejects the change depending on whether the loss function gets smaller.</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/298748084/image%20%2846%29.png" alt="" width="600" height="1200" class="img-fluid atto_image_button_text-bottom"><br></p>
<h3>Pros.</h3>
<p></p>
<ul>
    <li>Not much more complicated than random searching</li>
    <li>Can be much faster than random searching</li>
</ul>
<p></p>
<h3>Cons.</h3>
<p></p>
<ul>
    <li>Hard to choose how much of an adjustment to make</li>
    <li>Can get stuck in a local, non-global minimum</li>
    <li>Struggles with objective functions that are relatively flat</li>
    <li>Requires the function to be (mostly) continuous</li>
</ul>
<p>We can tweak the algorithm to improve it. We can use adaptive local search, the size of the neighborhood can be adapted, e.g. if there was no improvement made for some number of iterations. Also, we can use multiple restart, starting at different positions to avoid getting stuck at a local minimum</p>
<p></p>
<h2>Temperature</h2>
<p>Simulated annealing extends hill climbing with the ability to make bad choices (go uphill) instead of always making good choices (going downhill). This is to help overcome ridges and getting stuck at a local minimum.&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/298748084/image%20%2847%29.png" alt="" width="750" height="511" class="img-fluid atto_image_button_text-bottom"><br></p>
<p>The following figure illustrates the simulated annealing algorithm to find the line fit through the data above, using the temperature schedule.&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/298748084/image%20%2848%29.png" alt="" width="500" height="930" class="img-fluid atto_image_button_text-bottom"></p>
<p><br></p>
<h2>Population</h2>
<p>We can apply an analogue of evolution in optimisation, by:</p>
<p>
</p>
<ul>
    <li>Mutating (introducing random variables)</li>
    <li>Natural Selection (solution selection)</li>
    <li>Breeding (interchange between solutions)</li>
</ul>
<p>Algorithms that use these techniques are called genetic algorithms. These algorithms maintain a population of potential solutions, and some rule to preserve some and to cull the others. The parameter set is called the genotype of a solution.</p>
<p>A simple selection rule would be: Keep the top 25% solutions, ordered by loss. Each iteration would add some mutation, cull the weakest solution, and copy the remaining solutions a number of times to produce the offspring for the next step. The size of the large population is constant during each iteration. Using this, we can explore a large area of the space rather than a simple local search.&nbsp;</p>
<p>More advanced algorithms introduce some form of breeding or crossover. In crossover, two (or more) parameter vectors are merged to give a child parameter vector. Crossover works well when the parameter space can be partitioned into multiple components, so that the offspring can inherit good qualities from the parents. It works less well when the crossover simple becomes a mismatch of parent qualities that average out.</p>
<p>The pros of genetic algorithms are:&nbsp;</p>
<ul>
    <li>Easy to understand and applicable to many problems&nbsp;</li>
    <li>Requires only weak knowledge of the objective function</li>
    <li>Can be applied to problems with both discrete and continuous components&nbsp;</li>
    <li>Some robustness against local minima, although hard to control and guarantee</li>
    <li>Great flexibility in parameterisation such as mutation schemes, crossover schemes, fitness functions, selection functions, ect.&nbsp;</li>
</ul>
<p>The cons are:<br>
</p>
<ul>
    <li>Many hyperparameters that affect the performance of the optimisation</li>
    <li>No guarantee of convergence</li>
    <li>Very slow compared to using stronger knowledge of the objective function</li>
    <li>Many evaluations of the objective function required: one per population member per iterations</li>
</ul>
<p><br></p>
<p></p>
<h2>Memory<br></h2>
<p>The optimisation algorithms we have seen so far are memoryless. They investigate some part of the solutions space, check the loss, then move on. They may end up checking the same, or very similar solutions over and over again. We can use some memory, where the optimiser remembers good and bat bits of the parameter space, especially the good paths in the solution space. We want to have a population of parameter sets, and a memory of good paths through the space.&nbsp;</p>
<p>Pros:&nbsp;</p>
<p></p>
<ul>
    <li>Can be very good when solutions are separated by large, narrow valleys</li>
    <li>Can use fewer evaluations of the objective function than genetic algorithm if pheromones are used.</li>
    <li>When it works, <em>it really works</em></li>
</ul>
<p>Cons:<br></p>
<ul>
    <li>Moderately complex algorithm to implement&nbsp;</li>
    <li>No guarantee of convergence&nbsp;</li>
    <li>Even more hyperparameters&nbsp;</li>
</ul>
<p><br></p>
<h1>Quality of Optimisation</h1>
<p><br></p>
<h2>Convergence</h2>
<p>We say that an optimisation algorithm converges to a solution. In convex optimisation, we find the global maximum and the problem is solved. In non-convex optimisation, we have found a local maximum and are stuck.</p>
<p>A good optimisation algorithm converges quickly. This means that the objective function drops steeply, each iteration makes a big difference. A bad optimisation algorithm does not converge at all (it wanders forever, or diverges to infinity). We compare the convergence of the algorithms we saw above in solving the linear regression problem above.&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/298748084/image%20%2849%29.png" alt="" width="450" height="281" class="img-fluid atto_image_button_text-bottom"><br></p>
<p>Most optimisation algorithms only converges in the right conditions. The convergence depends on the initial conditions of the optimisation. Some algorithms are guaranteed to converge if a solution exists, while others (most heuristic optimisation algorithms) are not guaranteed to converge even if there is a solution. For iterative algorithms, it is possible to plot how the objective function is decreasing as the iteration proceeds to determine convergence problems. Ideally, the loss should drop as fast as possible.</p>
<p><br></p>
<h2>Tuning Optimisation</h2>
<p>To use an optimisation algorithm efficiently, we need to tune its hyperparameters. If we know the problem is of specific sort, then we should use the algorithm that converges the best for it, e.g.</p>
<p></p>
<ul>
    <li>If the problem is least-squares, use a least-squares solver (it might even be possible to solve it directly using pseudo-inverse)<br></li>
    <li>If the problem is convex, use a convex solver (a convex solver is much more efficient than a normal solver)</li>
    <li>If the problem is differentiable, use a first order method</li>
    <li>If none of these are known, use a genetic algorithm or simulated annealing.</li>
</ul>
<p>Many things could go wrong however, for example, the algorithm make slow progress. This can happen in local searching if the step size is too small, e.g. in gradient descent, a small step size would only change the objective function a small amount, meaning we are only searching a small portion of the space.&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/298748084/image%20%2850%29.png" alt="" width="400" height="275" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">The algorithm also may have noisy and divergent performance. Local searching can become unstable, especially if the jumps are too large,&nbsp; or if the objective function is infinitely decreasing.&nbsp;</p>
<p style="text-align:left;">The algorithm may get stuck at critical points of the functions</p>
<p style="text-align:left;"></p>
<ul>
    <li>Plateaus (neighborhood where the function is constant) : A memoryless algorithm might wander around, and a derivative-based one gets stuck. We can use momentum and other forms of memory to limit this effect.</li>
    <li>Local Minima : Local searching can get trapped in local minima. We can use random restart to limit this effect</li>
    <li>Saddle points (the function is increasing in one direction, and decreasing in some other direction) : This can slow or trap gradient descent methods since it will have a hard time trying to find the best direction to travel&nbsp;</li>
    <li>Very steep/discontinuous : This can produce huge barriers for gradient descent. Stochastic gradient descent can blur out these boundaries and still make progress.</li>
</ul>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/298748084/image%20%2851%29.png" alt="" width="500" height="576" class="img-fluid atto_image_button_text-bottom"><br></p>
<p></p>
<p></p>
<h1>Deep Neural Networks</h1>
<p><br></p>
<p>Deep learning or deep neural networks have become a major part of modern machine learning research. They have had astonishing success in fields like speech recognition, machine translation, image classification, and image synthesis.</p>
<p>In deep learning, we essentially aim to approximate a given function. For instance, we might be given a set of observations \( x_1,x_2,...,x_n \) and corresponding outputs&nbsp;\( y_1,y_2,...,y_n \), and we want to find a function&nbsp;\( y^\prime = f(x,\theta) \) where \(x\) is some observation and \(\theta\) is a set of parameters, such that</p>
<p style="text-align:center;">\( \theta^\ast = \underset{\theta}{\text{argmin}}\displaystyle\sum_i||f(x_i,\theta) = y_i|| \)<br></p>
<p style="text-align:left;">We can then use the function \(f\) to predict outputs for unseen observations. Clearly, it is an optimisation problem. The difficulty here is that we have millions of parameters, so we need to come up with a way to optimise in such a large vector space in a reasonable amount of time.&nbsp;</p>
<p style="text-align:left;">The way we deal with this is that the networks are constructed in a simple way. A neural network typically consists of layers, each of which is a linear map followed by a simple, fixed, non-linear function; such as rotate, stretch(linear map), and fold(simple fixed nonlinear folding)&nbsp; The output of one layer is the input of the next.&nbsp;</p>
<p style="text-align:left;">The linear map in each layer is specified by a matrix (known as a weight matrix). The network is completely parameterised by the entries of the weight matrices for each layer (all of these matrices can be seen as the parameter vector \(\theta\) ). The nonlinear function \(G(x)\) is fixed for each layer and cannto vary.&nbsp;</p>
<p style="text-align:left;">The reason this is efficient that the derivative of the objective function with respect to the weights can be computed for every weight in the network at the same time. This is called backpropagation, and it is an algorithm for automatic difference.</p>
<p style="text-align:left;"><br></p>
<h2>Heuristic Searching</h2>
<p>Heuristic searching methods are easy to understand and implement, and works in most cases. However,</p>
<p></p>
<ul>
    <li>It can be very slow. It can take many iterations to approach a minimum and require significant computation to compute each iteration.</li>
    <li>There is no guarantee of convergence (or even progress). The search can get stuck, or drift over plateaus</li>
    <li>There are a lot of hyperparameters that can be tweaked (temperature schedules, size of population, memory structure, etc.). Optimal choice of these parameters becomes an optimisation problem in itself.</li>
</ul>
<p>For optimisation problems like deep neural networks, heuristic search is inadequate. It is simply too slwo to make progress in training networks with millions of parameters. Instead, first-order optimisation is applied. First order algorithms can be orders of magnitude faster than heuristic search.&nbsp;</p>
<p><br></p>
<h2>Rolling a Ball</h2>
<p>An intuition for higher-order optimisation can be formed by considering aball rolling on a (smooth) surface, which represents the value of the objective function across a 2D domain (i.e. if we had parameter vector \(\theta\) with two elements). The ball will eventually settle in a configuration where there is a balanced set of forces applied to it. This will happen, for examlpe, if the ball settles at a minimum where the normal of the surface is parallel with gravity.&nbsp;</p>
<p><br></p>
<h1>Derivatives</h1>
<p><br></p>
<p>If the function \(f\) is a scalar function, then \( f^\prime(x) = \frac{dy}{dx} \) is the first derivative with respect to \(x\). The second derivative with respoect to \(x\) is&nbsp;\( f^{\prime\prime} = \frac{d^2y}{dx^2} \). In general, if the function \(f\) maps to \(\mathbb{R}^n\), then we have an \(n \times n\) derivatives matrixs, called the <strong>Jacobian, </strong>i.e.</p>
<p style="text-align:center;">\( f'(\vec{x}) = \vec{J} = \begin{bmatrix}
    \frac{\partial y_0}{\partial x_0} &amp; \frac{\partial y_0}{\partial x_1} &amp; \dots &amp; \frac{\partial y_0}{\partial x_n} \\
    \frac{\partial y_1}{\partial x_0} &amp; \frac{\partial y_1}{\partial x_1} &amp; \dots &amp; \frac{\partial y_1}{\partial x_n} \\
    \dots \\
    \frac{\partial y_m}{\partial x_0} &amp; \frac{\partial y_m}{\partial x_1} &amp; \dots &amp; \frac{\partial y_m}{\partial x_n} \\
    \end{bmatrix} \)<br></p>
<p>The gradient vector for the function is one rwo of the Jacobian matrix. It tells us how \(f\) would vary as we change one of the parameters with respect to each dimension, seprately. We will only consider functions going from \(\mathbb{R}^n\) to \(\mathbb{R}\), so&nbsp;</p>
<p style="text-align:center;">\( \nabla L(\vec{\theta}) = \left[ \frac{\partial L(\vec{\theta})}{\partial \theta_1},
    \frac{\partial L(\vec{\theta})}{\partial \theta_2}, \dots, \frac{\partial L(\vec{\theta})}{\partial \theta_n},\right] \)<br></p>
<p>If \(L(\theta)\) is a map \(\mathbb{R}^n\) to \(\mathbb{R}\), then&nbsp;\( \nabla L(\theta) \) is a map&nbsp;\( \mathbb{R}^n \rightarrow \mathbb{R}^n \). If \(L(\theta)\) is a map&nbsp;\( \mathbb{R}^n \rightarrow \mathbb{R}^m \), then&nbsp;\( \nabla L(\theta) \) is a map&nbsp;\( \mathbb{R}^n \rightarrow \mathbb{R}^{m\times n} \)</p>
<p>The <strong>Hessian </strong>matrix is the Jacobian of a gradient vector, denoted by&nbsp;\( \nabla^2 f(x) \). It is equivalent to the second derivative for vector functions. The function&nbsp;&nbsp;\( \nabla^2 L(\theta) \) is a map&nbsp;\( \mathbb{R}^n \rightarrow \mathbb{R}^{m \times n} \)</p>
<p style="text-align:center;">\( H(L) = \nabla \nabla L(\vec{\theta}) = \nabla^2 L(\vec{\theta}) =
    \begin{bmatrix}
    \frac{\partial^2 L(\vec{\theta})}{\partial \theta_1^2} &amp;
    \frac{\partial^2 L(\vec{\theta})}{\partial \theta_1\partial \theta_2} &amp;
    \frac{\partial^2 L(\vec{\theta})}{\partial \theta_1\partial \theta_3} &amp; \dots &amp;
    \frac{\partial^2 L(\vec{\theta})}{\partial \theta_1\partial \theta_n}\\
    \frac{\partial^2 L(\vec{\theta})}{\partial \theta_2\partial\theta_1} &amp;
    \frac{\partial^2 L(\vec{\theta})}{\partial \theta_2^2} &amp;
    \frac{\partial^2 L(\vec{\theta})}{\partial \theta_2\partial \theta_3} &amp; \dots &amp;
    \frac{\partial^2 L(\vec{\theta})}{\partial \theta_2\partial \theta_n}\\
    \dots\\
    \frac{\partial^2 L(\vec{\theta})}{\partial \theta_n\partial\theta_1} &amp;
    \frac{\partial^2 L(\vec{\theta})}{\partial \theta_n\partial \theta_2} &amp;
    \frac{\partial^2 L(\vec{\theta})}{\partial \theta_n\partial \theta_3} &amp; \dots &amp;
    \frac{\partial^2 L(\vec{\theta})}{\partial \theta_n^2}\\
    \end{bmatrix} \)<br></p>
<p style="text-align:center;"><br></p>
<h2>Differentiable Objective Functions</h2>
<p>For some objective functions, we can compute the exact derivatives of the objective function with respect to the parameters \(\theta\). For example, if \( L(\theta) = \theta^2 \), then \( L^\prime(\theta) = 2\theta \). We can use the derivative to move the parameters to the right direction. For multi-valued functions, we have a gradient vector instead of a scalar derivative. There are 3 types of algorithms:</p>
<p></p>
<ul>
    <li>Zeroth order optimisation algorithms only require evaluation of \(L(\theta)\), e.g. random search and simulated annealing</li>
    <li>First order optimisation algorithms require evaluation of \(L(\theta)\) and its derivative \(\nabla L(\theta)\). This class of algorithms are called gradient descent algorithms.&nbsp;</li>
    <li>Second order optimisation algorithms requireevaluation of \(L(\theta)\), \(\nabla L(\theta)\), and&nbsp;\(\nabla^2 L(\theta)\). This class of algorithms are called quasi-Newtonian algorithms.&nbsp;</li>
</ul>
<p><br></p>
<p></p>
<h1>Optimisation With Derivatives</h1>
<p><br></p>
<p>If we know or can compute the gradient of an objective function, we know the slope of a function at a given point. This gives us the direction of the fastest increase and the steepness of the slope. Knowing the derivative of the objective function massively improves the optimisation algorithm. We can go fown the direction of the fastest decrease of the value in hopes of finding the global minimum.</p>
<h2>Conditions</h2>
<p>A smooth function has continuous derivates up to some order. Smother functions are easier to do iterative optimisation on, because small changes in the current approximation can likely lead to small changes to the objective function. We say that a function \(C^n\) is continuous if the \(n\)-th derivative is continuous. The following images illustrate some \(C^n\) functions.&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/298748084/image%20%2852%29.png" alt="" width="400" height="202" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">There is a difference between having continuous derivatives and knowing what those derivatives are. First order optimisatino uses the first derivatives of the ojbective function with respect to the parameters. These techniques can only be applied if the objective function is</p>
<p style="text-align:left;"></p>
<ul>
    <li>At least \(C^1\) continuous (no step changes anywhere in the function or its derivative&nbsp;</li>
    <li>Differentiable (gradient is defined everywhere)</li>
</ul>
<p>In practice, these constraints are relaxed, as we will see. Many objective functions satisfy these conditions, and first-order methosd can be vastly more efficient than zeroth-order methods. For a particular class of functions (e.g. convex), there are known bounds on number of steps required to converge for specific first-order optimisers.&nbsp;</p>
<p>For first-order optimisation, we put a stronger requirement on functions thatn just \(C^1\) continuity and require the functions to be Lipshitz continuous. For a function \(\mathbb{R}^n \rightarrow \mathbb{R}\) (like the objective function) is equivalent to saying that the gradient is bounded and the function cannot change more quickly than a constant. There is a maximum steepness. The Lipshitz constant \(K\) for a function \(f\) is given by&nbsp;</p>
<p style="text-align:center;">\( K = \text{sup}[\frac{|f(x) - f(y)|}{|x-y|}] \)<br></p>
<p style="text-align:left;">This is the smallest value larger than every value of the function. A small \(K\) means that the function is smooth. If \(K = 0\), then it is flat. We do not need to know the value of \(K\) to compute the derivative, we just need to know that it exists.&nbsp;</p>
<p style="text-align:left;"><br></p>
<p></p>
<h2>Analytical Derivative</h2>
<p>There are manual techniques to compute the derivative and optimise a function \(f : \mathbb{R} \rightarrow \mathbb{R}\)</p>
<p></p>
<ul>
    <li>Compute the derivative \( f^\prime(x) \)</li>
    <li>Solve for&nbsp;&nbsp;\( f^\prime(x) = 0\) These are turning points/optima of the function&nbsp;</li>
    <li>Check if any of thse \(x\)-values satisfy&nbsp;\( f^{\prime\prime}(x) &gt; 0 \). Then, the \(x\) value is a local minimum</li>
</ul>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/298748084/image%20%2853%29.png" alt="" width="500" height="185" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">In this case, the optimisation process just takes one step, there is no iteration. However, we normally do not have a closed form for the derivative; we can only evaluate the derivative at some point. In this case, we can still dramatically accelerate optimisation by taking steps such that we "run downhill" as fast as possible. To do this, we need to be able to compute the gradient at any point on the objective function.&nbsp;</p>
<p style="text-align:left;">Essentially, for an objectiive function&nbsp;\( f : \mathbb{R}^n \rightarrow \mathbb{R} \), we will have a gradient function&nbsp;\( f : \mathbb{R}^n \rightarrow \mathbb{R}^n \). The gradient function takes in the same parameter. At a given point, the gradient of a function in the direction where the function increases the fastest. The magnitude of this vector is the rate at which the function is changing.&nbsp;</p>
<p style="text-align:left;"><br></p>
<p></p>
<h1>Gradient Descent</h1>
<p><br></p>
<p>The basic first-order algorithm is called gradient descent, and involves iterations, like in random searching, we start with some initial guess \(\theta_0\) and try to converge to the minimum using the equation&nbsp;</p>
<p style="text-align:center;">\( \theta_{n+1} = \theta_n - \partial \nabla L(\theta_n) \)<br></p>
<p style="text-align:left;">Here, \(\partial\) is the step size (a hyperparameter), this might be fixed or adaptive. By construction, the optimiser moves towards the point where the objective function drops most quickly. It is possible that going downhill may not be the fastest route. Nonetheless, gradient descent can be very fast. In gradient descent, we first choose a guess. Then, we loop, using the equation above to minimise the loss function.&nbsp;</p>
<p style="text-align:left;">The step size is crucial for convergence. If the step size is too small, then the convergence is too slow, as shown below</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/298748084/image%20%2854%29.png" alt="" width="500" height="259" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">Instead, if the step size is big, the optimisation process will oscillate and be slow as well.&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/298748084/image%20%2855%29.png" alt="" width="500" height="259" class="atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">Moreover, if the step size is too large, the optimisation process might not have the expected behaviour, it might go part the area where the minima is. The optimality of the step size is related to Lipschitz constant. We typically do not know the constant, so the step size is often set by approximate methods such as line search. We can generalise gradient descent to higher dimensions using the same strategy&nbsp;</p>
<p style="text-align:left;">&nbsp;</p>
<h2>Using Numerical Differences</h2>
<p>To use first-order optimisation, we need to know the derivative of a function. Although there is no direct way of applying this in the real world, we can apply it to the computation model so that it can be optimised. The definition of the derivative at a point is&nbsp;</p>
<p style="text-align:center;">\( f^\prime(x) = \underset{h\rightarrow 0}{\text{lim}}\frac{f(x+h)-f(x-h)}{2h} \)<br></p>
<p style="text-align:left;">Using small values for \(h\), we could potentially derive every function. This approach is called numerical differentiation, and these are finite differences. Numerical difference works fine in 1D cases, as shown</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/298748084/image%20%2856%29.png" alt="" width="500" height="482" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">In the first graph, we use different values of \(h\) to approximate the derivative. The values are quite close most of the time. The second graph shows the difference between the actual derivative and the approximations. In the second graph, we can see that as \(h\) gets smaller, it is not always the case that the error decreases. This is because there are numerical issues when computing the value</p>
<p style="text-align:center;">\( \frac{f(x+h)-f(x-h)}{2h} \)<br></p>
<p style="text-align:left;"></p>
<ul>
    <li>It adds a small number \(h\) to a potentially much larger number \(x\) (magnitude error)</li>
    <li>It then subtracts two very similar numbers \(f(x+h)\) and&nbsp;&nbsp;\(f(x-h)\) (cancellation error)</li>
    <li>Then it divides the result by a very small number \(2h\) (division magnification)</li>
</ul>
<p>Moreover, in higher dimensions, the curse of dimensionality reappears. To evaluate the gradient at \(x\), we need to compute numerical differences in each dimension. So, if \(\theta\) has one million dimensions, then each individual derivative evaluation would require two million evaluations of \(L(\theta)\). This would be too slow and the efficiency of gradient descent would be lsot.&nbsp;</p>
<p>Although gradient descent is much more efficient than zeroth-order methods, it has some disadvantages:</p>
<p></p>
<ul>
    <li>The gradient of the objective function must be computable at all points. Automatic differentiation helps with this</li>
    <li>Gradient descent can get stuck at local minima. Gradient descent is meant to just find local minima (only in convex function can we guarantee that a local minimum is global). Random restart and momentum can be used to counter this</li>
    <li>Gradient descent only work with smooth, differentiable objective functions. Stochastic gradient descent allows us to differentiate steep functions by adding noise</li>
    <li>Gradient descent can be slow if the objective function takes a long time to be evaluated. Stochastic gradient descent can massively improve this, given that the objective function can be written as the sum of smaller subproblems.</li>
</ul>
<p></p>
<h2>Automatic Differentiation</h2>
<br>If we had a closed form for the derivative of the objective function, this would be a simple task. However, it is not feasible to work out the closed for when we are dealing with hundreds of dimensions. Automatic differentiation algorithms take a function and automatically construct a function that evaluates the exact derivative of the function at a given point. We can use the package
<pre>autograd</pre> to allow us to differentiate a function
<p> The following figure illustrates the gradient descent to find the line fit through the data above.</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/298748084/image%20%2857%29.png" alt="" width="500" height="925" class="img-fluid atto_image_button_text-bottom"><br></p>
<h2>Limits to automatic differentiation</h2>
<p>We need the function to be differentiable to apply gradient descent. While it is possible to compute first-order derivatives in reasonable time, we will see later this is not feasible in a multi-dimensional setting.</p>
<p>Stochastic relaxation can make an impossibly steep gradient Lipschitz continuous by integrating over many different random conditions. The figure below explains how we relax a non-differentiable function and derive it</p>
<p><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/298748084/image%20%2859%29.png" alt="" width="600" height="300" class="img-fluid atto_image_button_text-bottom"><br></p>
<h1>Stochastic Gradient Descent</h1>
<p><br></p>
<p>In gradient descent, we have to evaluate the function and the gradient during each iteration. This can make the process quite long, e.g. if the data has thousands of dimensions If we can break the objective function into smaller chunks, then the optimiser can do gradient descent on each of those chunks separately, this might be much faster. This is called the stochastic gradient descent, because the step it takes depends on a random part of the objective function. So, assume that the objective function is of the form&nbsp;</p>
<p style="text-align:center;">\( L(\theta) = \displaystyle\sum_i{L_i(\theta)} \)</p>
<p style="text-align:left;"><span style="font-size:0.9375rem;">This is quite common when matching parameters to observations. For instance, in machine learning, we have some training sets \(x_i\) with \(y_i\), and we want to find the parameter vector \(\theta\) such that&nbsp;</span></p>
<p style="text-align:center;"><span style="font-size:0.9375rem;">\( L(\theta) = \displaystyle\sum_i || f(x_i,\theta)-y_i|| \)</span></p>
<p style="text-align:left;"><span style="font-size:0.9375rem;">Is minimised. We try to minimise the distance between the inputs and the outputs elementwise. Differentiating is a linear operation, meaning that&nbsp;</span></p>
<p style="text-align:center;"><span style="font-size:0.9375rem;">\( \frac{d}{dx}(af(x) + bg(x)) = af^\prime(x)+bg^\prime(x) \)&nbsp;&nbsp;</span></p>
<p style="text-align:left;">Therefore,&nbsp;</p>
<p style="text-align:center;">\( \nabla \displaystyle\sum_i || f(x_i,\theta)-y_i|| = \displaystyle\sum_i \nabla || f(x_i,\theta) - y_i|| \)<br></p>
<p style="text-align:left;">So, we can take a subset of the dataset, and compute the gradient for each sample/subset. Over time, the randomness will average out. Each subset is a minibatch, and one run (i.e. every dataset has been seen by the optimiser) through the dataset is an epoch.</p>
<p style="text-align:left;"><br></p>
<h2>Memory Advantages</h2>
<p>Stochastic gradient descent (SGD) has major advantages in terms of memory since computations are only applied to a small number of samples in minibatches. We just compute the gradient on a subsample, and move in that direction. It won't be exactly the right derivative of the whole objective function, but it will be a good approximation.&nbsp;</p>
<p>Particularly on good memory constrained devices such as GPUs, it can be impossible to store the entire. Splitting up into batches can get around this limitation. It can also have advantages with respect to the memory hierarchy, small batches of data may induce fewer cache misses and thus result in enhanced performance.</p>
<p><br></p>
<h2>Heuristic enhancement</h2>
<p>SDG can improve the performance of optimisation in terms of objective function decrease. In particular, it reduces the likelihood of getting stuck in minima. This is because random partitioning adds noise to the dataset. This allows the gradient descent to now always go downhill, but also to jump over a hill.&nbsp;<br></p>
<p>Adding noise is a heuristic approach, it is very effective. This is a limited version of stochastic relaxation. The function gets smoothened out by averaging over random subsamples. This means that if the function is not Lipschitz continuous (or has a high Lipschitz constant), then SGD can still work well when gradient descent wouldn't.</p>
<p>&nbsp;</p>
<h2>Using SGD</h2>
<p>There is no guarantee that SGD will move in the right direction. However, it is normally effective in practice. We can break the linear regression example into multiple subproblems by choosing a few random points at a time. The following figure illustrates the SGD to find the line fit through the data.</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/298748084/image%20%2860%29.png" alt="" width="600" height="1092" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:center;"><br></p>
<h1>Random Restart and Momentum</h1>
<p><br></p>
<p>We will try to apply gradient descent to a function which has:</p>
<p></p>
<ul>
    <li>Narrow valleys everywhere&nbsp;</li>
    <li>Multiple local minima&nbsp;</li>
    <li>A huge ridge through the middle</li>
</ul>
<p>The graph of this function is shown below:</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/298748084/image%20%2861%29.png" alt="" width="500" height="492" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">Clearly, gradient descent wasn't able to find a minimum. It gets stuck in a valley and wanders in a huge arc, never approaching even the local minima. We cant apply stochastic gradient descent either, the function cannot be broken into multiple segments.&nbsp;</p>
<p style="text-align:left;">We can use random restart instead. Gradient descent gets stuck at local minima. SGD can help if the minimum isn't too deep. A simple heuristic is to run gradient descent until it gets stuck, then starting with a different initial condition. We try this many times and try to find the global minimum. We can use random restart with any iterative optimisation algorithm.&nbsp;</p>
<p style="text-align:left;">Another approach is to use momentum. In real life, if we throw a ball down some hill, it will keep going down even if there are some bumps on the way, it has momentum and will keep going in that direction. We use a similar approach in optimisation algorithms. This is another form of memory, we introduce velocity \(v\). The optimiser moves in this direction. We gradually move the velocity to align with the derivative.&nbsp;</p>
<p style="text-align:center;">\( v = \alpha v + \delta \nabla L(\theta) \)<br></p>
<p style="text-align:left;">In this case, the next value for \(\theta\) is :</p>
<p style="text-align:center;">\( \theta_{i+1} = \theta_i - v \)<br></p>
<p style="text-align:left;">The value \(\alpha\) controls the change in velocity. If \(\alpha\) is close to 1, there is more momentum in the system. If instead \(\alpha\) is close to 0, this is pretty much the normal gradient descent.</p>
<p style="text-align:left;">Momentum will give the following traceback.</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/298748084/image%20%2863%29.png" alt="" width="500" height="494" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:center;"><br></p>
<p></p>
<h1>Second Order Derivatives</h1>
<p><br></p>
<p>The first order derivatives represent the slope of the function. The second order derivatives represent the curvature of the function. For every parameter component \(\theta_i\), the Hessian stores how the steepness of every other \(\theta_j\) changes.</p>
<p>If a person is on a hill,</p>
<p></p>
<ul>
    <li>The altitude is the value of the objective function</li>
    <li>The parameters that can change are position (e.g. north, south, east, west)</li>
    <li>The gradient vector is the change in altitude as the person steps in one of the two directions. This is the local steepness</li>
    <li>The Hessian captures how much steeper the hill gets stepping in some direction. There are two independent directions, so the Hessian is a \(2 \times 2\) matrix</li>
</ul>
<p>The gradient vector captures the first order derivatives, while the Hessian matrix captures the second order derivatives. For every pair of dimensions, there is an entry in the Hessian matrix. The hessian matrix captures how gradients of a function vary with respect to each other.&nbsp;</p>
<p>For a 2D surface, the gradient vector specifies the normal of the plane tangent to the surface at a given point. The Hessian matrix specifies a quadratic function (a smooth curve with one minima) following the curvature of the surface.&nbsp;</p>
<p><br></p>
<h2>Eigenvalues of the Hessian</h2>
<p>The eigenvalues of the Hessian allow us to characterise the type of stationary point. In particular,</p>
<p></p>
<ul>
    <li>If all the eigenvalues are strictly positive (i.e. positive definite), then the point is a minimum</li>
    <li>If all the eigenvalues are strictly negative (i.e. negative definite), then the point is a maximum<br></li>
    <li>If the eigenvalues have mixed signs, the point is a saddle point</li>
    <li>If the eigenvalues are all positive/negative, but with some zeroes, the matrix is semidefinite and the point is a plateau/ridge</li>
</ul>
<p><br></p>
<p></p>
<h2>Second Order Optimisation</h2>
<p>Second order optimisation uses the Hessian matrix to jump to the bottom of each local quadratic approximation in a single step. This can skip over flat plains and escape from saddle points that slow down the gradient descent. Second order derivatives are much faster in general than first-order methods. The following graph compares some optimisation algorithms.</p>
<p><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/298748084/image%20%2864%29.png" alt="" width="700" height="376" class="img-fluid atto_image_button_text-bottom"><br></p>
<p>Random searching does quite a bad job. Hill climbing does decrease the objective function significantly, but is still not that good. Gradient descent does quite a good job, but newton-CG (a second order optimisation algorithm) does a much better job.&nbsp;</p>
<p>Second order optimisation doesn't work in higher dimensions. Evaluating the Hessian takes \(d^2\) computations, and \(d^2\) storage. This isn't possible if \(d\) is large.</p>
<p>So, second order can move much more quickly through saddle points and plateaus than first order methods like gradient descent. However, it is not possible to use second order optimisation in high dimensions due to the curse of dimensionality.</p>
<p><br></p>
<h1>Introduction to Probability</h1>
<p><br></p>
<p>Probability is concerned with stochastic elements, the role of uncertainty, randomness, and statistics in computation. The study of probability is called <strong>probability theory</strong>. It allows us to deal with uncertain values and infer the most likely hypothesis from the observations. The following terms are used in probability theory:</p>
<p>
</p>
<ul>
    <li><strong>Experiment/Trial</strong>&nbsp;: An occurrence of an uncertain outcome</li>
    <li><strong>Outcome</strong>&nbsp;: The result of an experiment/one state of the world&nbsp;</li>
    <li><strong>Sample space</strong>: All the possible outcomes of an experiment</li>
    <li><strong>Event</strong>&nbsp;: A subset of possible outcomes with some common property</li>
    <li><strong>Probability</strong>&nbsp;: The probability of an event with respect to the sample space is the number of outcomes from the sample space that are in the event, divided by the total number of outcomes in the sample space. Since it is a ratio, it will be a number between 0 (impossible event) and 1 (certain event)&nbsp;</li>
    <li><strong>Probability Distribution</strong>&nbsp;: A mapping of the outcomes to probabilities. The sum of the probability distributions is 1. A trial must result in an outcome (with probability 1), so all of the possible outcomes is 1. A random variable has a probability distribution that maps each outcome to a probability&nbsp;</li>
    <li><strong>Random Variable</strong>&nbsp;: A variable representing an unknown value, whose probability distribution we know. The variable is associated with outcomes from a trial.&nbsp;</li>
    <li><strong>Observation</strong>&nbsp;: An outcome we have observed directly, i.e. the dataset</li>
    <li><strong>Sample</strong>&nbsp;: An outcome we have simulated according to a probability distribution&nbsp;</li>
    <li><strong>Expectation/Expected Value</strong>&nbsp;: The average value of a random variable&nbsp;</li>
</ul>
<p>A random variable \(X\) has probability distribution \(P(X)\), which assigns probabilities&nbsp;\( 0 \leq (P(X = x) \leq 1 \) to outcomes \(x\), which belongs in some sample space \( \text{x} \). The probability distribution is defined by a probability mass/density function&nbsp;\( f_\text{x}(x) \) which assigns probabilities to outcomes such that the sum of probabilities over all the possible outcomes is</p>
<p style="text-align:center;"><span style="font-size:0.9375rem;">\( \displaystyle\sum_{x\in X}f_\text{x}(x)=1 \)</span></p>
<p style="text-align:left;"><span style="font-size:0.9375rem;">We can observe specific outcomes \(x_i\) drawn from a distribution as a result of trials. We can sample new outcomes&nbsp;\( x_j^\prime \) given a distribution \(P(X)\). Assuming outcomes have values, we can evaluate the average expected value&nbsp;\( E[X] \) across infinitely many trials&nbsp;</span></p>
<p style="text-align:left;"><span style="font-size:0.9375rem;"><br></span></p>
<h1>Philosophy of Probability</h1>
<p><br></p>
<p>There are two types of probability, <strong>Bayesian</strong>&nbsp;and <strong>Frequentist</strong></p>
<p><strong><br></strong></p>
<h2>Bayesian Probability</h2>
<p>Bayesian probability is the calculus of belief. In this model, probabilities are measured in degrees of belief. \(P(A) = 0\) means that the event cannot be true, and \(P(A) = 1\) means that the event is certain. It makes sense, in Bayesian perspective, to say that the chance of rain today is 0.3 (even though this is a one-off event). Moreover, this doesn't mean that it is raining 30%, it means that we expect it to rain with 30% confidence due to some hypothesis.&nbsp;</p>
<p>In the Bayesian perspective, we are updating our beliefs. There are 3 aspects to it:</p>
<p>
</p>
<ul>
    <li>Prior belief (what we already know from previous observations)<br></li>
    <li>New evidence (the data we are observing at the moment)</li>
    <li>Updating our belief to compute the posterior</li>
</ul>
<p>Bayesian inference requires that we accept priors over events, that is, that we must explicitly quantify our assumptions with probability distributions. It is an extension of logic to uncertain information.&nbsp;</p>
<p><br></p>
<p></p>
<h2>Frequentist Probability</h2>
<p>On the other hand, under frequentist probability, we only consider the long term behaviour of repeated events, e.g. the probability of a coin landing on heads is 0.5 because over the long term, this will be the average proportion of times this occurs. It does not make sense in the world view to talk about the probability of events that can only happen once.&nbsp;</p>
<p>Bayesian Probability theory is sometimes said to be subjective because it requires the specification of prior belief, whereas frequentist models of probability do not admit the concept of priors and thus is objective.&nbsp;</p>
<p>Alternatively, the Bayesian model explicitly encodes uncertain knowledge and states universal formal rules for manipulating that knowledge, as formal logic does for definite knowledge. Frequentist methods are objective in the sense&nbsp; that they make statements about universal truths (i.e. the asymptotic behaviour), but they do not form a calculus of belief, and thus can't answer many questions of importance directly.</p>
<p><br></p>
<h2>Comparison</h2>
<p>In summary, Bayesian:</p>
<p>
</p>
<ul>
    <li>Includes priors</li>
    <li>Probability is a degree of belief</li>
    <li>Parameters of population are considered to be random variables, data to be known</li>
</ul>
<p>On the other hand, Frequentist:</p>
<p>
</p>
<ul>
    <li>No priors</li>
    <li>Probability is the long-term frequency of events</li>
    <li>Parameters of population assumed to be fixed, data to be random.&nbsp;</li>
</ul>
<p><br></p>
<h1>Forward and Inverse Probability</h1>
<p><br></p>
<p>Generative Process is the idea that there is some unknown process going on, the result of which can be observed. The process is governed by some variables that we do not know, but can be inferred. For example, consider the <strong>urn problem</strong>. say some mysterious entity has poured some balls (black and white) into an urn. You pull out four random balls from the urn and observe their colour, you get four white balls.&nbsp; We can ask the following questions about now:</p>
<p>
</p>
<ul>
    <li>What is the probability that the next ball that is drawn will be white?<ul>
            <li>this is a <strong>forward probability </strong>question. It asks questions related to the distribution of the observations</li>
        </ul>
    </li>

    <li>What is the distribution of white and black balls in the urn?<ul>
            <li>this is an <strong>inverse probability</strong>&nbsp;question. It asks questions related to unobserved variables that govern the process that generated the observations</li>
        </ul>
    </li>

    <li>Who is the mysterious entity?<ul>
            <li>This is an unknowable question. The observations we can make cannot resolve this.</li>
        </ul>
    </li>

</ul>
<p>There are a huge number of processes that can be framed as urn problems, urns where balls are replaced; problems where there are multiple urns and you don't know which urn the balls came from; problems where balls can move between urns; and so on...&nbsp;</p>
<p><br></p>
<h1>Axioms of Probability</h1>
<p><br></p>
<p>The following are the axioms of probability:<br></p>
<p></p>
<ul>
    <li><strong>The value</strong>&nbsp;- \( 0 \leq P(A) \leq 1 \), where \(A\) is an event : axiom of boundedness</li>
    <li><strong>The sum</strong>&nbsp;- \( \displaystyle\sum_A P(A)=1 \), where \(A\) is an outcome (single event : axiom of unitarity&nbsp;</li>
    <li><span><strong>The sum </strong>-<strong>&nbsp;</strong>\( P(A \vee B) = P(A) + P(B) - P(A \wedge B) \), where \(A\) and \(B\) are events : the sum rule&nbsp;</span></li>
    <li><span><strong>The conditional probability</strong>&nbsp;- \( P(A|B) = \frac{P(A\wedge B)}{P(B)} \), where \(A\) and \(B\) are events. \(P(A|B)\) refers to the probability of \(A\) happening given that \(B\) has happened. This is the conditional probability&nbsp;</span></li>
</ul>
<p><br></p>
<p></p>
<h2>Random Variables and Distributions</h2>
<p>A random variable can take some value, but we do not know what this value is. However, we can have some knowledge which captures the possible states which the variable could take on, and their corresponding probabilities. A random variable can be</p>
<p></p>
<ul>
    <li>The outcome of dice throw (discrete)</li>
    <li>Whether or not it is raining (discrete: binary)</li>
    <li>The height of a person we haven't met yet (continuous)&nbsp;</li>
</ul>
<p>A probability distribution defines how likely different states of random variables are. We can see \(X\) as the experiment and \(x\) as the outcome, with a function mapping every possible outcome to a probability. We denote \(P(X=x)\) for the probability that the random variable \(X\) takes the value \(x\). The outcome of a random variable can be thought of as taking some value. We write \(P(A)\) for the probability of the event \(A\), not a random variable \(A\).&nbsp;</p>
<p><br></p>
<p></p>
<h2>Discrete and Continuous Distributions</h2>
<p>There are 2 types of random variables:</p>
<p></p>
<ul>
    <li><strong>Discrete variables: </strong>The distributions of discrete random variables is given by a probability mass function (PMF). It gives each outcome of a specific value, e.g. a dictionary mapping outcomes to probabilities. It is denoted by&nbsp;\( f_\text{x}(x) \), where&nbsp;\( P(X = x) = f_\text{x}(x) \)</li>
    <li><strong>Continuous variables</strong>: The distribution of continuous random variables is given by a probability density function (PDF). It gives the spread of the probability over outcomes as a continuous function ( f_\text{x}(x) \). It is not the case that&nbsp;\( P(X = x) = f_\text{x}(x) \) for PDFs</li>
</ul>
<p>A PMF or PDF must sum/integrate to 1, the random variable must take some value. This is the unitary axiom. Every repetition of an experiment has exactly one outcome. So, we have;</p>
<p style="text-align:center;">\( \displaystyle\sum_i f_\text{x}(x)=1 \) for PMFs<br></p>
<p style="text-align:center;">\( \displaystyle\int_{x}f_\text{x}(x)~dx=1 \) for PDFs<br></p>
<p style="text-align:left;">For example, consider the probability of getting a value between 2 and 12 after two dice rolls. There is only one way to get values 2 and 12, so their probability is \(\frac{1}{36}\), but there are 6 ways to get a 7 (1+6, 2+5,3+4,4+3,5+2,6+1), so its probability is \(\frac{1}{6}\). The PMF contains the probability of each of the values between 2 and 12. The image below shows the probabilities&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%2866%29.png" alt="" width="800" height="442" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;"><br></p>
<h1>Expectation</h1>
<p><br></p>
<p>If a random variable takes on numerical values, then we define the expected value/expectation of a random variable by&nbsp;</p>
<p style="text-align:center;">\( E[X] = \displaystyle\int_x xf_\text{x}(x)~dx \)<br></p>
<p>For a discrete random variable, we have</p>
<p style="text-align:center;">\( E[X] = \displaystyle\sum_x xf_\text{x}(x) \)<br></p>
<p>In particular, if there are only finite possibilities, then&nbsp;</p>
<p style="text-align:center;">e\( E[X] = P(X=x_1)x_1 + P(X=x_2)x_2 + ... + P(X=x_n)x_n \)</p>
<p style="text-align:left;">The expectation is the average value of the random variable, the outcome we expect to happen. For example, the expected value of the sum of two dice is 7.</p>
<p style="text-align:left;">Expectations correspond to the mean, it is the true average of the value of all the outcomes that would be observed if we ran the experiment an infinite number of times, this is the population mean. So, the mean of a random variable is \(E[X]\), it is a measure of the central tendency. The variation of a random variable is \(E[X - [X]^2]\), it is a measure of spread.</p>
<p style="text-align:left;">We can apply a function to random variables, e.g. square of a random variable. Moreover, if \(g(X)\) is a continuous random variable \(X\), then&nbsp;</p>
<p style="text-align:center;">\( E[g(X)] = \displaystyle\int_x f_\text{x}(x)g(x)~dx \), or&nbsp;<br></p>
<p style="text-align:center;">\( E[g(X)] = \displaystyle\sum_x f_\text{x}(x)g(x) \)<br></p>
<p style="text-align:left;">for a discrete random variable. For example, the expected value of the square of the sum of two dice is 54.83. Note that&nbsp;\( E[g(X)] \neq g(E[X]) \). The expected value of the square of the sum of two dice is not \(7^2 = 49\), but instead 54.84. So, we have to apply the function to the random variable before multiplying by the probability mass function.&nbsp;</p>
<p style="text-align:left;">Expected values are essential in making rational decisions, the central problem of decision theory. They combine scores (or utility) with uncertainty (probability).</p>
<p style="text-align:left;"><br></p>
<p></p>
<h1>Samples and Sampling</h1>
<p><br></p>
<p>Samples (or observations) are observed outcomes form an experiment. We can sample from a distribution. This involves simulating outcomes according to the probability distribution. For example, we can sample from the sum of dice PMF by rolling two dice and summing the result. This is a sample or a draw from the distribution. For discrete random variable, this is easy, we simply produce samples by drawing each outcome according to its probability.</p>
<p>The PMF we estimate by using the observations for discrete random variables is called the empirical distribution. The empirical distribution is given by</p>
<p style="text-align:center;">\( P(X=x)=\frac{n_x}{N} \),<br></p>
<p style="text-align:left;">where \(n_x\) is the number of times that value \(x\) was observed, and \(N\) is the total number of trials. As the number of trials increases, the empirical distribution gets closer to the true PMF, assuming that we are sampling in a non-biased manner. For example, using 100 samples, the empirical PMF for two dice rolls is:</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%2867%29.png" alt="" width="800" height="405" class="img-fluid atto_image_button_text-bottom"><br></p>
<h2>Uniform Sampling</h2>
<p>There are algorithms that can return (pseudo-)random numbers uniformly distributed in some interval, e.g. between 0 and 1. A uniformly distributed number has equal probability of taking any value in the interval, and zero probability everywhere else. Although this is sampling from a continuous PDF, it is the key building block in sampling from arbitrary PMFs. It is denoted by\( X \sim U(a,b) \), where \(X\) is a random variable that can take values between \(a\) and \(b\), with equal possibility for any number in that interval. The symbol \(\sim\) means distributed as. In full, this means "\(X\) is distributed as a uniform distribution in the interval \([a,b]\)</p>
<p><br></p>
<h2>Discrete Sampling</h2>
<p>For a discrete PMF, we can sample outcomes by partitioning the unit interval. To do this, we first choose an arbitrary ordering of the outcomes \(x_1,x_2,...\) (the data is normally ordered if it is an array, for example). We assign each outcome to a "bin", which is a portion of the interval [0,1] so that the interval is divided into consecutive, non-overlapping intervals. We draw a uniform sample from [0,1], then, whichever "outcome bin" it lands in is the sample to draw (this is done using <code>np.digitize(samples,cmf)</code>). Since the cumulative sum of the arbitrary order will have last entry 1 (the total sum of all the outcomes is 1), we can sample using the interval [0,1].</p>
<p>For example, assume that our PMF has 4 word choices:&nbsp;<br></p>
<ul>
    <li>Cat, with probability 0.28</li>
    <li>Dog, with probability 0.5</li>
    <li>Sheep, with probability 0.2</li>
    <li>Dragon, with probability 0.02</li>
</ul><span style="font-size:0.9375rem;">Discrete sampling gives us the following<br>
</span>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%2868%29.png" alt="" width="600" height="957" class="img-fluid atto_image_button_text-bottom" style="font-size:0.9375rem;"></p>
<p style="text-align:left;"><span style="font-size:0.9375rem;">Clearly, as the number of samples increases, the empirical PMF gets closer to the true PMF. We will look at another example, consider the likelihood of characters in different text, Romeo and Juliet, and Metamorphosis</span></p>
<p style="text-align:left;"><span style="font-size:0.9375rem;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%2869%29.png" alt="" width="600" height="970" class="img-fluid atto_image_button_text-bottom"></span></p>
<p style="text-align:left;"><span style="font-size:0.9375rem;">The two text were written by two different authors, and so have somewhat different empirical PMFs. Nonetheless, they are quite similar since both texts are in English&nbsp;&nbsp;</span></p>
<p style="text-align:left;"><br></p>

<h1>Joint, Conditional, and Marginal</h1>
<p><br></p>
<p>The joint probability of two probabilities is written \(P(X,Y)\). It denotes the probability that both \(X\) and \(Y\) take the specific value at the same time, i.e. \(P(X=x) \wedge P(Y=y)\). The marginal probability is the derivation of \(P(X)\) from \(P(X,Y)\) found by integrating(summing) over all possible outcomes of \(Y\)</p>
<p>\( P(X) = \displaystyle\int_y P(X,Y)dy \) for a PDF&nbsp;<br></p>
<p>\( P(X) = \displaystyle\sum_y P(X,Y) \)&nbsp; for a PMF<br></p>
<p>This allows us to compute a distribution over one random variable from a joint distribution by summing over all the possible outcomes of the other variable involved.&nbsp;</p>
<p><strong>Marginalisation </strong>just means integrating over one or more variables from a joint distribution: it removes those variables from the distribution</p>
<p>Two random variables are <strong>independent</strong> if they do not have any dependence on each other. If this is the case then the joint distribution is just the product of the individual distributions: \(P(X,Y) = P(X)P(Y)\). This is not true in the general case where the variables have dependence</p>
<p>The <strong>Conditional Probability </strong>of a random variable \(X\) given a random variable \(Y\) is written as \(P(X|Y)\), and can be computed as</p>
<p style="text-align:center;">\( P(X|Y) = \frac{P(X,Y)}{P(Y)} \)<br></p>
<p style="text-align:left;">This tells us how likely the outcomes of \(X\) are if we already know the outcomes of \(Y\). This can be read as "probability of \(X\) taking on the value \(x\) given that \(Y\) has already taken on the value \(y\)". The conditional probability is \(P(X|Y) = P(X)\) and \(P(Y|X) = P(Y)\) if \(X\) and \(Y\) are independent.&nbsp;</p>
<p style="text-align:left;"><br></p>
<h2>Bigrams</h2>
<p>We can look at these in the case of the case of the character model of text we saw earlier. From the vector of character codes that make up "Metamorphosis" we can take every <em>pair of characters</em>, in the order that they appear. That is, we consider two characters \(c_{i-1}\) and \(c_i\) at some index \(i\). This is called a <strong>bigram</strong>&nbsp;model, and there are unigrams, trigrams, and n-gram generalisations of the idea. A&nbsp; "gram" refers to a unit like a character or word, and we are discussing a character <strong>bigram model.&nbsp;</strong></p>
<p></p>
<ul>
    <li>The joint distribution of bigrams \(P(C_i=c_i, C_{i-1}=c_{i-1})\) is given by the normalised count of each character pair</li>
    <li>The marginal distribution \(P(C_i = c_i)\) can be computed from \(P(C_i=c_i, C_{i-1} = c_{i-1})\) by summing over every possible character \(c_{i-1}\), and likewise to marginalise to find \(P(C_{i-1}=c_{i-1})\)</li>
    <li>The conditional distribution \( P(C_i=c_i|C_{i-1} = c_{i-1}) \) is given by the joint distribution, divided by the counts of \(P(C_{i-1}=c_{i-1})\). It tells us how likely we are to observe a specific character \(c_i\) given that we have observed a character \(c_{i-1}\) just beforehand.</li>
</ul>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%2870%29.png" alt="" width="800" height="679" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">This tells us how likely each possible <strong>pair </strong>of characters is. Some combinations are much more likely than others.</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%2871%29.png" alt="" width="1000" height="486" class="img-fluid atto_image_button_text-bottom"><br></p>
<p></p>
<p><strong><br></strong></p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%2872%29.png" alt="" width="800" height="693" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">This tells us what character to expect, given the character we know we saw before. It lets us predict the next character. We can look along the rows to do this look up. Each row sums to 1. For example. looking at row 'q', the probability is concentrated on the entry in the column 'u'</p>
<p style="text-align:left;"><br></p>
<h1>Writing and Manipulation of Probabilities</h1>
<p><br></p>
<p>Probabilities can be used to represent belief, but the raw numbers (e.g. \(P(X=x)=0.9999\)) are not always useful to make judgements, communicate results, or do computations.&nbsp;</p>
<p><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%2873%29.png" alt="" width="700" height="689" class="img-fluid atto_image_button_text-bottom"><br></p>
<p><br></p>
<h2>Odds &amp; Log Odds</h2>
<p>The <strong>odds</strong> of an event with probability \(p\) is defined by&nbsp;</p>
<p style="text-align:center;">\( \text{odds} = \frac{p}{1-p} \)</p>
<p>The odds are a more useful unit for discussing unlikely scenarios,&nbsp; saying something has odds of 999:1 is easier to understand than \(p = 0.001\).</p>
<p><strong>Log-odds</strong>, or <strong>logit</strong><strong>s</strong> are particularly useful for very unlikely scenarios</p>
<p style="text-align:center;">\( \text{logit}(p)=\log(\frac{p}{1-p}) \)<br></p>
<p>The logit scales proportionally to the number of zeroes in the numerator of the odds.&nbsp;</p>
<p><br></p>
<h2>Log Probabilities</h2>
<p>The probability of multiple independent random variables taking on a set of values can be computed from the product \(P(X,Y,Z) = P(X)P(Y)P(Z)\), and in general&nbsp;</p>
<p style="text-align:center;">\( P(X_1 = x_i, ... X_n = x_n) = \displaystyle\prod_{i-1}^n P(X_i=x_i) \)<br></p>
<p style="text-align:left;">We often have to compute such products, but to multiply lots of values \(&lt;1\) leads to numerical issues: we will get floating point underflow. Instead, it is numerically more reliable to manipulate log probabilities, which can be summed instead of multiplied&nbsp;</p>
<p style="text-align:center;">\( \log P(x_1,...,x_n)=\displaystyle\sum\log P(x_i) \)<br></p>
<p style="text-align:left;">This uses the identity&nbsp;\( \log(AB)=\log(A)+\log(B) \). This is simply a numerical convenience which avoids underflow. The <strong>log-likelihood </strong>is just \(\log P(B|A)\), and is more often convenient to work with than the raw likelihood.</p>
<p style="text-align:left;">When talking about <strong>likelihood</strong>, we often write&nbsp;\( \mathcal{L}(x_i) \) to mean the likelihood of \(x_i\). The likelihood is not a probability, it is a function of data, and \(\mathcal{L}(x_i) = f_\text{x}(x_i)\)&nbsp;</p>
<p style="text-align:left;">For example, consider the empirical PMF of Romeo and Juliet. This gives the probability of seeing any given character. The likelihood of seeing all the characters, given out per-character probability model, is:&nbsp;</p>
<p style="text-align:center;">\( \mathcal{L}(c_1,c_2,...,c_n)=\displaystyle\prod_i P(c_i) \)<br></p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%2876%29.png" alt="" width="200" height="44" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">The log-likelihood, on the other hand, does not have this underflow problem:</p>
<p style="text-align:left;"></p>
<p style="text-align:center;">\( \mathcal{L}(c_1,c_2,...,c_n)=\displaystyle\sum_i \log P(c_i) \)<br></p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%2875%29.png" alt="" width="200" height="51" class="img-fluid atto_image_button_text-bottom"><br></p><br>
<p></p>
<h2>Comparing log-likelihoods</h2>
<p>We could image that writing plays and novels is an activity that mysterious entities do by generating random characters according to a PMF. Under this very simplified assumption, we could take an "unknown" text (in this case Macbeth) and look at how likely it would have been to have been generated under two models.&nbsp;</p>
<p>
</p>
<ul>
    <li>A : It was generated using the PMF for Romeo and Juliet</li>
    <li>B : It was generated using the PMF for Metamorphosis&nbsp;</li>
</ul>
<p>Neither of these will be exactly true, but we can precisely quantify to what extent Macbeth appears to have been generated by a similar process to these two reference texts.</p>
<p>&nbsp;This is a very rough proxy for whether or not they were generated by the same entity (i.e. author). Our model is just the distribution of characters, so is a fairly weak model of different styles.</p>
<p><br></p>
<h1>Bayes Rule</h1>
<p><br></p>
<h2>Prior, Likelihood, and Posterior</h2>
<p>We might want to compute the value \(P(A|B)\) (probability of an event \(A\) given some other event \(B\)), but we might only be able to compute \(P(B|A)\). In general,&nbsp;\( P(A|B) \neq P(B|A) \).&nbsp;</p>
<p>This type of problem typically occurs where we w<span style="font-size:0.9375rem;">ant to know the probability of an event given some evidence (e.g. how likely is it I have a disease given that my blood test was positive), but we only know the probability of observing evidence given the event. (If you have this disease, the blood test will be positive 95% of the time)</span></p>
<p><span style="font-size:0.9375rem;"><strong>Bayes' rule</strong>&nbsp;gives the correct way to invert the probability distribution:</span></p>
<p style="text-align:center;"><span style="font-size:0.9375rem;">\( P(A|B) = \displaystyle\frac{P(B|A)P(A)}{P(B)} \)<br></span></p>
<p style="text-align:left;"><span style="font-size:0.9375rem;">This follows directly from the axioms of probability. Bayes' rule is a very important rule, with some surprising consequences.&nbsp;</span></p>
<p style="text-align:left;"><span style="font-size:0.9375rem;"><br></span></p>
<h3>Nomenclature</h3>
<p></p>
<ul>
    <li>\(P(A|B)\) is called the<strong>&nbsp;posterior</strong>&nbsp;- what we want to know&nbsp;</li>
    <li>\(P(B|A)\) is called the <strong>likelihood</strong>&nbsp;- how likely the event \(<span>A\)</span> is to produce the event we see</li>
    <li>\(P(A)\) is the <strong>prior&nbsp;</strong>- how likely the event \(A\) is regardless of evidence</li>
    <li>\(P(B)\) is the <strong>evidence</strong>&nbsp;- how likely the evidence \(B\) is regardless of the event</li>
</ul>
<p>Bayes' rule gives a consistent rule to take some prior belief and combine it with observed data to estimate a new distribution which combines them.&nbsp;</p>
<p>This can also be phrased as some <strong>Hypothesis </strong>\(H\) we want to know, given some <strong>data&nbsp;</strong>\(d\) we observe, and we write Bayes rule as&nbsp;</p>
<p></p>
<p style="text-align:center;"><span>\( P(H|D) = \displaystyle\frac{P(D|H)P(H)}{P(D)} \)<br></span></p>\(H\) and \(D\) are random variables in this expression. This can be read as : The probability of the hypothesis given the data is equal to the probability of the data given the hypothesis times the probability of the hypothesis, divided by the probability of the data. In other words, if we want to work out how likely a hypothesis is to be true given observations, but we only know how likely we are to have seen those observations if that hypothesis was true, we can use Bayes' rule to solve this problem.<br><br>
<p></p>
<h2>Integrating over the Evidence</h2>
<p>We can say that the posterior probability is <em>proportional</em>&nbsp;to the product of the prior and the likelihood, but to evaluate its value, we need to compute the <strong>evidence</strong>&nbsp;\(P(D)\).&nbsp;</p>
<p>It is difficult to see what this represents at first, but one way to think of it is as the result of marginalising the \(P(D)\) from the joint distribution \(P(H,D)\); that is, integrating \(P(H,D)\) over every possible outcome of \(H\) for each possible \(D\). Because probabilities must add up to 1, we can write \(P(B)\) as:&nbsp;</p>
<p style="text-align:center;">\( P(D) = \displaystyle\sum_i(P(D|H_i)P(H_i) \)<br></p>
<p style="text-align:left;">For a set of discrete outcomes \(A_i\), or&nbsp;</p>
<p style="text-align:center;">\( P(D) = \displaystyle\int_A P(D|H)P(H)dA \)<br></p>
<p style="text-align:left;">For a continuous distribution of outcomes. This trick is essential in understanding Bayes Rule. In general, this can be difficult to compute. For simple binary cases where are only two possible outcomes (H can be 0 or 1), Bayes' rule can be written as:</p>
<p style="text-align:center;">\( P(H=1|D)= \displaystyle\frac{P(D|H=1)P(H=1)}{P(D|H=1)P(H=1)+P(D|H=0)D(H=0)} \)<br></p>
<p style="text-align:left;"><br></p>
<h2>Natural Frequency</h2>
<p>Assume that there is a plague (\(X\)) spreading around the world. A new test is developed which can detect \(X\) with 95% accuracy. We'll assume that 95% accuracy means:</p>
<p></p>
<ul>
    <li>a 5% false positive rule, i.e. 5% of the time people who don't have the disease test positive</li>
    <li>a 5% false negative rule, i.e. 5% of the time people who have the disease test negative&nbsp;</li>
</ul>
<p>One in a hundred thousand (1:100000) people are known to have \(X\). If someone takes the test and the result it positive, then we can compute the likelihood of having the plague using Bayes' rule.</p>
<p>\( \begin{align}
    P(\text{Plague|Test}) &amp;=\displaystyle\frac{P(\text{Test|Plague)}P(\text{Plague})}{P(\text{Test})} \\
    &amp;=\displaystyle\frac{P(\text{Test|Plague)}P(\text{Plague})}{P(\text{Test}|\neg\text{Plague})P(\neg\text{Plague})+P(\text{Test|Plague)}P(\text{Plague})}
    \end{align} \)<br></p>
<p>So, after testing positive, there is a \(1:5263\) chance of having the plague. Although the test is 95% accurate, the probability of having the plague is very low. This is because so few people have actually gotten the plague.</p>
<p></p>
<p>There is an approach to explaining problems like this which make it much less likely to make poor judgements. <strong>Natural frequency</strong>&nbsp;explanations involve imagining concrete populations of a fixed size (10000 people in a room, for example), and considering the proportions of the populations as counts (e.g. how many people in the room have the plague?)</p>
<p>We can use this to visualise the problem above and explain the apparent paradox. The graph below shows the case \(P(\text{plague} = 0.005)\) (1 in 200), again with a 5% accurate test.&nbsp;</p>
<p><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%2877%29.png" alt="" width="1100" height="885" class="img-fluid atto_image_button_text-bottom"><br></p>
<p><br></p>
<h2>Bayes' rule for combining evidence</h2>
<p>Bayes' rule is the correct way to combine prior belief and observation to update beliefs. We always transform from one probability distribution (prior) to a new belief (posterior) using some observed evidence. This can be used to "learn", where "learning" means updating a probability distribution based on observations. It has enormous applications anywhere. Uncertain information must be fused together, whether from multiple sources (e.g. sensor fusion) or over time (e.g. probabilistic filtering)&nbsp;</p>
<p><br></p>
<h1>Entropy</h1>
<p><br></p>
<p>A key property of a probability distribution is the <strong>entropy</strong>. Intuitively, this is a measure of the "surprise" an observer would have when observing draws from the distribution, or alternatively, the (log) measure of a number of distinct "states" a distribution could represent. A flat, uniform distribution is very "surprising" because the values are very hard to predict. A narrow, peaked distribution is unsurprising because the values are always very similar.</p>
<p>This is a precise quantification - it gives the information in a distribution. The units of information are normally bits; where 1 bit of information tells you the answer to exactly one yes or no question. The entropy tells you exactly how many bits are needed (at minimum) to communicate a value from a distribution to an observer who knows the distribution already. Alternatively, you can see the number of distinct states the distribution describes as \(p=2^{H(X)}\) - this value is called the <strong>perplexity</strong>, and it can be fractional.&nbsp;</p>
<p>The entropy of a (discrete) distribution of a random variable \(X\) can be computed as:<br>
</p>
<p style="text-align:center;"><span style="font-size:0.9375rem;">\( H(X) = \displaystyle\sum_x-P(X=x)\log_2(P(X=x)) \)</span></p>
<p style="text-align:left;"><span style="font-size:0.9375rem;">This is just the expected value of the log-probability of a random variable (the "average" log-probability")&nbsp;</span></p>
<p><br></p>
<h2>Tossing Coins</h2>
<p>Consider a coin toss: This is sampling from a discrete random variable that can take on two states, heads and tails.</p>
<p>If we call our two possible states 0(heads) and 1(tails), we can characterise this with a single parameter \(q\), where \(P(X=0)=q\) and \(P(X=1)=(1-q)\). This follows from the fact that as \(P(X=0) + P(X=1)\) <em>must</em> equal 1, the coin must land on one side or the other.&nbsp;<br></p>
<p>If the process is very biased and heads are much more likely than tails&nbsp;\( (q \lt\lt 0.5) \), an observer will be unsurprised most of the time because predicting heads will be a good bet. If the process is unbiased&nbsp;\( (q = 0.5) \), an observer will have no way to predict if a heads or tails is more likely. We can write the entropy of this distribution:</p>
<p style="text-align:center;">\( \begin{align}
    H(X) &amp;= P(X=0)\log_2P(X=0)+P(X=1)\log_2P(X=1)\\
    &amp;= -q\log_2q-(1-q)\log_2P(1-q)
    \end{align} \)<br></p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%2878%29.png" alt="" width="400" height="275" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">This aligns with our intuition - an unbiased coin toss is the most surprising distribution. Entropy is a critically important concept in <strong>information theory</strong>&nbsp;which relates probability to the problems of communication. We can measure how well Bayesian updating is going by measuring the drop in entropy of the posterior belief. As we add evidence, we should become less surprised about the possible outcomes.&nbsp;</p>
<p style="text-align:left;"><br></p>
<h2>Interpreting Entropy</h2>
<p style="text-align:left;">In our letter-based bigram example, we can compute the entropy of each conditional distribution \(P(c_i|c_{i-1})\). This will tell us how surprised we will be by the characters that might follow \(c_i\).&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%2879%29.png" alt="" width="350" height="252" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">We can see that observing a "q" means the next character isn't surprising at all, we know for sure that it will be a "u", and thus the entropy is 0. There is only one configuration for the next character (under this model) if we have just seen a "q".</p>
<p style="text-align:left;">Likewise, seeing a space gives us very little information, and the next character could be anything. The character after a space will surprise us, there are lots of configurations that might follow a space. That surprise is as much surprise as we would get by tossing 4 coins, or an entropy of about 4 bits</p>
<p style="text-align:left;"><br></p>
<h1>Continuous Random Variables</h1>
<p><br></p>
<h2>Problems with Continuous Random Variables</h2>
<p>Continuous random variables are given by probability density functions (PDFs). A PMF is a vector is a vector of values, while a PDF is a function that maps any input in the domain to a probability. This results in many complexities:&nbsp;</p>
<p></p>
<ul>
    <li>The probability of a specific value \(x\) is always \(P(X=x)=0\), but it is possible that the PDF has support everywhere in its domain. i.e. the value could be this value</li>
    <li>There is no direct way to sample using a PDF</li>
    <li>We cannot estimate the true PDF using empirical observations, this will be finite(i.e. a PMF)</li>
    <li>It is not possible to directly apply Bayes' rule on a PMF (every probability is 0)</li>
    <li>Simple discrete distributions don't have a concept of dimensions. But we can have continuous values in \(\mathbb{R}\), or in vector spaces \(\mathbb{R}^n\), representing the probability of a random variable taking on a vector value.&nbsp;&nbsp;</li>
</ul>
<p><br></p>
<h2>Probability Distribution Functions</h2>
<p>The PDF&nbsp;\( f_\text{x}(x) \) of a random variable \(X\) maps a value \(x\) to a single number, the density at that point. It is a function \(\mathbb{R}^n \rightarrow \mathbb{R}^+\) ( \mathbb{R}^+ \) is the set of positive reals), with&nbsp;</p>
<p style="text-align:center;">\( \displaystyle\int_xf\text{x}(x)dx=1 \)<br></p>
<p style="text-align:left;">While the probability of some outcome with respect to the PDF is at most 1, it is not the case the value \( f\text{x}(x) \) is always less than 1.</p>
<p style="text-align:left;">The probability of the random variable being one value is 0, It is integrating from the value to itself. Instead, we talk about the probability of the random variable being in some range:</p>
<p style="text-align:center;">\( P(X\in(a,b)) = \displaystyle\int_a^b f\text{x}(x)dx \)<br></p>
<p style="text-align:left;">This is illustrated below&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%2880%29.png" alt="" width="500" height="259" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">We have a PDF on the left, and on the right is the corresponding cumulative density function(CDF). The red area gives the probability a value is between 0.25 and 0.5. Note that at 0, the value of the PDF is 2&gt;1.&nbsp;</p>
<p style="text-align:left;">The support of a PDF is the domain it maps from where the value \(f\text{x}(x)\)is not zero, i.e.</p>
<p style="text-align:center;">\( \text{supp}(x) = \{x \text{ such that }f\text{x}(x) &gt; 0\} \)<br></p>
<p style="text-align:left;">It is possible to have some support just around some interval, and no support everywhere else. This is called compact support. The following PDF has compact support.&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%2881%29.png" alt="" width="400" height="273" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">There is equal probability for the value to lie between -1 and 1, this is a uniform distribution. Also, it is possible for a PDF to have support over an infinite domain, e.g. normal distribution. In particular, the following figure illustrates normal distribution around 0.&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%2882%29.png" alt="" width="400" height="269" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">Sampling using normal distribution could take any value, it is just more likely to pick a value close to the mean of the distribution, This is called infinite support. It is also possible for a function to have semi-infinite support, like in the figure below.</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%2883%29.png" alt="" width="400" height="277" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">For a PDF, we have a&nbsp; cumulative distribution function (CDF) given by&nbsp;</p>
<p style="text-align:center;">\( F_\text{X}(x) = \displaystyle\int_{-\infty}^x f\text{x}(x) = P(X \le x) \)<br></p>
<p style="text-align:left;">The value of the CDF \(F_\text{X}(x)\) is always in the range [1,0]. Moreover, it gives us the probability that the random variable will take a value less than or equal to the given value. We can use the CDF to find the probability of the random variable lying in some range.</p>
<p style="text-align:center;">\( P(X\in[3,4]) = F_\text{X}(4) - F_\text{X}(3) \)<br></p>
<p style="text-align:left;"><br></p>
<h2>Normal Distribution</h2>
<p>The normal (or Gaussian) distribution is a PDF. It assigns probabilities for every \(x \in \mathbb{R}\), it has infinite support. It has density proportional to&nbsp;\( e^{-x^2} \) (with some scaling factors so that the integral is 1). It is called the squared exponential function. We denote&nbsp;\( X \sim \mathcal{N}(\mu,\sigma^2)
    \) to mean that the random variable \(X\) is distributed by as normal distribution mean \(\mu\) and variance \(\sigma^2\).</p>
<p>In the normal distribution, the PDF has the highest density at the mean \(\mu\), the function decays as we go further from it in both directions. In fact, if the value \(x\) isn't within \(\sigma\) of the mean, then its density is almost 0. For example, the following figure shows the PDF corresponding to normal distribution with mean 0 and standard deviation 0.5, along with samples taken from it.&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%2884%29.png" alt="" width="400" height="208" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">Clearly, there is a very high probability around 0, and the value goes down sharply. Normal modelling has many mathematical properties, and is easy to work with mathematically (i.e. without relying on numerical computation). Moreover, numerical modelling is applicable most of the time by the central limit theorem.&nbsp;</p>
<p style="text-align:left;"><br></p>
<h2>Central Limit Theorem</h2>
<p>If we have a sum of random variables \( Y = X_1 + X_2 + ... , \) then for almost any combination of \(X_1,X-2,...,\), the PDF of \(Y\) will be approximately normal. In other words, any process that involves a mixture of many random components will tend to be Gaussian under a wide variety of conditions. For example, consider the empirical PMF for multiple uniform variables&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%2885%29.png" alt="" width="400" height="216" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">As we expect, the probabilities are uniformly distributed. If we instead have 32 variables, it looks much more like a normal distribution</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%2886%29.png" alt="" width="400" height="204" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">In fact, if we had infinitely many uniform variables, the distribution would be perfectly normal.&nbsp;</p>
<p style="text-align:left;"><br></p>
<h1>Multivariate Distributions</h1>
<p>Continuous distributions can be generalised from \(\mathbb{R}\) to \(\mathbb{R}^n\), using PDFs. Like in \(mathbb{R}\), we require&nbsp;</p>
<p style="text-align:center;">\( \displaystyle\int_{x\in\mathbb{R}^n}f_\text{x}(x)dx=1 \)<br></p>
<p>This is the same as&nbsp;</p>
<p style="text-align:center;">\( \displaystyle\int^{x_0=\infty}_{x_0=-\infty}\displaystyle\int^{x_1=\infty}_{x_1=-\infty} ... \displaystyle\int^{x_n=\infty}_{x_n=-\infty}f_\text{x}([x_0,x_1,...x_n])dx_0,dx_1,...,dx_n=1 \)<br></p>
<p style="text-align:left;">We can generalise multivariable unform from uniform distributions in \(\mathbb{R}\). For example, the uniform distribution in the box \([0,1] \times [0,1] \text{ in } \mathbb{R}^2\) is :</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%2887%29.png" alt="" class="img-fluid"><br></p>
<p style="text-align:left;">We can transform this to any area in \(\mathbb{R}^2), using a matrix transformation and a vector offset</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%2888%29.png" alt="" class="img-fluid"><br></p>
<p style="text-align:left;">We can also generalise normal distribution, using mean \(\mu\) and standard deviation \(\sigma\) in \(\mathbb{R}\) into \(\mathbb{R}^n\), using the mean vector \(\sigma\) and the covariance matrix \(\Sigma\). We can look at the PDF of a multivariate normal for different covariances and mean vector (centres and spreads). For example. normal samples around mean vector 0 is the following:<br></p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%2889%29.png" alt="" class="img-fluid" style="font-size:0.9375rem;"></p>
<p style="text-align:left;">The samples are almost circularly spread around the mean. Like in uniform distribution, we can transform this to any area in \(\mathbb{R}^2\). Below are some different normal distributions centred at the origin, but with different covariance matrices&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%2890%29.png" alt="" width="400" height="219" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">We can also talk about the joint probability density function (density over all dimensions) and the marginal probability density function (density over some sub-sections of dimensions). Given a joint probability, we can marginalise it by integrating over one of the axis, as shown below.&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%2891%29.png" alt="" width="500" height="238" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">Now, we can construct the conditional probability for the two random variables.&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%2892%29.png" alt="" width="500" height="233" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">When we found an ellipsoidal shape that covered the data, we were finding a covariance matrix and mean vector such that the corresponding normal distribution was shaped like the ellipsoid.</p>
<p style="text-align:left;"><br></p>
<h1>Monte Carlo</h1>
<p><br></p>
<p>The Monte Carlo method allows us to approximately answer probabilistic problems. It involves setting up a simulation and with stochastic (random) components. By running the simulation many times with different random behaviour, the population of possible behaviours could be approximated.&nbsp;</p>
<p>For example, if we want to compute the expectation of a function of a random variable, we need to compute&nbsp;</p>
<p style="text-align:center;">\( E[g(X)] = \displaystyle\int_xf_\text{x}(x)g(x)dx \)<br></p>
<p style="text-align:left;">Since it involves integration, the value might be difficult, if not impossible to compute. However, it might be easy to compute \(g(x)\) for some \(x\). So, we can approximate</p>
<p style="text-align:center;">\( E[g(X)] \approx \frac{1}{N}\displaystyle\sum^N_{i=1}g(x_i) \)<br></p>
<p style="text-align:left;">where \(x_i\) are random samples from \(P(X=x)\), defined by the PDF \(f_\text{x}(x)\). This gets better as \(N\) gets larger.&nbsp;</p>
<p style="text-align:left;"><br></p>
<h2>Throwing Darts</h2>
<p style="text-align:left;">For example, imagine trying to work out the expectation of a dart throw. A dart board has sections giving different scores. We might model the position of the dart throw as a normal distribution over the dart space. This models the human variability in throwing. The expected score of a throw requires evaluating the integral of the normal PDF multiplied by the score at each point, which isn't feasible to compute directly</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%2893%29.png" alt="" width="300" height="312" class="img-fluid atto_image_button_text-bottom"><br></p>
<p>But we can sample from a multivariate normal distribution easily. We sample from \(d\) independent standard normals, and transform with a linear transform (matrix), So instead of trying to solve a very hard integral, we can simulate lots of dart throws, which follow the pattern of the normal distribution, and take the average score that they get. If we simulate a lot of darts, the average will be close to the true value of the integral.&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%2894%29.png" alt="" width="300" height="313" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:center;"><br></p>
<h1>Inference</h1>
<p><br></p>
<p>Inference statistics is concerned with estimating the properties of an unobserved "global" population of values from a limited set of observed samples. This assumes that there is some underlying distribution from which samples are being drawn. This is a hidden process (think the "mysterious entity" from before) which we only partially observe through the samples we see.&nbsp;</p>
<p><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%2895%29.png" alt="" width="800" height="624" class="img-fluid atto_image_button_text-bottom"><br></p>
<p>Population is the unknown set of outcomes (which may be infinite). Sample is some subset of the population that has been observed. So, if we compute the mean from the sample, it will not equal the true mean of the population.&nbsp;</p>
<p>In our model, assume a mysterious entity that generates data, according to some definite but unknown rules. These rules are based on some distribution (a type of rule), and parameters (the specifics on the rule applied). We assume the model has some randomness or stochastic elements, as this simplifies the model&nbsp;</p>
<p><strong>Inference</strong>&nbsp;is the process of determining these rules by looking at the aftermath of the actions of the mysterious entity. These are the samples or observations that we have. From these we can work out what the mysterious entity must be doing, or at least approximate it as best we can. We usually assume we know or have chosen a specific distribution which we expect to be governing the process, and focus on identifying the parameters involved. There are three approaches to inference&nbsp;</p>
<p></p>
<ul>
    <li><strong>Direct Estimation of Parameters</strong>&nbsp;: \( \hat\mu \Rightarrow \mu \) In this approach, we estimate the values of the parameter directly. For this, we need to know the distribution produced by the generative entity. It is quite efficient, but only works in some cases. In particular, we need an estimator function for each specific distribution we want to estimate. An estimator maps observations into parameter estimates<br><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%2896%29.png" alt="" class="img-fluid"><br></li>
    <li><strong>Maximum Likelihood Estimation</strong>: In this approach, we use optimisation to find the sets of parameters that match the observations the closest. This uses an iterative optimisation process, but it only works for models where the distribution has a known likelihood function. That is, we need to be able to compute how likely observations were to have been generated by that model<br><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%2897%29.png" alt="" width="200" height="119" class="img-fluid atto_image_button_text-bottom"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%2898%29.png" alt="" width="200" height="79" class="img-fluid atto_image_button_text-bottom"><br></li>
    <li><strong>Bayesian Inference</strong>&nbsp;:&nbsp;\( P(A|B) \propto P(B|A)P(A) \) In this approach, we explicitly encode belief about the behaviour of the mysterious entity using probability distributions. In Bayesian models, we assume a distribution over the parameters themselves, and consider the parameters to be random variables. We have some hypotheses (the prior), and we use observations to update these beliefs to make a stronger assumption about the parameters. We are not estimating a single parameter setting like in the cases above, but we always have a distribution over the possible parameters. Although this is a more robust and a more coherent way to do inference, it&nbsp; is much harder to represent and compute. We require both priors over parameters, and a likelihood function that tells us how likely data is to have been generated under a particular parameter setting.&nbsp;<br><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%2899%29.png" alt="" class="img-fluid"><br></li>
</ul>
<p>We illustrate the approaches by the linear regression problems, given a series of coordinates, find a best-fitting line through them. In particular, we are picking normally distributed points in the line \(2x+9\) in a normally distributed way with standard deviation 3. The following graph shows the chosen points</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%28100%29.png" alt="" width="400" height="285" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">The true generating function is \(2x+9\). But, the equation of the points is&nbsp;\( y = 2x+9+\epsilon \), where \(\epsilon\) represents the noise. We will assume that&nbsp;\( \epsilon \sim \mathcal{N}(0,\sigma^2) \) for standard deviation \(\sigma\) (we set it to 3 originally, but we will use this when inferring the equation of the line). So,&nbsp;</p>
<p style="text-align:center;">\( y \sim \mathcal{N}(mx+c,\sigma^2) \)<br></p>
<p style="text-align:left;">Here, \(y\) is the random variable, \(x\) is known, and we want to infer the gradient \(m\), offset \(c\) of the line, and the standard deviation \(\sigma\). We can construct a model that generates normally distributed points given \(m\), \(c\), and \(\theta\), and using inference we can find possible values of \(m\), \(c\), and \(\theta\).&nbsp;</p>
<p style="text-align:left;">We cannot estimate empirical distribution directly from observations for PDFs. For many continuous distributions, statisticians have developed estimators; functions that can be applied to sets of observations to estimate the most likely parameters of a PDF defining a distribution that might have generated them. The form of the distribution must be decided in advance (for example, the assumption that the data has been generated by an approximately normal distribution). This is usually called the <strong>model</strong>. The specific parameters can then be calculated under the assumption of this model.&nbsp;</p>
<p style="text-align:left;"><br></p>
<h1>Direct Estimation</h1>
<p><br></p>
<p>One way of doing inference is to use estimators of parameters, e.g. if it is normal, use mean and variance of this population distribution. These estimators are computed via statistics which are summarising functions we can apply to data. These estimators need to specially be derived for each specific kind of problem.&nbsp;</p>
<p>For example, the arithmetic mean and the standard deviation of a set of observed samples are statistics which are estimators of the parameters of \(\mu\) and \(\sigma\) of normal distribution. If we have observations drawn from a normal distribution, we can estimate \(\mu\) and \(\sigma\) of that distribution just by comparing the mean and&nbsp; standard deviation respectively.&nbsp;</p>
<p>In other words, we might want to know the 'true' average brightness, the population mean of the distribution which is 'generating' brightness. We have an assumption that a random process is creating these ratings, whose operational characteristics (parameters) we can learn from samples. But, we can only observe a limited sample of rating by measuring some specific subset of ratings from users who actually rated the app and computed the statistics of the result, the <strong>sample mean</strong>.&nbsp;</p>
<p><br></p>
<h2>Mean</h2>
<p>The arithmetic mean is a sum of sample values \(x_1, x_2, ..., x_n\) divided by the number of values</p>
<p style="text-align:center;">\( \hat\mu = \displaystyle\frac{1}{N}\displaystyle\sum^N_{i=1}x_i \)<br></p>
<p style="text-align:left;">This is called the sample mean. The population mean \(\mu\) is the expected value \(E[X]\), where \(X\) is a random variable.&nbsp; The arithmetic mean \(\hat\mu\) is a good estimate for \(\mu\) in general. The population mean is not usually computable, and is different to the sample mean. However, as we have more samples, we expect it to get closer to the true mean. This is shown in the following image</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%28101%29.png" alt="" width="500" height="330" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">The sample mean is a statistic (a function of observations), which is an estimator of the population mean (which may be a parameter of the distribution). We can put specific bounds on this estimate, the standard error allows us to measure how close we expect the arithmetic mean of samples is to the population mean, but this interpolation is not straightforward. The mean scalar measures the central tendency of a collection of values. The mean vector generalises this to higher dimensions.&nbsp;</p>
<p><br></p>
<h2>Variance</h2>
<p>The variance of sample values \(x_1,x_2,...x_n\) is given by&nbsp;</p>
<p style="text-align:center;">\( \hat\sigma^2 = \displaystyle\frac{1}{N}\displaystyle\sum^N_{i=1}(x_i - \hat\mu)^2 \)<br></p>
<p style="text-align:left;">It estimates the variance \(E[(X-E[X])^2]\). This is called the sample variance. The sample standard deviation is the square root.&nbsp;</p>
<p style="text-align:center;">\( \displaystyle\sqrt{\hat\sigma^2 = \displaystyle\frac{1}{N}\displaystyle\sum^N_{i=1}(x_i - \hat\mu)^2 } \)<br></p>
<p style="text-align:left;">The variance and the standard deviation measure the spread of a collection of values. The covariance matrix \(\Sigma\) is the generalisation to higher dimensions. As the number of samples increases, the variance also gets closer to the true sample.&nbsp;</p>
<p style="text-align:left;">We can use the sample mean and the sample variance to estimate the true mean and the true variance. By the central limit theorem, we know that the data is quite close to normal distributions, so this works quite often. Even if that doesn't apply, the mean and variance variance are still useful in descriptive statistics&nbsp;</p>
<p style="text-align:left;"><br></p>
<h2>Linear Regression</h2>
<p>In direct estimation, we solve linear regression using pseudo-inverses. This gives us an estimate of $$m$$ and $$ c $$, and we can compute a value for $$\sigma$$ using the difference of the points $$mx+c$$ (i.e. the variance). This gives us the following values:</p>
<p></p>
<ul>
    <li>$$m$$ = 2.07 (actual $$m$$ = 2)</li>
    <li>$$c$$ = 8.21 (actual $$c$$=9)</li>
    <li>$$\sigma$$ = 3.08 (actual 3)</li>
</ul>
<p>We can use these values to create the line fit&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%28102%29.png" alt="" width="400" height="271" class="img-fluid atto_image_button_text-bottom"><br></p>
<p><br></p>
<h1>Maximum Likelihood estimation</h1>
<p><br></p>
<p>If we do not have a pre built estimator that we can use for direct estimation, we can use maximum likelihood estimation. We can typically compute the (log) likelihood of an observation being generated by a specific underlying random distribution. For a PDF, the likelihood of a value \(x\) is just the value of the PDF at \(x\).&nbsp;</p>
<p>The log-likelihood is the sum of the individual log-likelihood</p>
<p style="text-align:center;">\( \log \mathcal{L}(x_1,...,x_n) = \displaystyle\sum_i\log f_\text{x}(x_i) \)<br></p>
<p>We can apply optimisation to work out a parameter setting under which the data we actually observed was mot likely, using the likelihood function .</p>
<p>If the likelihood depends on some parameters of a distribution $$\theta$$, then we write $$\mathcal{L}(\theta|x)$$. Then, we define an objective functions, maximising log likelihood, or minimising the negative of log likelihood. Therefore,&nbsp;</p>
<p style="text-align:center;">\( \theta^\ast = \underset{\theta}{\text{argmin}}L(\theta) \)<br></p>
<p style="text-align:left;">where,</p>
<p style="text-align:center;">\( L(\theta) = -\log\mathcal{L}(0|x_1,x_2,...x_n) = -\displaystyle\sum_i\log f_\text{x}(x_i,\theta) \)<br></p>
<p style="text-align:left;">We assume that $$f_\text{x}(x_i)$$ can be written as $$f(x,\theta)$$ to represent the PDF of $$f$$ with some specific choice of parameters given by $$\theta$$&nbsp;</p>
<p style="text-align:left;">The optimisation of negative log-likelihood is called maximum likelihood optimisation. It will find the best setting of parameters that would explain how the observations came to be. We can use any optimisation technique to optimise it, as long as we can evaluate the PDF $$f(x,\theta)$$ for any setting of parameters $$\theta$$.&nbsp;</p>
<p style="text-align:left;"><br></p>
<h2>MLE Linear Regression</h2>
<p>We can do linear regression using maximum likelihood estimation. For this, we need to write the problem in terms of the distribution of random variables. We can think of $$y=mx+c$$ as a model $$ Y \sim \mathcal{N}(mx+c,\sigma^2) $$. The random variable $$Y$$ has mean $$mx+c$$ and standard deviation $$\sigma$$. To avoid underflow, we will use log likelihood.</p>
<p>In the case where we have normally distributed noise for linear regression, this is exactly equivalent to the direct optimisation with linear least-squares. This is maximum likelihood linear regression. In terms of the objective function, our parameter is $$\theta = [m,c,\theta]$$&nbsp; and objective function computes the log-likelihood that the model $$\mathcal{L}(mx+c,\sigma^2)$$ produced the given set of results, i.e. the points that we originally had. In particular, for the example of $$2x+9+\epsilon$$ we had above, with $$\epsilon = \mathcal{N}(0,3)$$, MLE gives us the following predictions:</p>
<p></p>
<ul>
    <li>$$m$$ = 2.14 (actual $$m$$ = 2)</li>
    <li>$$c$$ = 7.19 (actual $$c$$ = 9)</li>
    <li>$$\sigma$$ = 3.14 (actual 3)</li>
</ul>
<p>This gives us the following line fit&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%28103%29.png" alt="" width="400" height="271" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:center;"><br></p>
<h1>Bayesian Inference</h1>
<p><br></p>
<p>In Bayesian inference, we represent the parameters of the distribution as random variables themselves, with distributions of their own. Prior distributions are defined over these parameters (e.g. we might believe that the mean app rating could be any value between 1 and 5 with equal probability), and we use evidence to refine our belief about the distribution of the parameters using Bayes' rule. We again consider our distribution to be characterised by some parameter vector $$\theta$$, and we want to refine a distribution over possible $$theta$$'s.</p>
<p>Instead of finding the most likely parameter setting, we infer a distribution over possible parameter settings compatible with the data. Given a likelihood $$P(D|\theta)$$ and prior $$P(\theta)$$, we can apply Bayes' rule.</p>
<p style="text-align:center;">\( P(\theta|D) = \displaystyle\frac{P(D|\theta)P(\theta)}{P(D)} \)<br></p>
<p style="text-align:left;">This gives us a new distribution over $$\theta$$ and given some observations. Pictorially, we expect the prior and the likelihood to affect the posterior as follows&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%28104%29.png" alt="" width="400" height="158" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">Bayes' rule applies to continuous distributions like it did to discrete distributions. However, algebraic computations (i.e. finding results in a closed form) are much harder. It is possible in certain cases, but the algebra is often complex and the model choices are limited. When it is possible, it is much more computationally efficient. We will approximate this by drawing samples from the posterior distribution $$P(\theta|D)$$</p>
<p style="text-align:left;"><br></p>
<h2>Bayesian Linear Regression</h2>
<p>MLE gives us a possible $$m$$, $$c$$, and $$\sigma$$, but now how confident we are with it. The Bayesian approach is to let the parameters themselves be random variables. We don't want to, we don't want to find the most likely parameters. We instead want to derive a belief about the parameters as a probability distribution. This is what Bayesians do, they represent belief with probability.&nbsp;</p>
<p>So, if $$\theta = [m,c,\sigma]$$, we can use Bayes' rule to infer the distribution over it. Here,&nbsp;</p>
<p>\( P(\theta|D) = \displaystyle\frac{P(D|\theta)P(\theta)}{P(D)} \)<br></p>
<p>Where $$D$$ is the data (an array of coordinates). So, we need:</p>
<p>
</p>
<ul>
    <li>A prior over the parameters $$P(\theta)$$. In the linear regression case, we need some initial belief about the possible values of $$m$$,$$c$$, and $$\sigma$$</li>
    <li>A way to calculate the likelihood $$P(D|\theta)$$</li>
    <li>A way of combining these using Bayes' rule. This is generally impossible, but we can sample from it using <strong>Markov Chain Monte Carlo.</strong>&nbsp;This will give us samples from the posterior distribution of $$P(\theta|D)$$ so we can see how sure we should be about our beliefs about the parameters of the mysterious entity.&nbsp;</li>
</ul>
<p>By sampling the parameters $$P(\theta)$$, we will get different samples for $$P(\theta|D)$$. This gives us a range of possible line fits that go through the coordinates.&nbsp;</p>
<p><br></p>
<h2>Probabilistic Programming</h2>
<p>We will use a computational representation where random variables are first-class values and we can write down our models without writing down the detailed mechanics of inference itself. This is probabilistic programming.&nbsp;</p>
<p>We can transform a mathematical expression to a graph. If we have multiple dependent random variables whose distribution we want to infer, we can draw a graph of dependencies between random variables (i.e. ones we don't know the value of precisely) and inference can be performed on the entire graph. So, we are writing expressions down without fixing the variables, and then allowing the distribution of the values to be inferred when we observe data. This inference narrows down the likely range a random variable could take on.&nbsp;</p>
<p>For example, $$y=mx+c$$ is modelled as follows:&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%28106%29.png" alt="" class="img-fluid"><br></p>
<p>Here, some nodes in the graph are observed, while others are unobserved. the coordinates $$(x,y)$$ for example, are observed, but the parameters $$m$$,$$c$$, and $$\sigma$$ are not observed. We can infer the posterior distribution of unobserved nodes by integrating over the possible values that could have occurred given the observed values.&nbsp;</p>
<p>We need to specify that $$y$$ is normally distributed with mean $$mx+c$$ and spread $$\sigma$$, and we can infer these values from the coordinates. So, the model with dependencies and observance is given below</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%28115%29.png" alt="" class="img-fluid"><br></p>
<p>A deterministic dependency means that finding the dependencies fully determines the value, e.g. knowing $$m$$ and $$x$$ fully determines the value $$mx$$. On the other hand hand, a stochastic dependency means that finding the dependencies still leaves some randomness to the value, e.g. knowing $$\mu$$ and $$\sigma$$ doesn't mean we know what $$y$$ is, we just know what range it is likely to lie in.&nbsp;</p>
<p>Our assumption here is that we will observe data which has a latent structure modelled by a linear dependence on a variable $$x$$, plus some normally-distributed observation noise. To apply Bayes' rule, we need to have some prior distribution on every stochastic node (i.e. $$m$$, $$c$$, and $$\sigma$$, but not $$mx$$). We can't have any "wild" stochastic nodes which do not eventually depend on deterministic nodes, via some chain of prior distributions.&nbsp;</p>
<p>We want to infer a distribution over $$m$$, $$c$$, $$\sigma$$m and not a distribution over the observations. That is, we will treat the parameters themselves as random variables, with their own distributions, and use Bayesian reasoning (i.e. applying Bayes' rule) to infer a posterior distribution over the parameters given some prior, and some evidence observed. We expect the posterior distribution to be much tighter than the prior distribution.&nbsp;</p>
<p>Given a prior and a likelihood and some observations, we can draw samples from the posterior $$P(\theta|D)$$. The issues here are:<br>
</p>
<ul>
    <li>We need $$P(D|\theta)$$ for a distribution over $$\theta$$, not just a few numbers. We need a closed form for the distribution function.&nbsp;</li>
    <li>The values $$ P(D) = \int_\theta)P(D|\theta)P(\theta)d\theta $$, which is often quite difficult (if not impossible) to compute.&nbsp;</li>
</ul>
<p>However, for a fixed $$\theta = [m,c,\sigma]$$, it is often trivial to compute $$P(\theta|D)$$. This is because we can easily compute $$P(D)$$ and $$P(D|\theta)$$ in that case. This can be thought of as drawing samples from the posterior distribution $$P(\theta|D)$$, instead of computing the distribution exactly.</p>
<p>We can also simplify this by only focusing on the relative probability of different parameter settings with data $$D$$. In that case,&nbsp;</p>
<p style="text-align:center;">\( P(\theta|D) \propto P(D|\theta)P(\theta) \)<br></p>
<p style="text-align:left;">This only makes sense because we are only considering one model with one set of data in this example.&nbsp;</p>
<p><br></p>
<h1>Markov Chain Monte Carlo</h1>
<p><br></p>
<p>We can implement a procedure to sample from the relative posterior distribution. This defines a random walk through the space of parameter settings, proposing small tweaks to the parameter settings, and accepting "jumps" if they make the estimate more likely, or with a probability proportional to the change in $$P(D|\theta)P(\theta)$$ if not. This means we can use samples from $$\theta$$ and not integrate. This is called <strong>Markov Chain Monte Carlo (MCMC)</strong>. To evaluate this, we need $$P(D|\theta)$$ (likelihood) and $$P(\theta)$$ (prior) for a given $$\theta$$.</p>
<p>MCMC allows us to run inference after writing down the model. However, the sampling strategy has a very large influence for the kind of sample runs that are practical to execute. True Bayesian inference depends only on the prior and the evidence, MCMC also depends on the sampling strategy used to approximate the posterior. The equation we use it&nbsp;</p>
<p style="text-align:center;">\( P(\theta|D) \propto P(D|\theta)P(\theta) \)<br></p>
<p style="text-align:left;">Where, $$P(\theta|D)$$ is the posterior, $$P(D|\theta)$$ is the likelihood, and $$P(\theta)$$ is the prior. We ignore the evidence as a normalising constant. In Python, we can use <code>PyMC3</code> to do inference.&nbsp;</p>
<p style="text-align:left;">In the linear regression model, the set of observations D is an array of coordinates. We represent the distribution parameters as $$\theta = [m,c,\theta]$$, and can talk about $$P(\theta|D)$$, a probability distribution over the parameter vectors. Our prior beliefs are:&nbsp;</p>
<p style="text-align:left;"></p>
<ul>
    <li>\( m \sim \mathcal{N}(0,50) \)<br></li>
    <li>\( c \sim \mathcal{N}(0,50) \)<br></li>
    <li>\( \sigma \sim U(0.01,100) \)<br></li>
</ul>
<p>These are very weak assumptions. Next we define the likelihood function. This is a function of the data given some parameter setting, and is the same as the MLE in this case. The likelihood of one sample is just the normal PDF evaluated at that point, and the likelihood of all samples is the product of these likelihoods</p>
<p style="text-align:center;">\( L(D|\theta) = L(x,y;m,c,\theta)=f_\text{x}(y-mx+c,\sigma^2) \)<br></p>
<p style="text-align:left;">We cab then sample form the model. We need to specify how many samples to take from the posterior predictive $$P(\theta|D)$$. This gives us a range of distributions for $$m$$, $$c$$, and $$\sigma$$, with 500 samples:</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%28107%29.png" alt="" width="249" height="171" class="img-fluid atto_image_button_text-bottom"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%28108%29.png" alt="" width="250" height="167" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%28109%29.png" alt="" width="250" height="176" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">We have quantified uncertainty, because our sample size was quite low, we cannot say with confidence that the value of $$m$$ is 1.9. This is an advantage of Bayesian inference. With more samples, our confidence on the distributions&nbsp; will increase. We can plot the 500 predictions on the regression lines as well.&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%28110%29.png" alt="" width="250" height="164" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">In summary, we are doing the following:</p>
<p style="text-align:left;">
</p>
<ul>
    <li>We infer parameters of the model in the form of samples (definite values for the parameters)</li>
    <li>We can then use those definite values to simulate behaviour using our linear model</li>
    <li>This gave us a collection of simulations</li>
    <li>We can plot all of these overlaid&nbsp;</li>
</ul>
<p><br></p>
<h2>Predictive Posterior</h2>
<p>We predicted the line fits with the posterior distributions of $$m$$, $$c$$, and $$\sigma$$. These are the values we expect the model parameters to take on, given the data we observed (likelihood) and our prior. We had also set prior distributions of $$m$$, $$c$$, and $$\sigma$$. We could plot these as well.</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%28111%29.png" alt="" width="250" height="155" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">These are much less accurate because our initial guess allowed for a lot more configurations of $$m$$, $$c$$, and $$\sigma$$.&nbsp;</p>
<p style="text-align:left;">The predictive posterior is the distribution over observations we would expect to see. We draw samples from the model, while integrating over parameters from the posterior. By sampling from the predictive posterior, we are generating new synthetic data that should never have the same statistical properties as the data (if our model is good).&nbsp;</p>
<p style="text-align:left;">In the linear regression example, the predictive posterior tells us the likely values of $$y$$ using the posterior distribution of $$m$$, $$c$$, and $$\theta$$. The following histogram shows the predictive posterior and the prior posterior&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%28112%29.png" alt="" width="250" height="181" class="img-fluid atto_image_button_text-bottom"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/965063187/image%20%28114%29.png" alt="" width="250" height="181" class="atto_image_button_text-bottom"><br></p>
<h1>Time Series and Signals</h1><br>
<p>Signals such as sound, light patterns, and temperatures are continuous in value and time/space. We can imagine them as real functions of real values: $$x_t = x(t)$$ for functions of time $$t$$, for example, or images where brightness is a function of space, and so on.</p>
<p>We have to sample them to make them amenable to digital signal processing, to turn a representation of a function into a numerical array. This involves quantising in both value and time/space. We must have&nbsp; a precise, fixed number of values for both $$t$$ and $$x(t)$$. Quantisation means to force something to a discrete set of values (e.g.
    integers in range [−128, 127]).</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/535666708/image%20%28116%29.png" alt="" width="500" height="275" class="img-fluid atto_image_button_text-bottom"><br></p>
<p>Let $$x_t = x(t)$$ represent a continuous (scalar) function with respect to time $$t$$. We sample it regularly to get a sequence of measurements $$x[t] = [x_1,x_2,...,x_n]$$. We do this by taking measurements at frequent, constant time intervals, the value $$x(t)$$ representing the value of the function at time $$t$$. This is time quantisation. The value $$x(t)$$ is further quantised so that it can be represented as fixed length in memory, for example to float64. This is amplitude quantisation.&nbsp;</p>
<p>Following time and amplitude quantisation, we can convert a signal into a 1D array of data. Since we expect the measurement to be taken with the same time interval, we do not need to store time explicitly, we only need to specify or know the starting time and the sampling rate.&nbsp;</p>
<p>We call the 1D array of samples (without time values) the sampled signal. There is no value of $$t$$ stored, only the value of $$x(t)$$, the time value is implied. Using the index of the value in the array, we can reconstruct the time value.</p>
<p>Along with the starting time, we need to store the sampling rate. This represents how frequently the data is being sampled, denoted by $$f_s$$, the unit of sampling is hertz. In terms of spatial functions, like images, the sampling rate is the number of measurements per some division of space (e.g. 72 pixels per inch). This value is also fixed when sampling, so we do not need to store the spatial coordinates in the sample.</p>
<p>We sample into signals to represent a continuous varying function in a compact, efficient manner. By not storing time, we are saving space. Also, since the data is represented as arrays, we can apply a vast collection of efficient/fast algorithms to them in order to analyse the data. Some meaningful operations are:&nbsp;</p>
<p>
</p>
<ul>
    <li>Removing an offset from a signal, which is equivalent to subtracting a value elementwise from a sample array</li>
    <li>Mixing two signals, which is equivalent to weighted elementwise sum of their sample arrays&nbsp;</li>
    <li>Correlation between signals, which can be computed using elementwise multiplication</li>
    <li>Selecting regions of signals, which can be done via slicing.</li>
    <li>Smoothing and regression, which can be applied to signal arrays</li>
</ul>
<p><br></p>
<h2>Noise</h2>
<p>Whenever we measure signals, we will always have some noise. For some time $$t$$, there is some true value $$y(t)$$ and some noise $$\epsilon(t)$$, the value we measure is $$x(t) = y(t) + \epsilon(t)$$. The signal to noise ratio is given by&nbsp;</p>
<p style="text-align:center;">\( SNR = \displaystyle\frac{S}{N} \)<br></p>
<p style="text-align:left;">where $$S$$ is the amplitude of $$y(t)$$ and $$N$$ is the amplitude of $$\epsilon(t)$$. A high SNR means that the signal is quite clean and not corrupted with noise, while a low $$SNR$$ means that the signal is heavily corrupted (and weak). We can represent this logarithmically,</p>
<p style="text-align:center;">\( SNR_{db}=10\log_{10}\displaystyle\frac{S}{N} \)<br></p>
<p style="text-align:left;">The units here is decibels. An increase of 10 decibels in the SNR makes the signal 10 times "louder" relative to the noise.&nbsp;</p>
<p style="text-align:left;">Ideally, we would want to remove all noise from the data. However, noise is random and difficult to control, it is therefore difficult to distinguish between noise and true data. However, if we make assumptions on the true value of the data $$y(t)$$, it is possible to remove some parts of the signal that couldn't be part of the true value. For example, the noise moving rapidly and we expect the true data to change at a much slower rate. We can remove/smoother the values to that the dataset moves less rapidly. This is filtering, removing some of the data $$x(t)$$ based on out assumptions on $$y(t)$$. If our assumptions are wrong, we might lose the wrong data and fail to remove the noise.&nbsp;</p>
<p><br></p>
<h2>Sampling and Noise</h2>
<p><br></p>
<p>Quantisation adds noise. The difference between the true value and the assigned value is random. There are different rates at which we could sample the data (i.e. time quantisation, converting continuous data to discrete samples, it makes the time $$t$$ discrete), and as the sampling rate increases, the noise decreases. Amplitude quantisation is how we realise this data into ndarrays, this makes the value $$f(t)$$ discrete. It reduces the range/precision of those values. There are typically a range of bits that the value is represented as.&nbsp;</p>
<p>Amplitude quantisation introduces noise, there is a difference between the value of the signal $$f(t)$$ and the quantised level. With coarser quantisation, although we have some more noise, we save storage space (and shorter time to perform computations on it). For example, we quantise a time signal below (from some audio) to 6 bits (i.e. $$2^6$$ = 64 levels)</p>
<p><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/535666708/image%20%28117%29.png" alt="" width="700" height="438" class="img-fluid atto_image_button_text-bottom"><br></p>
<p>&nbsp;We can also plot the residual difference&nbsp;</p>
<p><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/535666708/image%20%28119%29.png" alt="" width="600" height="381" class="img-fluid atto_image_button_text-bottom"><br></p>
<p>Clearly the noise has no structure and is oscillating randomly&nbsp;</p>
<p><br></p>
<h1>Sampling Theory</h1>
<p><br></p>
<p>If we sample a dataset enough, we can construct a continuous signal perfectly, as long as that signal does not have too much high frequency&nbsp; content. In particular, Nyquist's limit tells us that for a sampling rate $$f_s$$, we can reconstruct the original signal from a sampled signal if the signal contains frequencies at most.</p>
<p>\( f_n = \displaystyle\frac{f_s}{2} \)<br></p>
<p>The value $$f_n$$ is called the Nyquist limit. For example, since human hearing extends to about 20 KHz, audio is often recorded with a sample rate of 44.1 KHz.&nbsp;</p>
<p>If we do not sample enough (by not following this rule), the result is aliasing. In that case, when sampling a high-frequency $$ f_q \gt f_n $$ (where $$f_n$$ is the sampling rate), we will observe an artificial component of $$ f_n - (f_q \mod f_n) $$ in the sample (the value gets wrapped around). In videos, this creates a wagon wheel effect. Images must be filtered/smoothened to remove high-frequency components that cannot be sampled correctly before sampling (at a lower resolution), this is called anti-aliasing. When the resolution of an image is reduced, the sample spacing gets larger, so the signal must have less high-frequency content if it is to be represented accurately&nbsp;</p>
<p><br></p>
<h2>Regular Sampling</h2>
<p>We must sample at regular intervals. Below we have a moving average line for the height and the volume of cherry trees. This data does not come from regular sampling, and is unordered.</p>
<p><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/535666708/image%20%28120%29.png" alt="" width="650" height="448" class="img-fluid atto_image_button_text-bottom"><br></p>
<p>So, the line isn't of much use (if any). The line isn't starting from the first component since the data is unordered. Instead, if we look at the number of air passengers per year, the moving average behaves the way we want it to.&nbsp;</p>
<p><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/535666708/image%20%28121%29.png" alt="" width="600" height="407" class="img-fluid atto_image_button_text-bottom"><br></p>
<p>This is because the data was sampled at regular time intervals, and is ordered (with respect to time). So, we can only use the moving average if we have a regularly sampled signal. Nonetheless, there are 2 ways of drawing a line for cherry data. We can do regression (e.g. linear regression) to fit a line through the data (e.g. least squares) using optimisation or probability. We can also convert the data into regularly sampled form, and then use signal processing operations. This can be done using interpolation and resampling. This two-step process of interpolation and resampling is called gridding</p>
<p><br></p>
<h1>Gridding</h1>
<p><br></p>
<p>The first step of gridding is interpolation, we estimate a value between some known measurements. An interpolating function produces estimates for a value of a function in between the data points. Let the data points be $$[(t_0,x_0),...,(t_n,x_n)]$$. Interpolation will give us a function $$f$$ such that $$f(t_i) = x_i$$ for all the data points. Moreover, the interpolated function $$f$$ can be evaluated anywhere between $$x_0$$ and $$x_n$$. There are many ways to interpolate, such as:&nbsp;</p>
<p></p>
<ul>
    <li><strong>Constant/Nearest neighbour interpolation </strong>: Values are not changing between data points, i.e. we draw a straight line from the closest neighbour</li>
    <li><strong>Linear Interpolation </strong>: Values are changing between data points in a linear way (a straight line between data points)&nbsp;</li>
    <li><strong>Polynomial interpolation </strong>: We fit a polynomial (typically quadratic or cubic) between data points.&nbsp;</li>
</ul>
<p>Interpolations are mostly applied piecewise, we only care about the closest points before and after the point when interpolating. An example of interpolation is given below for the height and volume of cherry trees.&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/535666708/image%20%28122%29.png" alt="" width="600" height="420" class="img-fluid atto_image_button_text-bottom"><br></p>
<p>After interpolation, we resample. Using the interpolated graph, we evaluate $$t$$ at fixed time intervals to produce a time series. For the cherry tree example, with linear interpolation, we get the following moving average line:</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/535666708/image%20%28123%29.png" alt="" width="600" height="420" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">We can use resampling to align multiple time series that are being sampled at different rates, we can resample to a common sampling rate. This alignment process is essential when combining sensor data from multiple streams</p>
<p style="text-align:left;"><br></p>
<h1>Filtering and Smoothing</h1>
<p><br></p>
<p>To remove noise from signals, we need to filter data and smoothen it. Filtering takes advantage of the temporal structure of the data. We assume that the noise is random/independent. For example, we may have $$x[t] = y[t] + \mathcal{N}(0,\sigma)$$, where $$y[t]$$ is the true value and $$\mathcal{N}(0,\sigma)$$ is the noise. The values of $$\sigma$$ are independent with respect to time and the previous value $$y[t-1]$$</p>
<p>One way of getting rid of noise is by averaging over multiple time steps. It will also average the true signal. But, we expect the true signal to not change fast</p>
<p><br></p>
<h2>Moving Average</h2>
<p>We expect the true signal to change slowly, but the noise to change fast and randomly. So, we can use a moving average to smoothen the signal. Here, for a given point, we take a sliding window around this point (we consider some points before and some after), and assign the value of this point to be the mean of all the values in the window. We iteratively compute all the values. Therefore, if $$x[t]$$ is the observed data with noise, the new data $$y[t]$$ is given by</p>
<p style="text-align:center;">\( y[t] = \displaystyle\frac{1}{k}\sum^{K-1}_{i=0}x[t+i - K/2] \)</p>
<p style="text-align:left;">The sliding window takes a sampled signal of some (maybe unbounded) length, and reduces is to a collection of fixed length vectors. We break the signal into equally spaced chunks/windows, each of length $$K$$. We then process these as an $$N \times K$$ matrix, $$N$$ windows of $$K$$ samples. We can&nbsp; perform these any operation on these vectors/matrices (e.g. matrix operations, norms, quantisation). As we expect, the moving average depends on the length of the&nbsp; sliding window $$K$$. As $$K$$ gets bigger, the signal gets smoother.&nbsp;</p>
<p style="text-align:left;">Using moving averages on sound reduces the high frequency content. The longer the moving average, the smoother the waveform and the more high-frequencies are supressed. We can apply moving averages on images as well (spatially). This has the effect of blurring the image (box blur).</p>
<p><br></p>
<h1>Nonlinear Filtering</h1>
<p><br></p>
<p><span style="font-size:0.9375rem;">Moving average is a linear filter, it is the weighted average. Linearity means: $$&nbsp;</span><span style="font-size:0.9375rem;">f(x+y) = f(x) + f(y);~f(0) = 0;~ f(ax) = af(x) $$</span></p>
<p><br></p>
<h2>Median Filtering</h2>
<p>Any filtering not using the weighted sum of the sliding window is a non-linear filter. An example of this is median filtering. This is like the moving average, but we use the median for each sliding window to decide on the value. This is applicable when most measurements are good, but few are corrupted with noise, especially if there is no guarantee that the noise is small.</p>
<p>This is a special type of order filter, in an order filter, we sort all the values in the sliding window before choosing a value. Other order filters take the minimum or maximum, for example.&nbsp;</p>
<p>Median filtering adds more robustness to extreme values than moving averages. The following image illustrates how linear and median filtering helps remove noise from a corrupt signal.&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/535666708/image%20%28124%29.png" alt="" width="650" height="459" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/535666708/image%20%28125%29.png" alt="" width="650" height="443" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/535666708/image%20%28126%29.png" alt="" width="650" height="436" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">Clearly, median filtering is much better at removing extreme outliers than linear filtering. However, since we need to sort the array/use specialised mean cascade algorithms, these algorithms are slower</p>
<p style="text-align:left;"><br></p>
<h1>Convolution</h1>
<p><br></p>
<p>Most filters we apply are linear. These are very efficient and have been studied very well. A linear filter is a filter where the output is a weighted sum of the neighbouring values (and the original value). Although this is quite limited, there are many effects we can achieve using linear filtering. They are also quite efficient since the CPU (especially digital signal processing-specific CPUs) often have a multiply and accumulate operation. It can multiply by a constant and accumulate to a register. This makes linear filters very straightforward.</p>
<p>For example, a linear filter is:</p>
<p style="text-align:center;">\( f(x[t]) = 0.25x[t-1] + 0.5x[t] + 0.25[t+1] \)<br></p>
<p style="text-align:left;">This is a weighted sum of three samples. The total height is still 1, so the average amplitude of the signal does not change. This filter has the effect of spreading a spike, it is a smoothening filter. The following illustrates this.</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/535666708/image%20%28127%29.png" alt="" width="800" height="412" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">We can keep applying this filter to the result, which keeps spreading and smoothening the spike.</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/535666708/image%20%28128%29.png" alt="" width="800" height="407" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">The process of taking weighted sums of neighbouring value is called convolution.</p>
<p style="text-align:left;">For two functions $$f$$ and $$g$$, their convolutions is denoted by&nbsp;\( f * g \) Convolution is defined for continuous functions $$f(x)$$ and $$g(x)$$. For two sampled signal vectors $$x[n],y[m]$$ of length $$N$$ and $$M$$, the definition of convolution is&nbsp;</p>
<p style="text-align:center;">\( (x \ast y)[n] = \displaystyle\sum^M_{m=-M}x[n+m]y[m] \)<br></p>
<p style="text-align:left;">Note that we have $$x[n+m]$$ and not $$x[m]$$ since the sliding window starts at $$x[n-M]$$ and goes to $$x[n+M]$$. We can use convolution for effects like blurring and sharpening images, filtering audio, etc.&nbsp;</p>
<p style="text-align:left;"><br></p>
<h2>The Convolution Kernel</h2>
<p><br></p>
<p>in $$x * y$$, we can think of $$x$$ as the signal to be transformed, and $$y$$ as the operation to be performed. We call $$y$$ the convolution kernel. It is an array, just like $$x$$. However, $$x$$ is normally much longer than $$y$$. For example, a convolution kernel is the array $$[0.25,0.5,0.25]$$. It has the following effect on the signals given.&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/535666708/image%20%28129%29.png" alt="" width="600" height="352" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">We can view moving average as a convolution. For example, if $$N = 2$$, then $$y=[0.5,0.5]$$, and if $$N=5$$, then $$y=[0.2,0.2,0.2,0.2]$$. The weighted sum of these values will give us the moving average. Each element in an array has the same value (equal weight), and they sum to 1. (no scaling of the amplitude)</p>
<p style="text-align:left;"><br></p>
<h2>Algebraic Properties of Convolution</h2>
<p>The convolution operation is commutative, i.e. $$f * g = g * f$$, and associative, i.e. $$(f *g) *h = f *(g*h)$$. We can exploit the associativity property by first convolving the (much) shorter convolution kernels and then convolving with the times series, instead of convolving with time series twice, we expect the signal to be much longer than the convolution kernels, so it will be much more efficient. Moreover, since the convolution operation is linear, it is distributive, i.e. $$f&nbsp; * (g+h) = f*g + f*h$$.</p>
<p>We can illustrate these properties with an example. Consider the following convolutions, the edge detector convolution and the smoothing convolution. Their effects on the zero signal is given below:&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/535666708/image%20%28130%29.png" alt="" width="600" height="317" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/535666708/image%20%28131%29.png" alt="" width="600" height="333" class="img-fluid atto_image_button_text-bottom"><br></p>
<p>The convolving of these two kernels produces a smoothing edge detecting kernel. We will call it 'edgemooth'</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/535666708/image%20%28132%29.png" alt="" width="600" height="320" class="img-fluid atto_image_button_text-bottom"><br></p>
<p>Now, consider a noisy saw signal</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/535666708/image%20%28133%29.png" alt="" width="600" height="323" class="img-fluid atto_image_button_text-bottom"><br></p>
<p>If we apply the edge convolution to saw signal, and then the smoothing convolution, we get the following signal</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/535666708/image%20%28134%29.png" alt="" width="600" height="315" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">Moreover, if we apply the edgesmooth convolution to the saw signal we get this result</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/535666708/image%20%28135%29.png" alt="" width="600" height="310" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">Clearly, they are the same. This illustrates the associativity of convolution.</p>
<p style="text-align:left;"><br></p>
<h2>Dirac Delta Function</h2>
<p>The dirac delta function is given by&nbsp;</p>
<p style="text-align:center;">\( \delta (x) {\LARGE \{} {\small \begin{matrix} 0 &amp; x\neq0 \\ \infty &amp; x=0\end{matrix}} \)<br></p>
<p style="text-align:left;">This is not a proper function, but it satisfies</p>
<p style="text-align:center;">\( \displaystyle\int^\infty_{-\infty}\delta(x)dx=1 \)</p>
<p style="text-align:left;">The delta function does not change other convolutions, i.e. $$f * \delta = f$$. It is an identity element with some respect to convolution. Therefore, if we can simulate some system with the delta function, we can read back the convolution, it represents the linear model of the system.</p>
<p style="text-align:left;">The discrete version of the delta function is an array of zeroes with a single 1, the impulse. The figure below gives us the discrete dirac delta function&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/535666708/image%20%28136%29.png" alt="" width="600" height="312" class="img-fluid atto_image_button_text-bottom"></p>
<p style="text-align:left;">If we feed a perfect impulse to the system, we can recover the convolution kernel. This is a linear system identification.&nbsp;</p>
<p style="text-align:left;">We can also use the delta function to capture something that behaves like a linear filter. We can inject the delta function to something so that we can create a filter representing the environment. Since the dirac delta function is the identity convolution, we can add any signal to get the same sound back, but with this filter. In particular, we can add background to a sound (e.g. echo) using this property, this is called impulse response recovery, and is part of reverberation. Producing the dirac delta function (or any convolution) from the real world is difficult due to noise.</p>
<p style="text-align:left;"><br></p>
<h1>Frequency</h1>
<p><br></p>
<p style="text-align:left;">Until now, we have thought of a signal as a sequence of amplitude measurements over the time domain. We can also think of signals as a sum of&nbsp;oscillations over the frequency domain. These two representations are equivalent.<br></p>
<p style="text-align:left;">A pure oscillation is a sine wave<br></p>
<p style="text-align:center;">\( x(t) = A\sin(2\pi\omega t + \theta) \)<br></p>
<p style="text-align:left;">Where:</p>
<p style="text-align:left;"></p>
<ul>
    <li>$$\omega$$ is the frequency of the oscillation</li>
    <li>$$\theta$$&nbsp;is the phase of the oscillation (offset to time)</li>
    <li>$$A$$&nbsp;is the magnitude (length of the amplitude) of oscillation</li>
</ul>
<p>A frequency is the repetition period of a sinusoidal oscillation. Higher
    frequency indicates a shorter period. The frequency domain representation
    represents signals as sums of oscillations at all frequencies, where each frequency
    has a phase $$\theta$$ and magnitude $$A$$. A pure frequency is a sine wave with a specific
    period.<br></p>
<p><br></p>
<h2>Fourier transform</h2>
<p>The Fourier theorem tells us that any repeating function can be decomposed
    into sine waves. A single sine wave has a unique frequency $$\omega$$, amplitude $$A$$ and
    phase $$\theta$$, with\( x(t) = A\sin(2\pi\omega t + \theta) \). The Fourier transform allows us to write
    any signal as a sum of sinusoid functions, i.e<br></p>
<p style="text-align:center;">\( x(t) = \displaystyle\sum_i A_i \sin(2\pi\omega_i t + \theta_i) \)<br></p>
<p style="text-align:left;">To understand how the Fourier transform works, we can try to understand the correlation between signals $$a[t]$$ and $$b[t]$$. One way to do it is by computing the elementwise product of $$a[t]$$ and $$b[t]$$ for different values of $$t$$. When they are both the same sign, the product will be a large positive number. When they are opposite signs the product will be a large negative number. We can sum the products&nbsp;</p>
<p style="text-align:center;">\( c = \displaystyle\sum_t a[t]b[t] \)<br></p>
<p style="text-align:left;">The value $$c$$ can be used to interpret the correlation between two signals, or the inner product representation of signals. if $$ c \approx 0$$ , then the two signals are not correlated. If $$c$$ is large and positive, then the two signals are similar, i.e. $$a[t]=b[t]$$. Instead, if $$c$$ is large and negative, the two signals are close to being inverse, i.e. $$a[t] = -b[t]$$.</p>
<p style="text-align:left;">If we now generate every possible frequency of a sine wave as sampled signals $$a_1[t],a_2[t],...$$ and correlate each with a test signal $$b[t]$$, then for some $$a[t]$$, the result will be nearly zero, meaning the signals are almost unrelated, while for others it will be larger, since $$a[t]$$ has some parts that are oscillating at the test frequency of $$b[t]$$. This is the amplitude $$A$$ of the response. Using this, we can compute the function $$c(\omega)$$, which is a sine wave and $$c^\prime(\omega)$$ which is a cosine wave. Furthermore, we can compute the phase.&nbsp;</p>
<p style="text-align:center;">\( \theta = \frac{c(\omega)}{c^\prime(\omega)} \)<br></p>
<p style="text-align:left;">And the magnitude&nbsp;</p>
<p style="text-align:center;">\( A = \sqrt{c(\omega)^2+c^\prime(\omega)^2} \)<br></p>
<p style="text-align:left;">The Fourier transform allows us to write any (periodic) function as an infinite sum of sinusoids. It gives simple waves with distinct frequencies and phases. The Fourier transform is defined as&nbsp;</p>
<p style="text-align:center;">\( \hat f(\omega) = \displaystyle\int^\infty_{-infty} f(x)e^{-2\pi i \omega x}dx \)<br></p>
<p style="text-align:left;">Where&nbsp;</p>
<p style="text-align:center;">\( e^{2\pi i \theta} =\cos(2\pi \theta) + i \sin (2 \pi \theta) \)<br></p>
<p style="text-align:left;">The result is a complex number, even for real-value signals. The Fourier transform compares a function with every possible frequency of sine and cosine wave, and returns how much of that frequency is present and what phase the sinusoidal wave is in. The sine part and the cosine part are returned as the imaginary and the real components of this value respectively. The Fourier transform allows us to transform a signal from the time domain to the frequency domain.&nbsp;</p>
<p style="text-align:left;">We can invert the Fourier transform to reconstruct the original function, giving us the inverse Fourier transform(ITF)</p>
<p style="text-align:left;"></p>
<p></p>
<p>\( f(\omega) = \displaystyle\int^\infty_{-infty} \hat f(x)e^{2\pi i \omega x}dx \)<br>
</p>
<p><br></p>
<h2>Discrete Fourier Transform</h2>
<p>For vector data $$[x_0,...,x_{N-1}]$$, the discrete Fourier transform is given by&nbsp;</p>
<p style="text-align:center;">\( F[k] = \displaystyle\sum^{N-1}_{j=0}x[j]e^{-2\pi i \frac{j}{N}} \)<br></p>
<p style="text-align:left;">The DFT has as many frequency components as $$x[t]$$ has elements.&nbsp;</p>
<p style="text-align:left;">Using the DFT, we can compute the phase $$\theta$$ and the magnitude $$A$$. This is the same as the angle and the magnitude of a complex number. The frequency of the component by the $$k$$ index depends on the sampling rate $$X_0 = 0$$ and $$X_{N/2} = f_n$$, where $$f_n$$ is the Nyquist rate (half the original sampling rate). So, for the $$k$$-th component, the real frequency in terms of the original signal is: $$ \text{freq }=f_nk/N $$. This gives us an amplitude and phase for each component between 0 and $$f_N$$ in evenly spaced subdivisions, with as many subdivisions as there were elements of $$x[t]$$.</p>
<p style="text-align:left;">We illustrate this with some examples. Consider the following signal generated by the sum of 2 sine curves</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/535666708/image%20%28137%29.png" alt="" class="img-fluid"><br></p>
<p style="text-align:left;">The DFT of this signal is the following</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/535666708/image%20%28138%29.png" alt="" class="img-fluid"><br></p>
<p style="text-align:left;">We can see 2 frequency bands in the DFT, for the two different frequencies in the signal. The DFT is symmetric around $$k=\frac{N}{2}$$, and arises by the definition of the Fourier transform. In the following example, we have a single sine curve with frequency 440 Hz</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/535666708/image%20%28139%29.png" alt="" class="img-fluid"><br></p>
<p style="text-align:left;">We can extract the frequency/amplitude directly from the DFT. Moreover, if we have 15 sinusoids in our signal, the DFT can give us back the frequency/amplitude of each of them</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/535666708/image%20%28140%29.png" alt="" class="img-fluid"><br></p>
<p style="text-align:left;">Using this result, we can reconstruct the original signal (using the inverse DFT), but with only the first few frequencies. We typically choose those with the highest amplitude since they affect the signal the most.</p>
<p style="text-align:left;">The naïve implementation of DFT is $$O(N^2)$$, with lots of expensive floating point operations like exponentiation. There is a fast Fourier transform (FFT), which is $$O(N \log N)$$ and uses divide-and-conquer strategy. The FFT is&nbsp;$$O(N \log N)$$ if $$N$$ is a power of two (because the standard FFT splits the signal recursively into two), and $$O(N^2)$$ for any other number. Variations of FFT run in&nbsp;$$O(N \log N)$$ for vectors with length which is highly composite, but still $$O(N^2)$$ for prime $$N$$</p>
<p style="text-align:left;"><br></p>
<h2>The Convolution Theorem</h2>
<p>The convolution theorem states that the Fourier transform of the convolution of two signals is equal to the elementwise product of the Fourier transform of the two signals. That is,&nbsp;</p>
<p style="text-align:center;">\( FT(f(x) * g(x)) = FT(f(x)) \cdot FT(g(x)) \)<br></p>
<p style="text-align:left;">Where $$*$$ is the convolution and $$\cdot$$ is the elementwise product. Using this identity, we can compute the convolution $$f * g$$ by&nbsp;</p>
<p style="text-align:center;">\( f(x) * g(x) = IFT(FT(f(x)) \cdot FT(g(x))) \)<br></p>
<p style="text-align:left;">We can compute the convolution in&nbsp;$$O(N \log N)$$&nbsp; time&nbsp;</p>
<p style="text-align:left;"><br></p>
<h2>Frequency Domain Effects</h2>
<p>Time domain convolution affects the frequency spatial domain, as we can see in the convolution theorem. We can use this to analyse the effects of filters before we apply them to images. We can apply any frequency based filter in the frequency domain by simply multiplying, but having to take the DFT can be much slower and much more memory intensive than a small convolution. The opposite is true for large convolutions.&nbsp;</p>
<p>The following are some types of filters:&nbsp;</p>
<p></p>
<ul>
    <li>A smoothing/lowpass filter - reduces high frequencies&nbsp;</li>
    <li>A highpass filter - reduces low frequencies</li>
    <li>a bandpass filter - reduces frequencies out of a certain band</li>
    <li>a north/bandstop filter - reduces frequencies inside a certain band</li>
</ul>
<p>A linear filter (i.e. a convolution) can change the amplitude of frequencies, but not introduce a new one. If a frequency is not present in a signal, no linear filter can make it appear. Only non-linear filters can introduce new frequencies. This follows from the convolution theorem (\( FT(f(x) * g(x)) = FT(f(x)) \cdot FT(g(x)) \)), since no matter what $$g(x)$$ is, if $$FT(f(x)) = 0$$ (i.e. no frequency present), then the convolution will also have $$FT(f(x) * g(x)) = 0$$</p>
<p><br></p>
<h2>Sunspots Example</h2>
<p>We can apply some filters to the sunspots data, the number of sunspots seen every 5 years.</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/535666708/image%20%28141%29.png" alt="" width="800" height="437" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">We can apply the DFT (along with a smoothening filter) to extract the frequencies of the signal.&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/535666708/image%20%28142%29.png" alt="" width="800" height="428" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">This data can be thought of as having 3 components:&nbsp;</p>
<p style="text-align:left;"></p>
<ul>
    <li>Random fluctuations, which vary from day to day, which would be a high frequency&nbsp; noise</li>
    <li>Some long term trend(e.g. the sun rotating around the galaxy), which would be a low-frequency oscillation</li>
    <li>The oscillation of the nuclear processes within the sun.&nbsp;</li>
</ul>
<p>This can be sun in the image below&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/535666708/image%20%28143%29.png" alt="" width="700" height="470" class="img-fluid atto_image_button_text-bottom"><br></p>
<p>These 3 components produce 3 different frequencies. In this case, there is some kind of peak around about 0.1 cycles/year, or every 10 years. This is visible on the magnitude spectrum plot. By the convolution theorem, we can design a filter to select only these frequencies. We just define a mask in frequency space, convert to the time domain using the ITF and convolve. We can also multiply in the frequency domain and then invert. This is the same according the the convolution theorem.&nbsp;</p>
<p>So, we will create a simple function in the frequency domain that will select the frequencies we are interested in, and mast out the remainder. A Gaussian function (used for the PDF of a normal distribution) is an effective mask to use, as it is and smooth, we will use the one centred at 0.1</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/535666708/image%20%28144%29.png" alt="" class="img-fluid"><br></p>
<p>We can now take this frequency domain representation, and convert it to the time domain using the IFT. This will give us a convolution kernel we can apply to the signal.&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/535666708/image%20%28145%29.png" alt="" width="800" height="425" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">We can apply the convolution to the original signal now to get a new signal&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/535666708/image%20%28146%29.png" alt="" width="800" height="448" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">We can also look at this filtered signal in the frequency domain again, and see how the frequencies have been changed by applying the convolution kernel to the signal.&nbsp;</p>
<p style="text-align:center;"><img src="https://moodle.gla.ac.uk/draftfile.php/4276126/user/draft/535666708/image%20%28147%29.png" alt="" width="800" height="423" class="img-fluid atto_image_button_text-bottom"><br></p>
<p style="text-align:left;">The frequency of interest (0.1) has been preserved and the remaining frequencies have been dampened, as we had designed it.&nbsp;</p>